---
title: "Stat_text"
output: html_document
date: "2024-05-23"
header-includes:
  - \usepackage{graphicx}
  - \usepackage{float}
  - \usepackage{booktabs}
  - \usepackage{caption}
  - \usepackage{subcaption}
---
---

### 1. Obtaining the Data

We originally found the dataset on Kaggle. The data was collected by the Polytechnic Institute of Portalegre in Portugal to build machine learning models that predict a student’s outcome based on various socioeconomic factors and academic performance. This was done to develop an analytics tool for the tutoring program to direct their efforts more effectively.

The dataset was created from several disjoint databases and includes students enrolled in different undergraduate degrees, such as agronomy, design, education, nursing, journalism, management, social service, and technologies, it consists of 4424 records with 35 features.

The prediction problem is formulated as a three-category classification task, which assesses, based on socio-economic data and performance metrics during the academic years, whether a student will:

- Graduate within the three years of planned course activities (‘Graduate’)
- Change course or stop studying altogether (‘Dropout’)
- Fail to graduate in time (‘Enrolled’)

According to the literature in the field, there is no agreed-upon definition of what constitutes a dropout. In this work, the authors defined dropouts from a micro-perspective, considering field and institution changes as dropouts regardless of when they occur. This approach results in much higher dropout rates than the macro-perspective, which considers only students who leave the higher education system without a degree.

Given that the number of libraries we can use is limited to those covered during the course, we decided to employ this dataset to answer a different research question. Our analysis will focus on building a model to assign a probability score to new students. This score will quantify the likelihood that a student will finish their course within a three-year timeframe based on data collected at enrollment.

A detailed description of the original dataset can be found in Appendix A.

 
### Step 2. Data Preprocessing 

Let's start by loading our data and changing the output column in order to represent our research question correctly: we are going to convert all the 'Enrolled' labels into 'Dropouts' since they did not manage to complete their course in the scheduled timeframe

```{r,results='hide'}
data <- read.csv("C:\\Users\\gorni\\Downloads\\student_data.csv", 
                 sep = ';')
str(data)
table(data$Output)
# Remove 'Enrolled' to have a binomial problem

index_enrolled <- which(data$Output == 'Enrolled')
# data <- data[-index, ]
data$Output[index_enrolled] <- 'Dropout'
```

```{r, include=FALSE}
library(knitr)
# Rimuoviamo le variabili Curricular
data <- data[, -c(20:31)]
# Dato che raggruppiamo i corsi non c'interessa quali siano serali
data <- data[, -5]
# Rimuoviamo Application.mode e order che sono confusionarie
index <- which(colnames(data) == 'Application.mode' | 
                 colnames(data) == 'Application.order')
data <- data[, -index]
### Collapse some categorical factor


```


Missing values:

there were no missing values in the dataset.
```{r}
sum(is.na(data))
```


**Data labeling**

We changed the label criteria for some of our variables in order to increase model explainability, in the following section we will discuss and showcase our changes.



Marital status

Categorical variable indicating the marital status of the individual.
We only have 4 widowers and 6 legally separated instances, therefore we collapsed them under the variable 'Others', we also decided to merge 'divorced' and 'legally separated' since the difference between the two instances is not relevant for our analysis.
```{r,echo=FALSE}
## Marital status
#1 – single 2 – married 3 – widower 4-divorced 5 de-facto union 6–legally separated)



index_single <- which(data$Marital.status == 1) # Single
index_married <- which(data$Marital.status == 2) # Married
data$Marital.status[index_single] <- 'Single'
data$Marital.status[index_married] <- 'Married'
data$Marital.status[-c(index_single, index_married)] <- 'Others'

cat("Modified marital status \n")
table(data$Marital.status)
```




Mother/Father occupation

Categorical variables indicating the mother and father occupation respectively, while the original dataset had 32 labels for all kinds of different jobs, we decided to merge some of the labels and make this a 3-label categorical variable. Jobs were split based on a White/Blue collar distinction, as shown in the table below.
```{r,echo=FALSE}
## Mother's occupation
# Split in white / blue collar
index_m_no_worker <- which(data$Mother.s.occupation == 1| # Student
                             data$Mother.s.occupation == 12 | # Other
                             data$Mother.s.occupation == 13) # Blanck
index_m_white <- which(data$Mother.s.occupation == 2 |
                         data$Mother.s.occupation == 3 |
                         data$Mother.s.occupation == 4 |
                         data$Mother.s.occupation == 5 |
                         data$Mother.s.occupation == 6 |
                         data$Mother.s.occupation == 14 | # Armed forces Officers
                         data$Mother.s.occupation == 15 | # Armed forces sergeants
                         data$Mother.s.occupation ==  17 |
                         data$Mother.s.occupation == 19 |
                         data$Mother.s.occupation == 20 |
                         data$Mother.s.occupation == 21 |
                         data$Mother.s.occupation == 22 |
                         data$Mother.s.occupation == 23 |
                         data$Mother.s.occupation == 24 |
                         data$Mother.s.occupation == 25 |
                         data$Mother.s.occupation == 26 |
                         data$Mother.s.occupation == 27 |
                         data$Mother.s.occupation == 28 |
                         data$Mother.s.occupation == 29)
data$Mother.s.occupation[index_m_no_worker] <- 'Others'
data$Mother.s.occupation[index_m_white] <- 'White Collar'
data$Mother.s.occupation[-c(index_m_no_worker, 
                            index_m_white)] <- 'Blue Collar'

cat("Modified Mother's occupation \n")
table(data$Mother.s.occupation)

```




Mother/Father/student education

Categorical variable indicating the level of each parents’ qualification as well as the student's.
Again, we are dealing with many categorical variables, so we decided to merge them based on whether they have an completed an higher education cycle and if they have finished high school or not
```{r,echo=FALSE}
## Mother's qualification

index_m_secondary <- which(data$Mother.s.qualification == 1 | 
                             data$Mother.s.qualification == 6 |
                             data$Mother.s.qualification == 13 |
                             data$Mother.s.qualification == 15 |
                             data$Mother.s.qualification == 22 |
                             data$Mother.s.qualification == 23 |
                             data$Mother.s.qualification == 29 |
                             data$Mother.s.qualification == 31 |
                             data$Mother.s.qualification == 32)
index_m_higher <- which(data$Mother.s.qualification == 2 |
                          data$Mother.s.qualification == 3 |
                          data$Mother.s.qualification == 4 |
                          data$Mother.s.qualification == 5 |
                          data$Mother.s.qualification == 30 |
                          data$Mother.s.qualification == 33 |
                          data$Mother.s.qualification == 34)
data$Mother.s.qualification[index_m_secondary] <- 'Secondary'
data$Mother.s.qualification[index_m_higher] <- 'Higher'
data$Mother.s.qualification[-c(index_m_secondary, 
                               index_m_higher)] <- 'No Secondary'


# Print the modified column
cat("Modified Mother's qualification \n")
print(table(data$Mother.s.qualification))

```

```{r,include=FALSE}
## Father's occupation
# Slpit in white / blue collar
index_f_no_worker <- which(data$Father.s.occupation == 1| # Student
                             data$Father.s.occupation == 12 | #Other
                             data$Father.s.occupation == 13) # Blanck
index_f_white <- which(data$Father.s.occupation == 2 |
                         data$Father.s.occupation == 3 |
                         data$Father.s.occupation == 4 |
                         data$Father.s.occupation == 5 |
                         data$Father.s.occupation == 6 |
                         data$Father.s.occupation == 14 | # Armed forces Officers
                         data$Father.s.occupation == 15 | # Armed forces sergeants
                         data$Father.s.occupation ==  17 |
                         data$Father.s.occupation == 19 |
                         data$Father.s.occupation == 20 |
                         data$Father.s.occupation == 21 |
                         data$Father.s.occupation == 22 |
                         data$Father.s.occupation == 23 |
                         data$Father.s.occupation == 24 |
                         data$Father.s.occupation == 25 |
                         data$Father.s.occupation == 26 |
                         data$Father.s.occupation == 27 |
                         data$Father.s.occupation == 28 |
                         data$Father.s.occupation == 29)
data$Father.s.occupation[index_f_no_worker] <- 'Others'
table(data$Father.s.occupation)
data$Father.s.occupation[index_f_white] <- 'White Collar'
data$Father.s.occupation[-c(index_f_no_worker, 
                            index_f_white)] <- 'Blue Collar'

## Previous qualification
# Group on (no secondary education, secondary education, bachelor's degree, 
# master's degree, phd or highter)
table(data$Previous.qualification)
index_secondary <- which(data$Previous.qualification == 1 | 
                           data$Previous.qualification == 6 |
                           data$Previous.qualification == 14 |
                           data$Previous.qualification == 16)
# Aggiungiamo '6' perchè se frequenti l'uni ti sei già diplomato e 
# se fai dei corsi da specializzando tecnico dovresti aver 
# fatto le superiori, però non valgono come la laurea

index_higher <- which(data$Previous.qualification == 2 |
                        data$Previous.qualification == 3 |
                        data$Previous.qualification == 15 |
                        data$Previous.qualification == 4 | # Since just 8
                        data$Previous.qualification == 5| # Since just 1 phd
                        data$Previous.qualification == 17)
data$Previous.qualification[index_secondary] <- 'Secondary'
data$Previous.qualification[index_higher] <- 'Higher'
data$Previous.qualification[-c(index_secondary, 
                               index_higher)] <- 'No Secondary'

# Gender
index_male <- which(data$Gender == 1)
data$Gender[index_male] <- 'Male'
data$Gender[-index_male] <- 'Female'

str(data)


## Father's qualification
table(data$Father.s.qualification)
index_f_secondary <- which(data$Father.s.qualification == 1 | 
                             data$Father.s.qualification == 6 |
                             data$Father.s.qualification == 13 |
                             data$Father.s.qualification == 15 |
                             data$Father.s.qualification == 22 |
                             data$Father.s.qualification == 23 |
                             data$Father.s.qualification == 29 |
                             data$Father.s.qualification == 31 |
                             data$Father.s.qualification == 32)
index_f_higher <- which(data$Father.s.qualification == 2 |
                          data$Father.s.qualification == 3 |
                          data$Father.s.qualification == 4 |
                          data$Father.s.qualification == 5 |
                          data$Father.s.qualification == 30 |
                          data$Father.s.qualification == 33 |
                          data$Father.s.qualification == 34)
data$Father.s.qualification[index_f_secondary] <- 'Secondary'
data$Father.s.qualification[index_f_higher] <- 'Higher'
data$Father.s.qualification[-c(index_f_secondary, 
                               index_f_higher)] <- 'No Secondary'


```

Courses


Categorical variable representing the course chosen at enrollment, we originally had 17 different courses, we decided to relabel the courses using whether they are STEM subjects or not as a splitting criterion
```{r,results='hide'}
## Course
index_stem <- which(data$Course == 1 |
                      data$Course == 4 |
                      data$Course == 6 |
                      data$Course == 7 |
                      data$Course == 8 |
                      data$Course == 12 |
                      data$Course == 13)
data$Course[index_stem] <- 'Stem'
data$Course[-index_stem] <- 'No Stem'

cat("Modified course \n")
table(data$Course)
```

Nationality:

categorical variable representing a students nationality: looking at the data we saw that while we had a lot of labels (one for each nationality), the vast majority of people were Portuguese, therefore we labeled data in the following way
```{r,results='hide'}

## Nationality
table(data$Nacionality)
data$Nacionality[data$Nacionality != 1] <- 'Others' # just 2%
data$Nacionality[data$Nacionality == 1] <- 'Portoguese'

```

```{r,include=FALSE}
# Change features as categorical
colnames(data)
categorical_names <- colnames(data)[-c(15, 17:19)]

for(i in categorical_names){
  data[, i] <- as.factor(data[, i])
}

## Valutiamo quale variabile prende come riferimento
# Marital Status
levels(data$Marital.status)
data$Marital.status <- relevel(data$Marital.status, ref = "Single")
levels(data$Course)
levels(data$Previous.qualification)
levels(data$Previous.qualification)
data$Previous.qualification <- relevel(data$Previous.qualification, 
                                       ref = 'Higher')
levels(data$Nacionality)
data$Nacionality <- relevel(data$Nacionality, 
                            ref = 'Portoguese')
levels(data$Mother.s.qualification)
levels(data$Father.s.qualification)
levels(data$Mother.s.occupation)
levels(data$Father.s.occupation)
levels(data$Gender)
levels(data$Output)

table(data$Nacionality, data$International)
data <- data[, -which(colnames(data) == 'International')]
```

**FEATURE REMOVAL**

We decided to remove some features from our data (non mi viene cosa scrivere ora).

1 Curricular data: we removed all information concerning student performance over the span of three years since it doesn't help us answer our research question

2 Application mode/ application order: the original paper didn't offer a clear indication about the various labels of this feature, furthermore we don't believe them to be of any interest as far as our research question goes

**OUTLIERS DETECTION AND REMOVAL**

We looked for outliers among numerical features in our data, 'age at enrollment' was the only one that presented some.Our chosen method of outliers removal consists of removing all values which are not within 2 standards deviation from the mean.  We then perform a train-test split before and remove the outliers from the training set only: keeping outliers in the test set improves the ecological validity of the model and reduces overfitting.
```{r}
# Train test split
set.seed(71)
index <- sample(1:nrow(data), 
                round(0.75 * nrow(data)))
train <- data[index, ]
test <- data[-index, ]

# outliers removal
boxplot(train$Age.at.enrollment ~ train$Output)

index_age <- which(abs(scale(train$Age.at.enrollment)) > 2)
index_unemployment <- which(abs(scale(train$Unemployment.rate)) > 2)
index_inflation <- which(abs(scale(train$Inflation.rate)) > 2)
index_gdp <- which(abs(scale(train$GDP)) > 2)
index_outliers <- unique(c(index_age, 
                           index_unemployment, 
                           index_inflation, 
                           index_gdp))
train <- train[-index_outliers, ]

```
```{r, echo=TRUE, fig.show='hide'}

```

```{r}
boxplot(train$Age.at.enrollment ~ train$Output)
```





### 3. Exploratory Data Analysis

We are now going to plot our categorical an numerical variables...

| Marital.status        | Freq || Course                | Freq |
|:----------------------|-----:||:----------------------|-----:|
| Single                | 3919 || No Stem               | 2702 |
| Married               |  379 || Stem                  | 1722 |
| Others                |  126 ||                       |      |

| Previous.qualification | Freq || Nationality    | Freq |
|:-----------------------|-----:||:---------------|-----:|
| Higher                 |  204 || Portoguese     | 4314 |
| No Secondary           |  232 || Others         |  110 |
| Secondary              | 3988 ||                |      |

| Mother.s.qualification | Freq || Father.s.qualification | Freq |
|:-----------------------|-----:||:-----------------------|-----:|
| Higher                 |  591 || Higher                 |  415 |
| No Secondary           |  234 || No Secondary           |3076 |
| Secondary              | 3599 || Secondary              |  933 |

| Mother.s.occupation | Freq || Father.s.occupation | Freq |
|:--------------------|-----:||:--------------------|-----:|
| Blue Collar         | 2004 || Blue Collar         | 2567 |
| Others              |  231 || Others              |  212 |
| White Collar        | 2189 || White Collar        | 1645 |

| Displaced | Freq |       | Educational.special.needs | Freq |
|:----------|-----:|-------|:---------------------------|-----:|
| 0         | 1998 |       | 0                           | 4373 |
| 1         | 2426 |       | 1                           |   51 |

| Debtor | Freq |       | Tuition.fees.up.to.date | Freq |
|:-------|-----:|-------|:-------------------------|-----:|
| 0      | 3921 |       | 0                         |  528 |
| 1      |  503 |       | 1                         | 3896 |
-----------------       -----------------------------
| Gender | Freq |       | Scholarship.holder | Freq |
|:-------|-----:|-------|:-------------------|-----:|
| Female | 2868 |       | 0                  | 3325 |
| Male   | 1556 |       | 1                  | 1099 |

| Output   | Freq |
|:---------|-----:|
| Dropout  | 2215 |
| Graduate | 2209 |

```{r,include=FALSE}

library(knitr)

categorical_cols <- names(data)[sapply(data, is.factor)]

print_contingency_tables <- function(data, categorical_columns) {
  for (column in categorical_columns) {
    cat("\n Contingency Table for", column, "\n\n")
    contingency_table <- table(data[[column]])
    names(contingency_table) <- paste(column, names(contingency_table), sep = " = ")
    print(kable(contingency_table, caption = paste("Contingency Table for", column), align = 'c',col.names = c(column,"freq")))
  }
}

print_contingency_tables(data, categorical_cols)





```
```{r}

knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(htmltools)
```



### 4. Model building
We will now try to build different models using the training set,the models we will use include logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), ridge regression, and lasso regression.

This project encompasses a step-by-step approach to model implementation, evaluation, and comparison, utilizing libraries such as `MASS`, `glmnet`, and `class` for model fitting, prediction, and performance assessment.

In the following section we will build the models one by one and briefly talk about which insights tey offer, a more detailed discussion will be in the following section


**Logistic Regression**

One of the first model we employ is logistic regression, this seems like an obvious choice given the fact that our research question revolves around finding a probability in a binary classification task.

The core idea behind logistic regression is to model the relationship between one or more independent variables (features) and a binary dependent variable (outcome) using the logistic function. This function maps the output of a linear combination of the features to a probability score between 0 and 1.

In logistic regression, the coefficients associated with each feature are estimated using maximum likelihood estimation. These coefficients represent the impact of each feature on the log-odds of the outcome variable.
```{r,include=FALSE}
# ROC
library(pROC)
```


```{r,results='hide'}
######################################
# Logistic Regression
######################################
fit1 <- glm(Output ~ ., 
            family = binomial, 
            data = train)
summary(fit1)
fit2 <- step(fit1, 
             direction = 'backward')
summary(fit2)
```


| Variable             | Estimate | Std. Error | z value |Pr(>|z|) |
|----------------------|----------|------------|---------|----------|
| (Intercept)          | 3.38052  | 0.80612    | 4.194   | 2.75e-05 *** |
| Marital_Married      | 0.08176  | 0.46760    | 0.175   | 0.861201    |
| Marital_Others       | -1.74991 | 0.93198    | -1.878  | 0.060432 .  |
| Course_Stem          | 0.26065  | 0.09506    | 2.742   | 0.006106 ** |
| Prev_NoSecondary     | -13.54671| 235.71280  | -0.057  | 0.954170    |
| Prev_Secondary       | -0.09158 | 0.45666    | -0.201  | 0.841058    |
| Nationality_Others   | 0.46162  | 0.31499    | 1.466   | 0.142780    |
| Mother_NoSecondary   | -0.27751 | 0.26401    | -1.051  | 0.293192    |
| Mother_Secondary     | 0.16344  | 0.12751    | 1.282   | 0.199913    |
| Debtor               | -0.84311 | 0.18519    | -4.553  | 5.30e-06 *** |
| Tuition_Paid         | 2.28948  | 0.26585    | 8.612   | < 2e-16  *** |
| Gender_Male          | -0.65291 | 0.09854    | -6.626  | 3.45e-11 *** |
| Scholarship          | 1.21674  | 0.11197    | 10.867  | < 2e-16  *** |
| Age_Enrollment       | -0.31217 | 0.02527    | -12.352 | < 2e-16  *** |
| Unemployment_Rate    | 0.05150  | 0.01865    | 2.761   | 0.005769 ** |
| GDP                  | 0.08171  | 0.02177    | 3.754   | 0.000174 *** |

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



```{r}
par(mfrow = c(2, 2))
plot(fit2)
par(mfrow = c(1, 1))
```


```{r}

pred_logit <- predict(fit2, 
                      newdata = test, 
                      type = 'response')
roc_out_logit <- roc(test$Output ~ pred_logit, 
                     levels = c('Dropout', 'Graduate'))
plot(roc_out_logit,  
     print.auc=TRUE, 
     legacy.axes=TRUE, 
     xlab="False Positive Rate", 
     ylab="True Positive Rate",
     main="Logistic Regression")
auc(roc_out_logit)
```




***Linear Discriminant Analysis (LDA)***

LDA is a classic statistical technique utilized in machine learning and pattern recognition for classification tasks. We employ it to identify which linear combination of features best separates multiple classes or categories in our dataset.

At its core, LDA operates under the assumption that the data can be represented as multivariate Gaussian distributions and that the classes share the same covariance matrix. It seeks to find a projection of the data onto a lower-dimensional space while maximizing the separation between classes and minimizing the variance within each class.

One of the key advantages of LDA is its simplicity and interpretability. The resulting linear discriminant functions provide insights into how different features contribute to the classification decision. Additionally, LDA is robust when the assumptions regarding the data distribution are met, making it a reliable choice for many classification problems.

In practice, LDA is often used in conjunction with other techniques for data exploration, classification, and feature extraction. It finds applications in diverse fields such as finance, biology, and image processing, where the goal is to effectively differentiate between multiple classes based on available features.


```{r,results='hide'}
######################################
# Linear Discriminant Analysis (LDA)
######################################

library(MASS)
lda_fit <- lda(Output ~ ., 
               data = train)
lda_fit
```


```{r,eval=FALSE}
# plot the values of the discriminant function for the two groups
# 
plot(lda_fit, 
     type="histogram")
plot(lda_fit) # histogram is the default value
plot(lda_fit, type="density")
```


```{r,include=FALSE}
plot(lda_fit, type="both")


pred_lda <- predict(lda_fit, 
                    newdata = test, 
                    type = 'response')
roc_out_lda <- roc(test$Output ~ pred_lda$posterior[, 2], 
                   levels = c('Dropout', 'Graduate'))
```


```{r}
##auc(roc_out_lda)
plot(roc_out_lda,  
     print.auc=TRUE, 
     legacy.axes=TRUE, 
     xlab="False Positive Rate", 
     ylab="True Positive Rate", 
     main = 'LDA')


```

***Quadratic Discriminant Analysis (QDA)***
Qda...
```{r}
######################################
# Quadratic Discriminant Analysis (QDA)
######################################
qda_fit <- qda(Output ~ ., 
               data = train)
qda_fit
pred_qda <- predict(qda_fit, 
                    newdata = test, 
                    type = 'response')
roc_out_qda <- roc(test$Output ~ pred_qda$posterior[, 2], 
                   levels = c('Dropout', 'Graduate'))
plot(roc_out_qda,  
     print.auc=TRUE, 
     legacy.axes=TRUE, 
     xlab="False Positive Rate", 
     ylab="True Positive Rate", 
     main = 'LDA')
auc(roc_out_qda)



```

***Ridge Regression***

Ridge regression is a regularization technique used in linear regression to mitigate the issues of multicollinearity and overfitting: it can be used to work with high dimensional data, which is why we find it valuable in our case
```{r}
###########################
# Ridge Regression
###########################

library(glmnet)


design_matrix_train <- model.matrix(Output ~ .,
                                    data = train)

design_matrix_test <- model.matrix(Output ~ .,
                                   data = test)

# Adattamento del modello Ridge con regressione Binomiale
ridge_model <- glmnet(x = design_matrix_train, 
                      y = train$Output,
                      alpha = 0, 
                      intercept = F, 
                      family = "binomial")
# Find the best lambda

predictions <- predict(ridge_model, 
                       newx = design_matrix_test, 
                       type = 'response')
##########################

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$Output)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index <- which.max(accuracies)
ridge_pred <- matrix(predictions[, best_lambda_index], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index]

colnames(ridge_pred) <- 'ridge_pred'


roc_out_ridge <- roc(test$Output ~ as.numeric(ridge_pred), 
                     levels = c('Dropout', 'Graduate'))
plot(roc_out_ridge,  
     print.auc=TRUE, 
     legacy.axes=TRUE, 
     xlab="False Positive Rate", 
     ylab="True Positive Rate", 
     main = 'Ridge Regression')
auc(roc_out_ridge)

```

***Lasso Regression***

This regression techniques takes a slightly different approach by adding a penalty term that penalizes the absolute values of the regression coefficients, instead of their squares (which is what Rigde regression does).

This penalty term encourages sparsity in the coefficient vector, effectively driving some coefficients to exactly zero. As a result, Lasso regression not only helps in shrinking coefficient values but also performs variable selection by automatically excluding irrelevant features from the model.
```{r}
###########################
# Lasso Regression
###########################

# Adattamento del modello Lasso con regressione Binomiale
lasso_model <- glmnet(x = design_matrix_train, 
                      y = train$Output,
                      alpha = 1, 
                      intercept = F, 
                      family = "binomial")
# Find the best lambda

predictions <- predict(lasso_model, 
                       newx = design_matrix_test, 
                       type = 'response')
##########################

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$Output)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index <- which.max(accuracies)
lasso_pred <- matrix(predictions[, best_lambda_index], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index]

colnames(lasso_pred) <- 'lasso_pred'


roc_out_lasso <- roc(test$Output ~ as.numeric(lasso_pred), 
                     levels = c('Dropout', 'Graduate'))
plot(roc_out_lasso,  
     print.auc=TRUE, 
     legacy.axes=TRUE, 
     xlab="False Positive Rate", 
     ylab="True Positive Rate", 
     main = 'Lasso regression')
auc(roc_out_lasso)

##à DA vedere se lancia
# add labels to identify the variables
plot(ridge_model, xvar="lambda", label=TRUE)

plot(ridge_model, xvar = "norm", label=TRUE)
plot(ridge_model, xvar = "dev",  label=TRUE)

plot(ridge_model, xvar = 'norm')


```

### appendix A: Dataset

```{r echo=FALSE, message=FALSE}
# Load necessary library
library(knitr)

# Create a data frame that represents your dataset
data <- data.frame(
  Variable = c("Marital status", "Application mode", "Application order", "Course", 
               "evening attendance", "Displaced", "Educational special needs", 
               "Debtor", "Tuition fees up to date", "Gender", "Scholarship holder", 
               "Age at enrollment", "International", "Curricular units 1st sem (credited)", 
               "Curricular units 1st sem (enrolled)", "Curricular units 1st sem (evaluations)", 
               "Curricular units 1st sem (approved)", "Curricular units 1st sem (grade)", 
               "Curricular units 1st sem (without evaluations)", "Curricular units 2nd sem (credited)", 
               "Curricular units 2nd sem (enrolled)", "Curricular units 2nd sem (evaluations)", 
               "Curricular units 2nd sem (approved)", "Curricular units 2nd sem (grade)", 
               "Curricular units 2nd sem (without evaluations)", "Unemployment rate", 
               "Inflation rate", "GDP", "output", "Previous qualification",
               "Nationality", "Mother's qualification", "Father's qualification",
               "Mother's occupation", "Father's occupation"),
  Description = c("Categorical variable indicating the marital status of the individual",
                  "Categorical variable indicating the mode of application",
                  "Numeric variable indicating the order of application",
                  "Categorical variable indicating the chosen course",
                  "Binary variable indicating whether the individual attends classes during the daytime or evening",
                  "Binary variable indicating whether the individual has been displaced",
                  "Binary variable indicating whether the individual has educational special needs",
                  "Binary variable indicating whether the individual is a debtor",
                  "Binary variable indicating whether the tuition fees are up to date",
                  "Binary variable indicating the gender of the individual",
                  "Binary variable indicating whether the individual holds a scholarship",
                  "Numeric variable indicating the age of the individual at the time of enrollment",
                  "Binary variable indicating whether the individual is international",
                  "Numeric variable indicating the number of credited curricular units in the 1st semester",
                  "Numeric variable indicating the number of enrolled curricular units in the 1st semester",
                  "Numeric variable indicating the number of evaluations for curricular units in the 1st semester",
                  "Numeric variable indicating the number of approved curricular units in the 1st semester",
                  "Numeric variable indicating the average grade for curricular units in the 1st semester",
                  "Numeric variable indicating the number of curricular units in the 1st semester without evaluations",
                  "Numeric variable indicating the number of credited curricular units in the 2nd semester",
                  "Numeric variable indicating the number of enrolled curricular units in the 2nd semester",
                  "Numeric variable indicating the number of evaluations for curricular units in the 2nd semester",
                  "Numeric variable indicating the number of approved curricular units in the 2nd semester",
                  "Numeric variable indicating the average grade for curricular units in the 2nd semester",
                  "Numeric variable indicating the number of curricular units in the 2nd semester without evaluations",
                  "Variable indicating the unemployment rate (Unemployment rate (%))",
                  "Numeric variable indicating the inflation rate (Inflation rate (%))",
                  "Numeric variable indicating the Gross Domestic Product",
                  "Categorical variable indicating the target variable (e.g., Dropout, Graduate, Enrolled)",
                  "Numeric variable indicating the level of the previous qualification",
                  "Categorical variable indicating the nationality of the individual",
                  "Numeric variable indicating the level of the mother's qualification",
                  "Numeric variable indicating the level of the father's qualification",
                  "Categorical variable indicating the mother's occupation",
                  "Categorical variable indicating the father's occupation")
)

# Create the table using kable
kable(data, col.names = c("Variable", "Description"), align = c('l', 'l'), caption = "Dataset Description")
```
