---
title: "Stat_text"
output: html_document
date: "2024-05-23"
header-includes:
  - \usepackage{graphicx}
  - \usepackage{float}
  - \usepackage{booktabs}
  - \usepackage{caption}
  - \usepackage{subcaption}
---
---

### 1. Obtaining the Data

We originally found the dataset on Kaggle. The data was collected by the Polytechnic Institute of Portalegre in Portugal to build machine learning models that predict a student’s outcome based on various socioeconomic factors and academic performance. This was done to develop an analytics tool for the tutoring program to direct their efforts more effectively.

The dataset was created from several disjoint databases and includes students enrolled in different undergraduate degrees, such as agronomy, design, education, nursing, journalism, management, social service, and technologies, it consists of 4424 records with 35 features.

The prediction problem is formulated as a three-category classification task, which assesses, based on socio-economic data and performance metrics during the academic years, whether a student will:

- Graduate within the three years of planned course activities (‘Graduate’)
- Change course or stop studying altogether (‘Dropout’)
- Fail to graduate in time (‘Enrolled’)

According to the literature in the field, there is no agreed-upon definition of what constitutes a dropout. In this work, the authors defined dropouts from a micro-perspective, considering field and institution changes as dropouts regardless of when they occur. This approach results in much higher dropout rates than the macro-perspective, which considers only students who leave the higher education system without a degree.

Given that the number of libraries we can use is limited to those covered during the course, we decided to employ this dataset to answer a different research question. Our analysis will focus on building a model to assign a probability score to new students. This score will quantify the likelihood that a student will finish their course within a three-year timeframe based on data collected at enrollment.

A detailed description of the original dataset can be found in Appendix A.

 
### Step 2. Data Preprocessing 

Let's start by loading our data and changing the output column in order to represent our research question correctly: we are going to convert all the 'Enrolled' labels into 'Dropouts' since they did not manage to complete their course in the scheduled timeframe

```{r,results='hide'}
data <- read.csv("C:\\Users\\gorni\\Downloads\\student_data.csv", 
                 sep = ';')
str(data)
table(data$Output)
# Remove 'Enrolled' to have a binomial problem

index_enrolled <- which(data$Output == 'Enrolled')
# data <- data[-index, ]
data$Output[index_enrolled] <- 'Dropout'
```

```{r, include=FALSE}
library(knitr)
# Rimuoviamo le variabili Curricular
data <- data[, -c(20:31)]
# Dato che raggruppiamo i corsi non c'interessa quali siano serali
data <- data[, -5]
# Rimuoviamo Application.mode e order che sono confusionarie
index <- which(colnames(data) == 'Application.mode' | 
                 colnames(data) == 'Application.order')
data <- data[, -index]
### Collapse some categorical factor


```


Missing values:

there were no missing values in the dataset.
```{r}
sum(is.na(data))
```


**Data labeling**

We changed the label criteria for some of our variables in order to increase model explainability, in the following section we will discuss and showcase our changes.



Marital status

Categorical variable indicating the marital status of the individual.
We only have 4 widowers and 6 legally separated instances, therefore we collapsed them under the variable 'Others', we also decided to merge 'divorced' and 'legally separated' since the difference between the two instances is not relevant for our analysis.
```{r,echo=FALSE}
## Marital status
#1 – single 2 – married 3 – widower 4-divorced 5 de-facto union 6–legally separated)



index_single <- which(data$Marital.status == 1) # Single
index_married <- which(data$Marital.status == 2) # Married
data$Marital.status[index_single] <- 'Single'
data$Marital.status[index_married] <- 'Married'
data$Marital.status[-c(index_single, index_married)] <- 'Others'

cat("Modified marital status \n")
table(data$Marital.status)
```




Mother/Father occupation

Categorical variables indicating the mother and father occupation respectively, while the original dataset had 32 labels for all kinds of different jobs, we decided to merge some of the labels and make this a 3-label categorical variable. Jobs were split based on a White/Blue collar distinction, as shown in the table below.
```{r,echo=FALSE}
## Mother's occupation
# Split in white / blue collar
index_m_no_worker <- which(data$Mother.s.occupation == 1| # Student
                             data$Mother.s.occupation == 12 | # Other
                             data$Mother.s.occupation == 13) # Blanck
index_m_white <- which(data$Mother.s.occupation == 2 |
                         data$Mother.s.occupation == 3 |
                         data$Mother.s.occupation == 4 |
                         data$Mother.s.occupation == 5 |
                         data$Mother.s.occupation == 6 |
                         data$Mother.s.occupation == 14 | # Armed forces Officers
                         data$Mother.s.occupation == 15 | # Armed forces sergeants
                         data$Mother.s.occupation ==  17 |
                         data$Mother.s.occupation == 19 |
                         data$Mother.s.occupation == 20 |
                         data$Mother.s.occupation == 21 |
                         data$Mother.s.occupation == 22 |
                         data$Mother.s.occupation == 23 |
                         data$Mother.s.occupation == 24 |
                         data$Mother.s.occupation == 25 |
                         data$Mother.s.occupation == 26 |
                         data$Mother.s.occupation == 27 |
                         data$Mother.s.occupation == 28 |
                         data$Mother.s.occupation == 29)
data$Mother.s.occupation[index_m_no_worker] <- 'Others'
data$Mother.s.occupation[index_m_white] <- 'White Collar'
data$Mother.s.occupation[-c(index_m_no_worker, 
                            index_m_white)] <- 'Blue Collar'

cat("Modified Mother's occupation \n")
table(data$Mother.s.occupation)

```




Mother/Father/student education

Categorical variable indicating the level of each parents’ qualification as well as the student's.
Again, we are dealing with many categorical variables, so we decided to merge them based on whether they have an completed an higher education cycle and if they have finished high school or not
```{r,echo=FALSE}
## Mother's qualification

index_m_secondary <- which(data$Mother.s.qualification == 1 | 
                             data$Mother.s.qualification == 6 |
                             data$Mother.s.qualification == 13 |
                             data$Mother.s.qualification == 15 |
                             data$Mother.s.qualification == 22 |
                             data$Mother.s.qualification == 23 |
                             data$Mother.s.qualification == 29 |
                             data$Mother.s.qualification == 31 |
                             data$Mother.s.qualification == 32)
index_m_higher <- which(data$Mother.s.qualification == 2 |
                          data$Mother.s.qualification == 3 |
                          data$Mother.s.qualification == 4 |
                          data$Mother.s.qualification == 5 |
                          data$Mother.s.qualification == 30 |
                          data$Mother.s.qualification == 33 |
                          data$Mother.s.qualification == 34)
data$Mother.s.qualification[index_m_secondary] <- 'Secondary'
data$Mother.s.qualification[index_m_higher] <- 'Higher'
data$Mother.s.qualification[-c(index_m_secondary, 
                               index_m_higher)] <- 'No Secondary'


# Print the modified column
cat("Modified Mother's qualification \n")
print(table(data$Mother.s.qualification))

```

```{r,include=FALSE}
## Father's occupation
# Slpit in white / blue collar
index_f_no_worker <- which(data$Father.s.occupation == 1| # Student
                             data$Father.s.occupation == 12 | #Other
                             data$Father.s.occupation == 13) # Blanck
index_f_white <- which(data$Father.s.occupation == 2 |
                         data$Father.s.occupation == 3 |
                         data$Father.s.occupation == 4 |
                         data$Father.s.occupation == 5 |
                         data$Father.s.occupation == 6 |
                         data$Father.s.occupation == 14 | # Armed forces Officers
                         data$Father.s.occupation == 15 | # Armed forces sergeants
                         data$Father.s.occupation ==  17 |
                         data$Father.s.occupation == 19 |
                         data$Father.s.occupation == 20 |
                         data$Father.s.occupation == 21 |
                         data$Father.s.occupation == 22 |
                         data$Father.s.occupation == 23 |
                         data$Father.s.occupation == 24 |
                         data$Father.s.occupation == 25 |
                         data$Father.s.occupation == 26 |
                         data$Father.s.occupation == 27 |
                         data$Father.s.occupation == 28 |
                         data$Father.s.occupation == 29)
data$Father.s.occupation[index_f_no_worker] <- 'Others'
table(data$Father.s.occupation)
data$Father.s.occupation[index_f_white] <- 'White Collar'
data$Father.s.occupation[-c(index_f_no_worker, 
                            index_f_white)] <- 'Blue Collar'

## Previous qualification
# Group on (no secondary education, secondary education, bachelor's degree, 
# master's degree, phd or highter)
table(data$Previous.qualification)
index_secondary <- which(data$Previous.qualification == 1 | 
                           data$Previous.qualification == 6 |
                           data$Previous.qualification == 14 |
                           data$Previous.qualification == 16)
# Aggiungiamo '6' perchè se frequenti l'uni ti sei già diplomato e 
# se fai dei corsi da specializzando tecnico dovresti aver 
# fatto le superiori, però non valgono come la laurea

index_higher <- which(data$Previous.qualification == 2 |
                        data$Previous.qualification == 3 |
                        data$Previous.qualification == 15 |
                        data$Previous.qualification == 4 | # Since just 8
                        data$Previous.qualification == 5| # Since just 1 phd
                        data$Previous.qualification == 17)
data$Previous.qualification[index_secondary] <- 'Secondary'
data$Previous.qualification[index_higher] <- 'Higher'
data$Previous.qualification[-c(index_secondary, 
                               index_higher)] <- 'No Secondary'

# Gender
index_male <- which(data$Gender == 1)
data$Gender[index_male] <- 'Male'
data$Gender[-index_male] <- 'Female'

str(data)


## Father's qualification
table(data$Father.s.qualification)
index_f_secondary <- which(data$Father.s.qualification == 1 | 
                             data$Father.s.qualification == 6 |
                             data$Father.s.qualification == 13 |
                             data$Father.s.qualification == 15 |
                             data$Father.s.qualification == 22 |
                             data$Father.s.qualification == 23 |
                             data$Father.s.qualification == 29 |
                             data$Father.s.qualification == 31 |
                             data$Father.s.qualification == 32)
index_f_higher <- which(data$Father.s.qualification == 2 |
                          data$Father.s.qualification == 3 |
                          data$Father.s.qualification == 4 |
                          data$Father.s.qualification == 5 |
                          data$Father.s.qualification == 30 |
                          data$Father.s.qualification == 33 |
                          data$Father.s.qualification == 34)
data$Father.s.qualification[index_f_secondary] <- 'Secondary'
data$Father.s.qualification[index_f_higher] <- 'Higher'
data$Father.s.qualification[-c(index_f_secondary, 
                               index_f_higher)] <- 'No Secondary'


```

Courses


Categorical variable representing the course chosen at enrollment, we originally had 17 different courses, we decided to relabel the courses using whether they are STEM subjects or not as a splitting criterion
```{r,echo=FALSE}
## Course
index_stem <- which(data$Course == 1 |
                      data$Course == 4 |
                      data$Course == 6 |
                      data$Course == 7 |
                      data$Course == 8 |
                      data$Course == 12 |
                      data$Course == 13)
data$Course[index_stem] <- 'Stem'
data$Course[-index_stem] <- 'No Stem'

cat("Modified course \n")
table(data$Course)
```

Nationality:

categorical variable representing a students nationality: looking at the data we saw that while we had a lot of labels (one for each nationality), the vast majority of people were Portuguese, therefore we labeled data in the following way
```{r,echo=FALSE}

## Nationality

data$Nacionality[data$Nacionality != 1] <- 'Others' # just 2%
data$Nacionality[data$Nacionality == 1] <- 'Portoguese'

cat("Modified nationality \n")
table(data$Nacionality)
```

```{r,include=FALSE}
# Change features as categorical
colnames(data)
categorical_names <- colnames(data)[-c(15, 17:19)]

for(i in categorical_names){
  data[, i] <- as.factor(data[, i])
}

## Valutiamo quale variabile prende come riferimento
# Marital Status
levels(data$Marital.status)
data$Marital.status <- relevel(data$Marital.status, ref = "Single")
levels(data$Course)
levels(data$Previous.qualification)
levels(data$Previous.qualification)
data$Previous.qualification <- relevel(data$Previous.qualification, 
                                       ref = 'Higher')
levels(data$Nacionality)
data$Nacionality <- relevel(data$Nacionality, 
                            ref = 'Portoguese')
levels(data$Mother.s.qualification)
levels(data$Father.s.qualification)
levels(data$Mother.s.occupation)
levels(data$Father.s.occupation)
levels(data$Gender)
levels(data$Output)

table(data$Nacionality, data$International)
data <- data[, -which(colnames(data) == 'International')]
```

**FEATURE REMOVAL**

We decided to remove some features from our data (non mi viene cosa scrivere ora).

1 Curricular data: we removed all information concerning student performance over the span of three years since it doesn't help us answer our research question

2 Application mode/ application order: the original paper didn't offer a clear indication about the various labels of this feature, furthermore we don't believe them to be of any interest as far as our research question goes

**OUTLIERS DETECTION AND REMOVAL**

We looked for outliers among numerical features in our data, 'age at enrollment' was the only one that presented some.Our chosen method of outliers removal consists of removing all values which are not within 2 standards deviation from the mean.  We then perform a train-test split before and remove the outliers from the training set only: keeping outliers in the test set improves the ecological validity of the model and reduces overfitting.
```{r}
# Train test split
set.seed(71)
index <- sample(1:nrow(data), 
                round(0.75 * nrow(data)))
train <- data[index, ]
test <- data[-index, ]

# outliers removal
boxplot(train$Age.at.enrollment ~ train$Output)

index_age <- which(abs(scale(train$Age.at.enrollment)) > 2)
index_unemployment <- which(abs(scale(train$Unemployment.rate)) > 2)
index_inflation <- which(abs(scale(train$Inflation.rate)) > 2)
index_gdp <- which(abs(scale(train$GDP)) > 2)
index_outliers <- unique(c(index_age, 
                           index_unemployment, 
                           index_inflation, 
                           index_gdp))
train <- train[-index_outliers, ]

```
```{r, echo=TRUE, fig.show='hide'}

```

```{r}
boxplot(train$Age.at.enrollment ~ train$Output)
```





### 3. Exploratory Data Analysis

In this section we will take a closer look at our data. First, we'll plot some contingency tables for our categorical variables. 
TESTO PRESO DA CPT, BEPPE VEDI SE ALCUNE COSE HANNO SENSO
Key Findings
Among single students, the dropout rate is notably high with 1904 dropouts compared to 2015 graduates. This trend suggests that single students face challenges that significantly impact their ability to complete their studies. In contrast, married students exhibit a lower dropout rate (231) and a relatively modest number of graduates (148), indicating different dynamics at play in this group. Students with other marital statuses show much lower numbers overall, making it difficult to draw significant conclusions.

When examining course types, students enrolled in non-STEM courses exhibit substantial dropout (1333) and graduation (1309) rates. Those in STEM courses have fewer dropouts (822) and slightly more graduates (900), suggesting a marginally higher retention rate in STEM fields.

The influence of prior qualifications is pronounced. Students with higher qualifications before enrollment show lower dropout numbers (124) and a smaller cohort of graduates (80). Conversely, those with only secondary education qualifications have the highest dropout (1922) and graduation (2066) rates, indicating a larger population and perhaps greater variability in outcomes.

Nationality plays a crucial role, with Portuguese students showing high numbers of both dropouts (2159) and graduates (2155), reflecting their dominant presence in the dataset. Other nationalities have significantly lower figures, with 56 dropouts and 54 graduates, suggesting different educational dynamics or smaller population sizes.

Parental education appears influential. Students whose parents have secondary education show the highest numbers of both dropouts and graduates, pointing to the significant impact of secondary-level parental education. Higher parental education correlates with fewer dropouts but does not significantly boost graduate numbers.

Parental occupation also correlates with student outcomes. Students with blue-collar parents have higher dropout rates compared to those with white-collar parents. In particular, blue-collar fathers correlate with 1220 dropouts compared to 849 for white-collar fathers. The data suggests a connection between parental occupation and student success, with white-collar backgrounds associated with higher graduation rates.

Displacement status significantly affects outcomes. Displaced students have a much lower dropout rate (1113) and a higher graduation rate (1324) compared to their non-displaced peers, highlighting the effectiveness of support systems for displaced individuals.

Financial stability, indicated by up-to-date tuition fees and scholarship holding, plays a crucial role. Students with up-to-date tuition fees show higher graduation numbers (1810) compared to those with overdue fees (264 graduates). Scholarship holders, in particular, fare better with 1374 graduates compared to 1340 non-holders, and fewer dropouts (801 compared to 1507), underscoring the positive impact of financial aid on student retention and success.

Gender differences are evident, with female students showing higher numbers of both dropouts (1207) and graduates (1681) compared to males (1008 dropouts and 548 graduates). This indicates a larger female student population and a higher success rate among females, pointing to gender-specific factors that influence educational outcomes.

<div style="display: flex; flex-wrap: wrap;">

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Marital Status vs Output**
|        | Dropout | Graduate | Sum |
|--------|--------:|---------:|----:|
| Single |    1904 |     2015 | 3919 |
| Married|     231 |      148 |  379 |
| Others |      80 |       46 |  126 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Course vs Output**
|        | Dropout | Graduate | Sum |
|--------|--------:|---------:|----:|
| No Stem|    1393 |     1309 | 2702 |
| Stem   |     822 |      900 | 1722 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Previous Qualification vs Output**
|             | Dropout | Graduate | Sum |
|-------------|--------:|---------:|----:|
| Higher      |     124 |       80 |  204 |
| No Secondary|     169 |       63 |  232 |
| Secondary   |    1922 |     2066 | 3988 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Nationality vs Output**
|           | Dropout | Graduate | Sum |
|-----------|--------:|---------:|----:|
| Portuguese|    2159 |     2155 | 4314 |
| Others    |      56 |       54 |  110 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Mother's Qualification vs Output**
|             | Dropout | Graduate | Sum |
|-------------|--------:|---------:|----:|
| Higher      |     319 |      272 |  591 |
| No Secondary|     158 |       76 |  234 |
| Secondary   |    1738 |     1861 | 3599 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Father's Qualification vs Output**
|             | Dropout | Graduate | Sum |
|-------------|--------:|---------:|----:|
| Higher      |     237 |      178 |  415 |
| No Secondary|    1502 |     1574 | 3076 |
| Secondary   |     476 |      457 |  933 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Mother's Occupation vs Output**
|             | Dropout | Graduate | Sum |
|-------------|--------:|---------:|----:|
| Blue Collar |     961 |     1043 | 2004 |
| Others      |     166 |       65 |  231 |
| White Collar|    1088 |     1101 | 2189 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Father's Occupation vs Output**
|             | Dropout | Graduate | Sum |
|-------------|--------:|---------:|----:|
| Blue Collar |    1220 |     1347 | 2567 |
| Others      |     146 |       66 |  212 |
| White Collar|     849 |      796 | 1645 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Displaced vs Output**
|   | Dropout | Graduate | Sum |
|---|--------:|---------:|----:|
| 0 |    1113 |      885 | 1998 |
| 1 |    1102 |     1324 | 2426 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Educational Special Needs vs Output**
|   | Dropout | Graduate | Sum |
|---|--------:|---------:|----:|
| 0 |    2187 |     2186 | 4373 |
| 1 |      28 |       23 |   51 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Debtor vs Output**
|   | Dropout | Graduate | Sum |
|---|--------:|---------:|----:|
| 0 |    1813 |     2108 | 3921 |
| 1 |     402 |      101 |  503 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Tuition Fees Up-to-date vs Output**
|   | Dropout | Graduate | Sum |
|---|--------:|---------:|----:|
| 0 |     499 |       29 |  528 |
| 1 |    1716 |     2180 | 3896 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Gender vs Output**
|       | Dropout | Graduate | Sum |
|-------|--------:|---------:|----:|
| Female|    1207 |     1661 | 2868 |
| Male  |    1008 |      548 | 1556 |
</div>

<div style="flex: 1; min-width: 300px; padding: 10px;">
### **Scholarship Holder vs Output**
|   | Dropout | Graduate | Sum |
|---|--------:|---------:|----:|
| 0 |    1951 |     1374 | 3325 |
| 1 |     264 |      835 | 1099 |
</div>

</div>

```{r,include=FALSE}

library(knitr)

categorical_cols <- names(data)[sapply(data, is.factor)]

print_contingency_tables <- function(data, categorical_columns) {
  for (column in categorical_columns) {
    cat("\n Contingency Table for", column, "\n\n")
    contingency_table <- table(data[[column]])
    names(contingency_table) <- paste(column, names(contingency_table), sep = " = ")
    print(kable(contingency_table, caption = paste("Contingency Table for", column), align = 'c',col.names = c(column,"freq")))
  }
}

print_contingency_tables(data, categorical_cols)





```
```{r}

knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(htmltools)
```



### 4. Model building
We will now try to build different models using the training set,the models we will use include logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), ridge regression, and lasso regression.

This project encompasses a step-by-step approach to model implementation, evaluation, and comparison, utilizing libraries such as `MASS`, `glmnet`, and `class` for model fitting, prediction, and performance assessment.

In the following section we will build the models one by one and briefly talk about which insights tey offer, a more detailed discussion will be in the following section


**Logistic Regression**

One of the first model we employ is logistic regression, this seems like an obvious choice given the fact that our research question revolves around finding a probability in a binary classification task.

The core idea behind logistic regression is to model the relationship between one or more independent variables (features) and a binary dependent variable (outcome) using the logistic function. This function maps the output of a linear combination of the features to a probability score between 0 and 1.

In logistic regression, the coefficients associated with each feature are estimated using maximum likelihood estimation. These coefficients represent the impact of each feature on the log-odds of the outcome variable.
```{r,include=FALSE}
# ROC
library(pROC)
```


```{r,results='hide'}
######################################
# Logistic Regression
######################################
fit1 <- glm(Output ~ ., 
            family = binomial, 
            data = train)
summary(fit1)
fit2 <- step(fit1, 
             direction = 'backward')
summary(fit2)
```


| Variable             | Estimate | Std. Error | z value |Pr(>|z|) |
|----------------------|----------|------------|---------|----------|
| (Intercept)          | 3.38052  | 0.80612    | 4.194   | 2.75e-05 *** |
| Marital_Married      | 0.08176  | 0.46760    | 0.175   | 0.861201    |
| Marital_Others       | -1.74991 | 0.93198    | -1.878  | 0.060432 .  |
| Course_Stem          | 0.26065  | 0.09506    | 2.742   | 0.006106 ** |
| Prev_NoSecondary     | -13.54671| 235.71280  | -0.057  | 0.954170    |
| Prev_Secondary       | -0.09158 | 0.45666    | -0.201  | 0.841058    |
| Nationality_Others   | 0.46162  | 0.31499    | 1.466   | 0.142780    |
| Mother_NoSecondary   | -0.27751 | 0.26401    | -1.051  | 0.293192    |
| Mother_Secondary     | 0.16344  | 0.12751    | 1.282   | 0.199913    |
| Debtor               | -0.84311 | 0.18519    | -4.553  | 5.30e-06 *** |
| Tuition_Paid         | 2.28948  | 0.26585    | 8.612   | < 2e-16  *** |
| Gender_Male          | -0.65291 | 0.09854    | -6.626  | 3.45e-11 *** |
| Scholarship          | 1.21674  | 0.11197    | 10.867  | < 2e-16  *** |
| Age_Enrollment       | -0.31217 | 0.02527    | -12.352 | < 2e-16  *** |
| Unemployment_Rate    | 0.05150  | 0.01865    | 2.761   | 0.005769 ** |
| GDP                  | 0.08171  | 0.02177    | 3.754   | 0.000174 *** |

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



```{r}
par(mfrow = c(2, 2))
plot(fit2)
par(mfrow = c(1, 1))
```


```{r}

pred_logit <- predict(fit2, 
                      newdata = test, 
                      type = 'response')
roc_out_logit <- roc(test$Output ~ pred_logit, 
                     levels = c('Dropout', 'Graduate'))
plot(roc_out_logit,  
     print.auc=TRUE, 
     legacy.axes=TRUE, 
     xlab="False Positive Rate", 
     ylab="True Positive Rate",
     main="Logistic Regression")
auc(roc_out_logit)
```




***Linear Discriminant Analysis (LDA)***

LDA is a classic statistical technique utilized in machine learning and pattern recognition for classification tasks. We employ it to identify which linear combination of features best separates multiple classes or categories in our dataset.

At its core, LDA operates under the assumption that the data can be represented as multivariate Gaussian distributions and that the classes share the same covariance matrix. It seeks to find a projection of the data onto a lower-dimensional space while maximizing the separation between classes and minimizing the variance within each class.



```{r,results='hide'}
######################################
# Linear Discriminant Analysis (LDA)
######################################

library(MASS)
lda_fit <- lda(Output ~ ., 
               data = train)
lda_fit
```


```{r,eval=FALSE}
# plot the values of the discriminant function for the two groups
# 
plot(lda_fit, 
     type="histogram")
plot(lda_fit) # histogram is the default value
plot(lda_fit, type="density")
```


```{r,include=FALSE}
plot(lda_fit, type="both")


pred_lda <- predict(lda_fit, 
                    newdata = test, 
                    type = 'response')
roc_out_lda <- roc(test$Output ~ pred_lda$posterior[, 2], 
                   levels = c('Dropout', 'Graduate'))
```


```{r}
##auc(roc_out_lda)
plot(roc_out_lda,  
     print.auc=TRUE, 
     legacy.axes=TRUE, 
     xlab="False Positive Rate", 
     ylab="True Positive Rate", 
     main = 'LDA')


```

***Quadratic Discriminant Analysis (QDA)***

QDA is an extension of Linear Discriminant Analysis (LDA) used for classification tasks. While LDA assumes that different classes share the same covariance matrix, QDA relaxes this assumption, allowing each class to have its own covariance matrix.


```{r}
######################################
# Quadratic Discriminant Analysis (QDA)
######################################
qda_fit <- qda(Output ~ ., 
               data = train)
qda_fit
pred_qda <- predict(qda_fit, 
                    newdata = test, 
                    type = 'response')
roc_out_qda <- roc(test$Output ~ pred_qda$posterior[, 2], 
                   levels = c('Dropout', 'Graduate'))
plot(roc_out_qda,  
     print.auc=TRUE, 
     legacy.axes=TRUE, 
     xlab="False Positive Rate", 
     ylab="True Positive Rate", 
     main = 'QDA')
auc(roc_out_qda)



```

***Ridge Regression***

Ridge regression is a regularization technique used in linear regression to mitigate the issues of multicollinearity and overfitting: it can be used to work with high dimensional data, which is why we find it valuable in our case
```{r}
###########################
# Ridge Regression
###########################
```


```{r,results='hide'}
suppressWarnings(library(glmnet))
```


```{r}
design_matrix_train <- model.matrix(Output ~ .,
                                    data = train)

design_matrix_test <- model.matrix(Output ~ .,
                                   data = test)

# Adattamento del modello Ridge con regressione Binomiale
ridge_model <- glmnet(x = design_matrix_train, 
                      y = train$Output,
                      alpha = 0, 
                      intercept = F, 
                      family = "binomial")
# Find the best lambda

predictions <- predict(ridge_model, 
                       newx = design_matrix_test, 
                       type = 'response')
##########################

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$Output)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index <- which.max(accuracies)
ridge_pred <- matrix(predictions[, best_lambda_index], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index]

colnames(ridge_pred) <- 'ridge_pred'


roc_out_ridge <- roc(test$Output ~ as.numeric(ridge_pred), 
                     levels = c('Dropout', 'Graduate'))
plot(roc_out_ridge,  
     print.auc=TRUE, 
     legacy.axes=TRUE, 
     xlab="False Positive Rate", 
     ylab="True Positive Rate", 
     main = 'Ridge Regression')
auc(roc_out_ridge)

```

***Lasso Regression***

This regression techniques takes a slightly different approach by adding a penalty term that penalizes the absolute values of the regression coefficients, instead of their squares (which is what Rigde regression does).

This penalty term encourages sparsity in the coefficient vector, effectively driving some coefficients to exactly zero. As a result, Lasso regression not only helps in shrinking coefficient values but also performs variable selection by automatically excluding irrelevant features from the model.
```{r}
###########################
# Lasso Regression
###########################

# Adattamento del modello Lasso con regressione Binomiale
lasso_model <- glmnet(x = design_matrix_train, 
                      y = train$Output,
                      alpha = 1, 
                      intercept = F, 
                      family = "binomial")
# Find the best lambda

predictions <- predict(lasso_model, 
                       newx = design_matrix_test, 
                       type = 'response')
##########################

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$Output)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index <- which.max(accuracies)
lasso_pred <- matrix(predictions[, best_lambda_index], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index]

colnames(lasso_pred) <- 'lasso_pred'


roc_out_lasso <- roc(test$Output ~ as.numeric(lasso_pred), 
                     levels = c('Dropout', 'Graduate'))
plot(roc_out_lasso,  
     print.auc=TRUE, 
     legacy.axes=TRUE, 
     xlab="False Positive Rate", 
     ylab="True Positive Rate", 
     main = 'Lasso regression')
auc(roc_out_lasso)

##à DA vedere se lancia
# add labels to identify the variables
plot(ridge_model, xvar="lambda", label=TRUE)

plot(ridge_model, xvar = "norm", label=TRUE)
plot(ridge_model, xvar = "dev",  label=TRUE)

plot(ridge_model, xvar = 'norm')


```

### appendix A: Dataset

```{r echo=FALSE, message=FALSE}
# Load necessary library
library(knitr)

# Create a data frame that represents your dataset
original_data <- data.frame(
  Variable = c("Marital status", "Application mode", "Application order", "Course", 
               "evening attendance", "Displaced", "Educational special needs", 
               "Debtor", "Tuition fees up to date", "Gender", "Scholarship holder", 
               "Age at enrollment", "International", "Curricular units 1st sem (credited)", 
               "Curricular units 1st sem (enrolled)", "Curricular units 1st sem (evaluations)", 
               "Curricular units 1st sem (approved)", "Curricular units 1st sem (grade)", 
               "Curricular units 1st sem (without evaluations)", "Curricular units 2nd sem (credited)", 
               "Curricular units 2nd sem (enrolled)", "Curricular units 2nd sem (evaluations)", 
               "Curricular units 2nd sem (approved)", "Curricular units 2nd sem (grade)", 
               "Curricular units 2nd sem (without evaluations)", "Unemployment rate", 
               "Inflation rate", "GDP", "output", "Previous qualification",
               "Nationality", "Mother's qualification", "Father's qualification",
               "Mother's occupation", "Father's occupation"),
  Description = c("Categorical variable indicating the marital status of the individual",
                  "Categorical variable indicating the mode of application",
                  "Numeric variable indicating the order of application",
                  "Categorical variable indicating the chosen course",
                  "Binary variable indicating whether the individual attends classes during the daytime or evening",
                  "Binary variable indicating whether the individual has been displaced",
                  "Binary variable indicating whether the individual has educational special needs",
                  "Binary variable indicating whether the individual is a debtor",
                  "Binary variable indicating whether the tuition fees are up to date",
                  "Binary variable indicating the gender of the individual",
                  "Binary variable indicating whether the individual holds a scholarship",
                  "Numeric variable indicating the age of the individual at the time of enrollment",
                  "Binary variable indicating whether the individual is international",
                  "Numeric variable indicating the number of credited curricular units in the 1st semester",
                  "Numeric variable indicating the number of enrolled curricular units in the 1st semester",
                  "Numeric variable indicating the number of evaluations for curricular units in the 1st semester",
                  "Numeric variable indicating the number of approved curricular units in the 1st semester",
                  "Numeric variable indicating the average grade for curricular units in the 1st semester",
                  "Numeric variable indicating the number of curricular units in the 1st semester without evaluations",
                  "Numeric variable indicating the number of credited curricular units in the 2nd semester",
                  "Numeric variable indicating the number of enrolled curricular units in the 2nd semester",
                  "Numeric variable indicating the number of evaluations for curricular units in the 2nd semester",
                  "Numeric variable indicating the number of approved curricular units in the 2nd semester",
                  "Numeric variable indicating the average grade for curricular units in the 2nd semester",
                  "Numeric variable indicating the number of curricular units in the 2nd semester without evaluations",
                  "Variable indicating the unemployment rate (Unemployment rate (%))",
                  "Numeric variable indicating the inflation rate (Inflation rate (%))",
                  "Numeric variable indicating the Gross Domestic Product",
                  "Categorical variable indicating the target variable (e.g., Dropout, Graduate, Enrolled)",
                  "Numeric variable indicating the level of the previous qualification",
                  "Categorical variable indicating the nationality of the individual",
                  "Numeric variable indicating the level of the mother's qualification",
                  "Numeric variable indicating the level of the father's qualification",
                  "Categorical variable indicating the mother's occupation",
                  "Categorical variable indicating the father's occupation")
)

# Create the table using kable
kable(original_data, col.names = c("Variable", "Description"), align = c('l', 'l'), caption = "Dataset Description")
```
