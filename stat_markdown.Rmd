---
title: "stat_markdown"
output: pdf_document
date: "2024-06-11"
---

---
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
---

------------------------------------------------------------------------


### 1. Obtaining the Data

We originally found the dataset on Kaggle. The data was collected by the Polytechnic Institute of Portalegre in Portugal to build machine learning models that predict a student’s outcome based on various socioeconomic factors and academic performance. This was done to develop an analytics tool for the tutoring program to direct their efforts more effectively.

The dataset was created from several disjoint databases and includes students enrolled in different undergraduate degrees, such as agronomy, design, education, nursing, journalism, management, social service, and technologies, it consists of 4424 records with 35 features.

The prediction problem is formulated as a three-category classification task, which assesses, based on socio-economic data and performance metrics during the academic years, whether a student will:

-   Graduate within the three years of planned course activities (‘Graduate’)
-   Change course or stop studying altogether (‘Dropout’)
-   Fail to graduate in time (‘Enrolled’)

According to the literature in the field, there is no agreed-upon definition of what constitutes a dropout. In this work, the authors defined dropouts from a micro-perspective, considering field and institution changes as dropouts regardless of when they occur. This approach results in much higher dropout rates than the macro-perspective, which considers only students who leave the higher education system without a degree.

Given that the number of libraries we can use is limited to those covered during the course, we decided to employ this dataset to answer a different research question. Our analysis will focus on building a model to assign a probability score to new students. This score will quantify the likelihood that a student will finish their course within a three-year timeframe based on data collected at enrollment.

A detailed description of the original dataset can be found in Appendix A.

### Step 2. Data Preprocessing

Let's start by loading our data and changing the output column in order to represent our research question correctly: we are going to convert all the 'Enrolled' labels into 'Dropouts' since they did not manage to complete their course in the scheduled timeframe

```{r,results = 'hide'}
data <- read.csv("C:\\Users\\gorni\\OneDrive\\Desktop\\statistical learning\\student_data.csv", 
                 sep = ';')
str(data)

# Rename Colums
names(data)[names(data) == 'Nacionality'] <- 'Nationality'
table(data$Output)

# Remove 'Enrolled' to have a binomial problem
data$Output[which(data$Output == 'Enrolled')] <- 'Dropout'
```

```{r, include = F}
library(knitr)
library(kableExtra)
# Remove useless columns
# RICORDA CHE I DISPLACED SONO I FUORISEDE
data <- data[, -which(colnames(data) == 'Tuition.fees.up.to.date' | 
                        colnames(data) == 'Debtor' | 
                        colnames(data) == 'Unemployment.rate' |
                        colnames(data) == 'Inflation.rate' |
                        colnames(data) == 'GDP')]

# Remove Application and Curricular and evening data
data <- data[, -which(grepl('Application', colnames(data)) | 
                        grepl('Curricular', colnames(data)) |
                        grepl('evening', colnames(data)))]
### Collapse some categorical factor



```

Missing values:

there were no missing values in the dataset.

```{r}
sum(is.na(data))
```

**Data labeling**

We changed the label criteria for some of our variables in order to increase model explainability, in the following section we will discuss and showcase our changes.


```{r, echo = F}
## Marital status
#1 – single 2 – married 3 – widower 4-divorced 5 de-facto union 6–legally separated)



index_single <- which(data$Marital.status == 1) # Single
index_married <- which(data$Marital.status == 2) # Married
data$Marital.status[index_single] <- 'Single'
data$Marital.status[index_married] <- 'Married'
data$Marital.status[-c(index_single, index_married)] <- 'Others'
```

```{r, echo = F}
## Mother's occupation
# Split in white / blue collar
index_m_no_worker <- which(data$Mother.s.occupation == 1| # Student
                             data$Mother.s.occupation == 12 | # Other
                             data$Mother.s.occupation == 13) # Blanck
index_m_white <- which(data$Mother.s.occupation == 2 |
                         data$Mother.s.occupation == 3 |
                         data$Mother.s.occupation == 4 |
                         data$Mother.s.occupation == 5 |
                         data$Mother.s.occupation == 6 |
                         data$Mother.s.occupation == 14 | # Armed forces Officers
                         data$Mother.s.occupation == 15 | # Armed forces sergeants
                         data$Mother.s.occupation ==  17 |
                         data$Mother.s.occupation == 19 |
                         data$Mother.s.occupation == 20 |
                         data$Mother.s.occupation == 21 |
                         data$Mother.s.occupation == 22 |
                         data$Mother.s.occupation == 23 |
                         data$Mother.s.occupation == 24 |
                         data$Mother.s.occupation == 25 |
                         data$Mother.s.occupation == 26 |
                         data$Mother.s.occupation == 27 |
                         data$Mother.s.occupation == 28 |
                         data$Mother.s.occupation == 29)
data$Mother.s.occupation[index_m_no_worker] <- 'Others'
data$Mother.s.occupation[index_m_white] <- 'White Collar'
data$Mother.s.occupation[-c(index_m_no_worker, 
                            index_m_white)] <- 'Blue Collar'


## Father's occupation
# Slpit in white / blue collar
index_f_no_worker <- which(data$Father.s.occupation == 1| # Student
                             data$Father.s.occupation == 12 | #Other
                             data$Father.s.occupation == 13) # Blanck
index_f_white <- which(data$Father.s.occupation == 2 |
                         data$Father.s.occupation == 3 |
                         data$Father.s.occupation == 4 |
                         data$Father.s.occupation == 5 |
                         data$Father.s.occupation == 6 |
                         data$Father.s.occupation == 14 | # Armed forces Officers
                         data$Father.s.occupation == 15 | # Armed forces sergeants
                         data$Father.s.occupation ==  17 |
                         data$Father.s.occupation == 19 |
                         data$Father.s.occupation == 20 |
                         data$Father.s.occupation == 21 |
                         data$Father.s.occupation == 22 |
                         data$Father.s.occupation == 23 |
                         data$Father.s.occupation == 24 |
                         data$Father.s.occupation == 25 |
                         data$Father.s.occupation == 26 |
                         data$Father.s.occupation == 27 |
                         data$Father.s.occupation == 28 |
                         data$Father.s.occupation == 29)
data$Father.s.occupation[index_f_no_worker] <- 'Others'
data$Father.s.occupation[index_f_white] <- 'White Collar'
data$Father.s.occupation[-c(index_f_no_worker, 
                            index_f_white)] <- 'Blue Collar'

```

```{r, echo = F}
## Mother's qualification

index_m_secondary <- which(data$Mother.s.qualification == 1 | 
                             data$Mother.s.qualification == 6 |
                             data$Mother.s.qualification == 13 |
                             data$Mother.s.qualification == 15 |
                             data$Mother.s.qualification == 22 |
                             data$Mother.s.qualification == 23 |
                             data$Mother.s.qualification == 29 |
                             data$Mother.s.qualification == 31 |
                             data$Mother.s.qualification == 32)
index_m_higher <- which(data$Mother.s.qualification == 2 |
                          data$Mother.s.qualification == 3 |
                          data$Mother.s.qualification == 4 |
                          data$Mother.s.qualification == 5 |
                          data$Mother.s.qualification == 30 |
                          data$Mother.s.qualification == 33 |
                          data$Mother.s.qualification == 34)
data$Mother.s.qualification[index_m_secondary] <- 'Secondary'
data$Mother.s.qualification[index_m_higher] <- 'Higher'
data$Mother.s.qualification[-c(index_m_secondary, 
                               index_m_higher)] <- 'No Secondary'

```

```{r, include = F}

## Previous qualification
# Group on (no secondary education, secondary education, bachelor's degree, 
# master's degree, phd or highter)
index_secondary <- which(data$Previous.qualification == 1 | 
                           data$Previous.qualification == 6 |
                           data$Previous.qualification == 14 |
                           data$Previous.qualification == 16)
# Aggiungiamo '6' perchè se frequenti l'uni ti sei già diplomato e 
# se fai dei corsi da specializzando tecnico dovresti aver 
# fatto le superiori, però non valgono come la laurea

index_higher <- which(data$Previous.qualification == 2 |
                        data$Previous.qualification == 3 |
                        data$Previous.qualification == 15 |
                        data$Previous.qualification == 4 | # Since just 8
                        data$Previous.qualification == 5| # Since just 1 phd
                        data$Previous.qualification == 17)
data$Previous.qualification[index_secondary] <- 'Secondary'
data$Previous.qualification[index_higher] <- 'Higher'
data$Previous.qualification[-c(index_secondary, 
                               index_higher)] <- 'No Secondary'

# Gender
index_male <- which(data$Gender == 1)
data$Gender[index_male] <- 'Male'
data$Gender[-index_male] <- 'Female'



## Father's qualification
index_f_secondary <- which(data$Father.s.qualification == 1 | 
                             data$Father.s.qualification == 6 |
                             data$Father.s.qualification == 13 |
                             data$Father.s.qualification == 15 |
                             data$Father.s.qualification == 22 |
                             data$Father.s.qualification == 23 |
                             data$Father.s.qualification == 29 |
                             data$Father.s.qualification == 31 |
                             data$Father.s.qualification == 32)
index_f_higher <- which(data$Father.s.qualification == 2 |
                          data$Father.s.qualification == 3 |
                          data$Father.s.qualification == 4 |
                          data$Father.s.qualification == 5 |
                          data$Father.s.qualification == 30 |
                          data$Father.s.qualification == 33 |
                          data$Father.s.qualification == 34)
data$Father.s.qualification[index_f_secondary] <- 'Secondary'
data$Father.s.qualification[index_f_higher] <- 'Higher'
data$Father.s.qualification[-c(index_f_secondary, 
                               index_f_higher)] <- 'No Secondary'


```



```{r, echo = F}
## Course
index_stem <- which(data$Course == 1 |     # Biofuel Production Technologies
                      data$Course == 4 |   # Agronomy
                      data$Course == 6 |   # Veterinary Nursing
                      data$Course == 7 |   # Informatics Engineerin
                      data$Course == 8 |   # Equiniculture
                      data$Course == 12 |  # Nursing
                      data$Course == 13)   # Oral Hygiene
data$Course[index_stem] <- 'Stem'
data$Course[-index_stem] <- 'No Stem'

```

```{r, echo = F}

## Nationality

data$Nationality[data$Nationality != 1] <- 'Others' # just 2%
data$Nationality[data$Nationality == 1] <- 'Portoguese'

```

```{r, include = F}
# Change features as categorical
# colnames(data)
# categorical_names <- colnames(data)[-c(15, 17:19)]
# 
# for(i in categorical_names){
#   data[, i] <- as.factor(data[, i])
# }

# Change variable's levels
data$Marital.status <- factor(data$Marital.status, 
                              levels = c('Single', 'Married', 'Others'))
data$Course <- factor(data$Course, 
                      levels = c('Stem', 'No Stem'))
data$Previous.qualification <- factor(data$Previous.qualification, 
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Nationality <- factor(data$Nationality, 
                           levels = c('Portoguese', 'Others'))
data$Mother.s.qualification <- factor(data$Mother.s.qualification, 
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Father.s.qualification <- factor(data$Father.s.qualification,
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Mother.s.occupation <- factor(data$Mother.s.occupation,
                                   levels = c('Blue Collar', 'White Collar', 'Others'))
data$Father.s.occupation <- factor(data$Father.s.occupation,
                                   levels = c('Blue Collar', 'White Collar', 'Others'))
data$Gender <- factor(data$Gender,
                      levels = c('Female', 'Male'))
data$Output <- factor(data$Output,
                      levels = c('Dropout', 'Graduate'))


table(data$Nationality, data$International)
data <- data[, -which(colnames(data) == 'International')]
```

Marital status

Categorical variable indicating the marital status of the individual. We only have 4 widowers and 6 legally separated instances, therefore we collapsed them under the variable 'Others', we also decided to merge 'divorced' and 'legally separated' since the difference between the two instances is not relevant for our analysis.

```{r, echo = F}
kable_styling(kable(table(data$Marital.status),
                    caption = "Marital status",
                    col.names = c("Status", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)
```

Mother/Father occupation

Categorical variables indicating the mother and father occupation respectively, while the original dataset had 32 labels for all kinds of different jobs, we decided to merge some of the labels and make this a 3-label categorical variable. Jobs were split based on a White/Blue collar distinction, as shown in the table below.

```{r, echo = F}
t_occ <- data.frame(Occupation = levels(data$Mother.s.occupation),
                    Mother = matrix(table(data$Mother.s.occupation)), 
                    Father = matrix(table(data$Father.s.occupation)))
kable_styling(kable(t_occ,
                    caption = "Parent's occupation",
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)
```

Mother/Father/Student education

Categorical variable indicating the level of each parents’ qualification as well as the student's. Again, we are dealing with many categorical variables, so we decided to merge them based on whether they have an completed an higher education cycle and if they have finished high school or not

```{r, echo = F}
t_qual <- data.frame(Qualification = levels(data$Mother.s.qualification),
                     Mother = matrix(table(data$Mother.s.qualification)), 
                     Father = matrix(table(data$Father.s.qualification)), 
                     Student = matrix(table(data$Previous.qualification)))
kable_styling(kable(t_qual,
                    caption = "Qualification",
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)
```              


Courses

Categorical variable representing the course chosen at enrollment, we originally had 17 different courses, we decided to relabel the courses using whether they are STEM subjects or not as a splitting criterion

```{r, echo = F}
kable_styling(kable(table(data$Course),
                    caption = "Course",
                    col.names = c("Course", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)
```

Nationality:

categorical variable representing a students nationality: looking at the data we saw that while we had a lot of labels (one for each nationality), the vast majority of people were Portuguese, therefore we labeled data in the following way

```{r, echo = F}
kable_styling(kable(table(data$Nationality),
                    caption = "Nationality", 
                    col.names = c("Nationality", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position",
              full_width = F,
              font_size = 9)
```

**FEATURE REMOVAL**

Given that to build our model we are going to use only information that was present at enrollment time there are some features which are not useful and will therefore be removed.

1 Curricular data: we removed all information concerning student performance over the span of three years since it doesn't help us answer our research question

2 Application mode/ application order: the original paper didn't offer a clear indication about the various labels of this feature, furthermore we don't believe them to be of any interest as far as our research question goes.

3 Macroeconomics data (Inflation rate, GDP, Unemployment rate): these economic indicators were taken over the course of the three years data collection period, we also don't believe them to be of any use in answering our research question

4 Tuition fees up do date



### 3. Exploratory Data Analysis




```{r, echo = F}
library(knitr)
library(xtable)
t1 <- kable(addmargins(table(data$Marital.status, data$Output), 2), 
            format = "latex", booktabs = T)
t2 <- kable(addmargins(table(data$Course, data$Output), 2), 
            format = "latex", booktabs = T)
t3 <- kable(addmargins(table(data$Previous.qualification, data$Output), 2), 
            format = "latex", booktabs = T)
t4 <- kable(addmargins(table(data$Nationality, data$Output), 2),
            format = "latex", booktabs = T)
t5 <- kable(addmargins(table(data$Mother.s.qualification, data$Output), 2),
            format = "latex", booktabs = T)
t6 <- kable(addmargins(table(data$Father.s.qualification, data$Output), 2),
            format = "latex", booktabs = T)
t7 <- kable(addmargins(table(data$Mother.s.occupation, data$Output), 2),
            format = "latex", booktabs = T)
t8 <- kable(addmargins(table(data$Father.s.occupation, data$Output), 2),
            format = "latex", booktabs = T)
t9 <- kable(addmargins(table(data$Displaced, data$Output), 2),
            format = "latex", booktabs = T)
t10 <- kable(addmargins(table(data$Educational.special.needs, data$Output), 2),
            format = "latex", booktabs = T)
t11 <- kable(addmargins(table(data$Scholarship.holder, data$Output), 2),
             format = "latex", booktabs = T)
t12 <- kable(addmargins(table(data$Gender, data$Output), 2),
             format = "latex", booktabs = T)
# t12 <- kable(addmargins(table(data$Tuition.fees, data$Output), 2),
#              format = "latex", booktabs = T)


```
*AGGIUNGERE LINK TABELLE DI RIFERIMENTO*

To perform EDA we plot contingency tables for our categorical variables, looking at them gives us some insights: 
the majority of single people manages to graduate in time, the same can't be said for the other two categories.

People who enroll after completing secondary score tend to graduate in time more compared to oter categories (portalegre university allows some students to enroll courses without having having an high-school diploma).

Although there are more students enrolled in non-STEM courses (2,702 vs 1,722), the graduation rate is higher for those in STEM courses (52.3% vs 48.4%).

It's worth noting that mothers generally have a higher level of education compared to fathers. Specifically, 234 mothers versus 3,076 fathers did not complete high school. Interestingly, regardless of parents' educational background, students with parents who graduated from college have a lower graduation rate compared to those whose parents have lower levels of education.

There is no significant difference in graduation rates based on the parents' professions, except for those marked 'Other,' who have a lower on-time graduation rate.

Students who left their parents home to study appear (i.e. the 'displaced' variable) have a higher graduation rate compared to those who are not.

Students who receive a scholarship have a higher graduation rate than those who do not (76.0% vs 41.3%).

Finally, it is evident that women are significantly more likely than men to graduate on time (57.9% vs 35.2%).
add info about age at enrollment

*Da valutare di togliere le tabelle prevedenti e tenere solo queste che contengono le stesse informazioni più la differenza tra i due gruppi, e sopra fare solo una descrizione di come sono stati raggruppati i gruppi.*

```{r sample, echo = F, results = 'asis'}

# \\fontsize{18}{5}\\selectfont # Il primo {} è la dimensione del font, 
#  il secondo è lo spazio tra le righe
# \\fontsize{18}{5}\\selectfont # Il primo {} è la dimensione del font, 
#  il secondo è lo spazio tra le righe

cat(c("\\begin{table}[!htb]
    \\fontsize{9}{11}\\selectfont
    \\begin{minipage}{.5\\linewidth}
      \\caption{Marital status VS Output}
      \\centering",
        t1,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Previous qualification VS Output}",
        t3,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Course VS Output}
      \\centering",
        t2,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Nationality VS Output}",
        t4,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Mother's qualification VS Output}
      \\centering",
        t5,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Father's qualification VS Output}",
        t6,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Mother's occupation VS Output}
      \\centering",
        t7,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Father's occupation VS Output}",
        t8,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Displaced VS Output}
      \\centering",
        t9,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Education special VS Output}",
        t10,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Scholarship VS Output}
      \\centering",
        t11,
    "\\end{minipage}
        
        
\\end{table}"

))  
```


**OUTLIERS DETECTION AND REMOVAL**

We looked for outliers among numerical features in our data, 'age at enrollment' was the only one that presented some. Since it have not a normal distribution, we decided to remove all values which are outliers in the boxplot of values condictioned by the outputs. We then perform a train-test split before and remove the outliers from the training set only: keeping outliers in the test set improves the ecological validity of the model and reduces overfitting.

```{r, echo = F, fig.align = 'center', out.height = '40%'}
# RICORDA CHE I DISPLACED SONO I FUORISEDE
data <- data[, -which(colnames(data) == 'Tuition.fees.up.to.date' | 
                        colnames(data) == 'Debtor' | 
                        colnames(data) == 'Unemployment.rate' |
                        colnames(data) == 'Inflation.rate' |
                        colnames(data) == 'GDP')]


# Train test split
set.seed(71)
index <- sample(1:nrow(data), 
                round(0.75 * nrow(data)))
train <- data[index, ]
test <- data[-index, ]

# outliers removal
par(mfrow = c(1, 2))
boxplot(train$Age.at.enrollment ~ train$Output, 
        col = c('red', 'blue'),
        main = 'Age at enrollment',
        xlab = 'Output')
plot(density(train$Age.at.enrollment[train$Output == 'Graduate']), 
     col = 'blue',
     main = 'Age at enrollment', 
     xlab = 'Age at enrollment')
lines(density(train$Age.at.enrollment[train$Output == 'Dropout']), 
      col = 'red')
legend('topright', 
       legend = c('Graduate', 'Dropout'), 
       col = c('blue', 'red'), 
       lty = 1)
par(mfrow = c(1, 1))

```

```{r, echo = F, fig.show = 'hide'}
# Removing outliers
max_age_drop <- min(boxplot(train$Age.at.enrollment[train$Output == 'Dropout'])$out)
index_age_drop <- which(train$Age.at.enrollment >= max_age_drop &
                          train$Output == 'Dropout')
max_age_grad <- min(boxplot(train$Age.at.enrollment[train$Output == 'Graduate'])$out)
index_age_grad <- which(train$Age.at.enrollment >= max_age_grad &
                          train$Output == 'Graduate')
train <- train[-c(index_age_drop, 
                  index_age_grad), ]


```

```{r, echo = F, out.height = '30%', fig.align = 'center', height = 5}
# After removal
par(mfrow = c(1, 2))
boxplot(train$Age.at.enrollment ~ train$Output, 
        col = c('red', 'blue'),
        main = 'Age at enrollment',
        xlab = 'Output', 
        ylab = 'Age at enrollment')
plot(density(train$Age.at.enrollment[train$Output == 'Graduate']), 
     col = 'blue',
     main = 'Age at enrollment', 
     xlab = 'Age at enrollment')
lines(density(train$Age.at.enrollment[train$Output == 'Dropout']), 
      col = 'red')
legend('topright', 
       legend = c('Graduate', 'Dropout'), 
       col = c('blue', 'red'), 
       lty = 1)
par(mfrow = c(1, 1))
```

### 4. Model building
We will now try to build different models using the training set,the models we will use include logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), ridge regression, and lasso regression.

**Logistic Regression**

One of the first model we employ is logistic regression, this seems like an obvious choice given the fact that our research question revolves around finding a probability in a binary classification task.

The core idea behind logistic regression is to model the relationship between one or more independent variables (features) and a binary dependent variable (outcome) using the logistic function. This function maps the output of a linear combination of the features to a probability score between 0 and 1.

In logistic regression, the coefficients associated with each feature are estimated using maximum likelihood estimation. These coefficients represent the impact of each feature on the log-odds of the outcome variable.

```{r, echo = F, message = F}
library(MASS)

######################################
# Logistic Regression
######################################
fit_logit_1 <- glm(Output ~ .,
            family = binomial,
            data = train)
summary(fit_logit_1)
fit_logit_2 <- step(fit_logit_1)
summary(fit_logit_2)
# par(mfrow = c(2, 2))
# plot(fit_logit_2)
# par(mfrow = c(1, 1))

# Removing outliers
index_outliers <- which(abs(scale(fit_logit_2$residuals)) > 2)
train <- train[-index_outliers, ]
fit_logit_3 <- update(fit_logit_2, 
               .~.)
summary(fit_logit_3)

# Predictions
pred_logit <- predict(fit_logit_3, 
                      newdata = test, 
                      type = 'response')

# We want also see if there are some interesting interations between variables
fit_int_1 <- update(fit_logit_3, 
                          .~.*.)
summary(fit_int_1)
fit_int_2 <- step(fit_int_1)
summary(fit_int_2)

# Predictions
# Predictions
pred_int <- predict(fit_int_2, 
                      newdata = test, 
                      type = 'response')

```

In the Q-Q residuals plot, it is evident that the residuals are not normally distributed, suggesting that the variables included in the model are insufficient to explain the data variability. Additionally, the residuals vs leverage plot reveals the presence of points with very high leverage, indicating that there are still outliers in the dataset.




```{r, echo = F, out.width = '90%', out.height = '40%', fig.align = 'center'}
kable(round(summary(fit_logit_3)$coefficients, 3), 
      caption = 'Logistic regression coefficients',
      align = 'r',
      format = 'latex',
      booktabs = T, 
      font_size = 6)

par(mfrow = c(2, 2))
plot(fit_logit_3)
title('Logistic regression', outer = T, cex = 1.5, line = -2)

plot(fit_int_2)
title('Logistic regression with interactions', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))


```

***Linear Discriminant Analysis (LDA)***

LDA is a classic statistical technique utilized in machine learning and pattern recognition for classification tasks. We employ it to identify which linear combination of features best separates multiple classes or categories in our dataset.

At its core, LDA operates under the assumption that the data can be represented as multivariate Gaussian distributions and that the classes share the same covariance matrix. It seeks to find a projection of the data onto a lower-dimensional space while maximizing the separation between classes and minimizing the variance within each class.

```{r, echo = F, out.height = '30%'}
######################################
# Linear Discriminant Analysis (LDA)
######################################

fit_lda <- lda(Output ~ ., 
               data = train)
# Plot density of lda coefficients with respect to the output
plot(fit_lda, 
     type = 'both',
     main = 'LDA') # col not working


# kable_styling(kable((round(fit_lda$scaling, 3)), 
#       caption = 'LDA coefficients',
#       align = 'c',
#       format = 'latex',
#       booktabs = T), 
#       latex_options = c("hold_position", "scale_down"),
#       full_width = F,
#       position = "center",
#       font_size = 9)


# Predictions
pred_lda <- predict(fit_lda, 
                    newdata = test)$posterior[, 2]


```

***Quadratic Discriminant Analysis (QDA)***

QDA is an extension of Linear Discriminant Analysis (LDA) used for classification tasks. While LDA assumes that different classes share the same covariance matrix, QDA relaxes this assumption, allowing each class to have its own covariance matrix.

```{r, echo = F}
######################################
# Quadratic Discriminant Analysis (QDA)
######################################

fit_qda <- qda(Output ~ ., 
               data = train)

# kable_styling(kable((round(fit_qda$scaling, 3)),
#       caption = 'QDA coefficients',
#       align = 'c',
#       format = 'latex',
#       booktabs = T), 
#       latex_options = c("hold_position", "scale_down"),
#       full_width = F,
#       position = "right",
#       font_size = 9)

# Plot density of qda coefficients with respect to the output
pred_qda <- predict(fit_qda, 
                    newdata = test)$posterior[, 2]
detach('package:MASS')
```

***Ridge Regression***

Ridge regression is a regularization technique used in linear regression to mitigate the issues of multicollinearity and overfitting: it can be used to work with high dimensional data, which is why we find it valuable in our case
```{r, echo = F, message= F}
###########################
# Ridge Regression
###########################

library(glmnet)

design_matrix_train <- model.matrix(Output ~ .,
                                    data = train)

design_matrix_test <- model.matrix(Output ~ .,
                                   data = test)

# Creating a grid for lamda
grid <- 10^seq(2, -3, length = 100)

# Adattamento del modello Ridge con regressione Binomiale
ridge_model <- glmnet(x = design_matrix_train, 
                      y = train$Output,
                      alpha = 0, 
                      lambda = grid,
                      intercept = T, 
                      family = "binomial")
# Find the best lambda

predictions <- predict(ridge_model, 
                       newx = design_matrix_test, 
                       type = 'response')
##########################

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$Output)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index_r <- which.max(accuracies)
pred_ridge <- matrix(predictions[, best_lambda_index_r], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index_r]

colnames(pred_ridge) <- 'pred_ridge'
```

***Lasso Regression***

This regression techniques takes a slightly different approach by adding a penalty term that penalizes the absolute values of the regression coefficients, instead of their squares (which is what Rigde regression does).

This penalty term encourages sparsity in the coefficient vector, effectively driving some coefficients to exactly zero. As a result, Lasso regression not only helps in shrinking coefficient values but also performs variable selection by automatically excluding irrelevant features from the model.
```{r, echo = F}
###########################
# Lasso Regression
###########################


# Adattamento del modello Lasso con regressione Binomiale
lasso_model <- glmnet(x = design_matrix_train, 
                      y = train$Output,
                      alpha = 1, 
                      lambda = grid,
                      intercept = T, 
                      family = "binomial")
# Find the best lambda

predictions <- predict(lasso_model, 
                       newx = design_matrix_test, 
                       type = 'response')
##########################

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$Output)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index_l <- which.max(accuracies)
pred_lasso <- matrix(predictions[, best_lambda_index_l], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index_l]

colnames(pred_lasso) <- 'pred_lasso'
```
```{r, echo = F, fig.align = 'center'}
# Some fancy graphs about ridge model
par(mfrow = c(1, 2))
plot(ridge_model, xvar = 'lambda', label = T)
plot(ridge_model, xvar = 'dev', label = T)
title('Ridge model', outer = T, cex = 1.5, line = -2)

# Some fancy graphs about lasso model
plot(lasso_model, xvar = 'lambda', label = T)
plot(lasso_model, xvar = 'dev', label = T)
title('Lasso model', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```


### 5. Model Evaluation
we are going to evaluate a our models by plotting ROC curve and by computing some standard evaluation metrics: TALK ABOUT ROC ECC ECC HERE 


```{r, echo = F, message = F, warning = F}
# Create roc curve for all models
library(pROC)

# Logit
roc_out_logit <- roc(test$Output ~ pred_logit, 
                     levels = c('Dropout', 'Graduate'))

roc_out_int <- roc(test$Output ~ pred_int, 
                     levels = c('Dropout', 'Graduate'))

# LDA
roc_out_lda <- roc(test$Output ~ pred_lda, 
                   levels = c('Dropout', 'Graduate'))


# QDA
roc_out_qda <- roc(test$Output ~ pred_qda, 
                   levels = c('Dropout', 'Graduate'))


# Ridge
roc_out_ridge <- roc(test$Output ~ pred_ridge, 
                     levels = c('Dropout', 'Graduate'))

# Lasso
roc_out_lasso <- roc(test$Output ~ pred_lasso, 
                     levels = c('Dropout', 'Graduate'))

# Plotting the ROC curves and resize the plot on 'F Positive Rate' = c(1, 0)
plot(roc_out_logit, 
     col = 'red', 
     lwd = 2, 
     main = 'ROC curves', 
     xlab = 'F Positive Rate', 
     ylab = 'T Positive Rate')
lines(roc_out_int, 
      col = 'black', 
      lwd = 2)
lines(roc_out_lda, 
      col = 'blue', 
      lwd = 2)
lines(roc_out_qda,
      col = 'green', 
      lwd = 2)
lines(roc_out_ridge,
      col = 'purple', 
      lwd = 2)
lines(roc_out_lasso,
      col = 'orange', 
      lwd = 2)
legend('bottomright', 
       legend = c('Logit', 'Interaztion', 'LDA', 'QDA', 'Ridge', 'Lasso'), 
       col = c('red', 'black', 'blue', 'green', 'purple', 'orange'), 
       lty = 1)


```



```{r, echo = F}
# AUC
auc_logit <- round(auc(roc_out_logit), 3)
auc_int <- round(auc(roc_out_int), 3)
auc_lda <- round(auc(roc_out_lda), 3)
auc_qda <- round(auc(roc_out_qda), 3)
auc_ridge <- round(auc(roc_out_ridge), 3)
auc_lasso <- round(auc(roc_out_lasso), 3)

# Confusion matrix
confusion_matrix_logit <- addmargins(table(round(pred_logit), test$Output), 2)
confusion_matrix_int <- addmargins(table(round(pred_int), test$Output), 2)
confusion_matrix_lda <- addmargins(table(round(pred_lda), test$Output), 2)
confusion_matrix_qda <- addmargins(table(round(pred_qda), test$Output), 2)
confusion_matrix_ridge <- addmargins(table(round(pred_ridge), test$Output), 2)
confusion_matrix_lasso <- addmargins(table(round(pred_lasso), test$Output), 2)


# Accuracy
accuracy_logit <- sum(diag(confusion_matrix_logit)) / sum(confusion_matrix_logit[, -3])
accuracy_int <- sum(diag(confusion_matrix_int)) / sum(confusion_matrix_int[, -3])
accuracy_lda <- sum(diag(confusion_matrix_lda)) / sum(confusion_matrix_lda[, -3])
accuracy_qda <- sum(diag(confusion_matrix_qda)) / sum(confusion_matrix_qda[, -3])
accuracy_ridge <- sum(diag(confusion_matrix_ridge)) / sum(confusion_matrix_ridge[, -3])
accuracy_lasso <- sum(diag(confusion_matrix_lasso)) / sum(confusion_matrix_lasso[, -3])

# Precision
precision_logit <- confusion_matrix_logit[2, 2] / sum(confusion_matrix_logit[2, -3])
precision_int <- confusion_matrix_int[2, 2] / sum(confusion_matrix_int[2, -3])
precision_lda <- confusion_matrix_lda[2, 2] / sum(confusion_matrix_lda[2, -3])
precision_qda <- confusion_matrix_qda[2, 2] / sum(confusion_matrix_qda[2, -3])
precision_ridge <- confusion_matrix_ridge[2, 2] / sum(confusion_matrix_ridge[2, -3])
precision_lasso <- confusion_matrix_lasso[2, 2] / sum(confusion_matrix_lasso[2, -3])

# Recall
recall_logit <- confusion_matrix_logit[2, 2] / sum(confusion_matrix_logit[, 2])
recall_int <- confusion_matrix_int[2, 2] / sum(confusion_matrix_int[, 2])
recall_lda <- confusion_matrix_lda[2, 2] / sum(confusion_matrix_lda[, 2])
recall_qda <- confusion_matrix_qda[2, 2] / sum(confusion_matrix_qda[, 2])
recall_ridge <- confusion_matrix_ridge[2, 2] / sum(confusion_matrix_ridge[, 2])
recall_lasso <- confusion_matrix_lasso[2, 2] / sum(confusion_matrix_lasso[, 2])
  
# F1 score
f1_logit <- 2 * (precision_logit * recall_logit) / (precision_logit + recall_logit)
f1_int <- 2 * (precision_int * recall_int) / (precision_int + recall_int)
f1_lda <- 2 * (precision_lda * recall_lda) / (precision_lda + recall_lda)
f1_qda <- 2 * (precision_qda * recall_qda) / (precision_qda + recall_qda)
f1_ridge <- 2 * (precision_ridge * recall_ridge) / (precision_ridge + recall_ridge)
f1_lasso <- 2 * (precision_lasso * recall_lasso) / (precision_lasso + recall_lasso)

# Plot a matrix with all the results
results <- data.frame(Model = c('Logit', 'Interaction', 'LDA', 'QDA', 'Ridge', 'Lasso'),
                      AUC = round(c(auc_logit, auc_int, auc_lda, 
                                    auc_qda, auc_ridge, auc_lasso), 3),
                      Accuracy = round(c(accuracy_logit, accuracy_int, accuracy_lda, 
                                         accuracy_qda, accuracy_ridge, accuracy_lasso), 3),
                      Precision = round(c(precision_logit, precision_int, precision_lda, 
                                          precision_qda, precision_ridge, precision_lasso), 3),
                      Recall = round(c(recall_logit, recall_int, recall_lda, 
                                       recall_qda, recall_ridge, recall_lasso), 3),
                      F1 = round(c(f1_logit, f1_int, f1_lda, 
                                   f1_qda, f1_ridge, f1_lasso), 3))



kable_styling(kable(results,
      caption = 'Model evaluation',
      align = 'c',
      format = 'latex',
      booktabs = T), 
              latex_options = "hold_position", 
              full_width = F,
              # position = "center", 
              font_size = 9)
```

```{r, echo = F}
# coef_matrix <- matrix(0, 
#                       ncol = 4, 
#                       nrow = 25)
# coef_names <- names(coef(fit_logit_1))
# coef1 <- coef(fit_logit_1)
# coef2 <- coef(fit_logit_3)
# coef3 <- coef(ridge_model)[, best_lambda_index_r]
# coef4 <- coef(lasso_model)[, best_lambda_index_l]
# 
# coef_matrix[, 1] <- coef1
# for(i in 1:25){
#   if(coef_names[i] %in% names(coef2)){
#     coef_matrix[i, 2] <- coef2[which(names(coef2) == coef_names[i])]
#   }
# }
# coef_matrix[, 3] <- coef3[-2]
# coef_matrix[, 4] <- coef4[-2]
# 
# coef_df <- data.frame(Logit_full = coef_matrix[, 1],
#                       Logit_final = coef_matrix[, 2],
#                       Ridge = coef_matrix[, 3],
#                       Lasso = coef_matrix[, 4], 
#                       row.names = coef_names)
```

APPENDIX A

```{r echo=FALSE, message=FALSE}

# Load necessary library
library(knitr)

# Create a data frame that represents your dataset
original_data <- data.frame(
  Variable = c("Marital status", "Application mode", "Application order", "Course", 
               "evening attendance", "Displaced", "Educational special needs", 
               "Tuition fees up to date", "Gender", "Scholarship holder", 
               "Age at enrollment", "International", "Curricular units 1st sem (credited)", 
               "Curricular units 1st sem (enrolled)", "Curricular units 1st sem (evaluations)", 
               "Curricular units 1st sem (approved)", "Curricular units 1st sem (grade)", 
               "Curricular units 1st sem (without evaluations)", "Curricular units 2nd sem (credited)", 
               "Curricular units 2nd sem (enrolled)", "Curricular units 2nd sem (evaluations)", 
               "Curricular units 2nd sem (approved)", "Curricular units 2nd sem (grade)", 
               "Curricular units 2nd sem (without evaluations)", "Unemployment rate", 
               "Inflation rate", "GDP", "output", "Previous qualification",
               "Nationality", "Mother's qualification", "Father's qualification",
               "Mother's occupation", "Father's occupation"),
  Description = c("Categorical variable indicating the marital status of the individual",
                  "Categorical variable indicating the mode of application",
                  "Numeric variable indicating the order of application",
                  "Categorical variable indicating the chosen course",
                  "Binary variable indicating whether the individual attends classes during the daytime or evening",
                  "Binary variable indicating whether the individual has been displaced",
                  "Binary variable indicating whether the individual has educational special needs",
                  "Binary variable indicating whether the tuition fees are up to date",
                  "Binary variable indicating the gender of the individual",
                  "Binary variable indicating whether the individual holds a scholarship",
                  "Numeric variable indicating the age of the individual at the time of enrollment",
                  "Binary variable indicating whether the individual is international",
                  "Numeric variable indicating the number of credited curricular units in the 1st semester",
                  "Numeric variable indicating the number of enrolled curricular units in the 1st semester",
                  "Numeric variable indicating the number of evaluations for curricular units in the 1st semester",
                  "Numeric variable indicating the number of approved curricular units in the 1st semester",
                  "Numeric variable indicating the average grade for curricular units in the 1st semester",
                  "Numeric variable indicating the number of curricular units in the 1st semester without evaluations",
                  "Numeric variable indicating the number of credited curricular units in the 2nd semester",
                  "Numeric variable indicating the number of enrolled curricular units in the 2nd semester",
                  "Numeric variable indicating the number of evaluations for curricular units in the 2nd semester",
                  "Numeric variable indicating the number of approved curricular units in the 2nd semester",
                  "Numeric variable indicating the average grade for curricular units in the 2nd semester",
                  "Numeric variable indicating the number of curricular units in the 2nd semester without evaluations",
                  "Variable indicating the unemployment rate (Unemployment rate (%))",
                  "Numeric variable indicating the inflation rate (Inflation rate (%))",
                  "Numeric variable indicating the Gross Domestic Product",
                  "Categorical variable indicating the target variable (e.g., Dropout, Graduate, Enrolled)",
                  "Numeric variable indicating the level of the previous qualification",
                  "Categorical variable indicating the nationality of the individual",
                  "Numeric variable indicating the level of the mother's qualification",
                  "Numeric variable indicating the level of the father's qualification",
                  "Categorical variable indicating the mother's occupation",
                  "Categorical variable indicating the father's occupation")
)

# Create the table using kable
kable(original_data, col.names = c("Variable", "Description"), align = c('l', 'l'), caption = "Dataset Description")
```

```

