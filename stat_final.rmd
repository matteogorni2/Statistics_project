---
title: "Statistical learning final project"
date: "2024-07-11"
author: "Di Pasquale Giuseppe, Gorni Selvestrini Matteo"
output: 
    pdf_document:
      toc: true
      number_sections: true
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
---

------------------------------------------------------------------------

\newpage
# Introduction

The data was originally collected by the Polytechnic Institute of Portalegre in Portugal to build machine learning models that predict a student’s outcome based on various socioeconomic factors and academic performance. This was done to develop an analytics tool for the tutoring program to direct their efforts more effectively.

The dataset was created from several disjoint databases and includes students enrolled in different undergraduate degrees, such as agronomy, design, education, nursing, journalism, management, social service, and technologies, it consists of 4424 records with 35 features.

The prediction problem is formulated as a three-category classification task, which assesses, based on socio-economic data and performance metrics during the academic years, whether a student will:

-   Graduate within the three years of planned course activities (‘Graduate’)
-   Change course or stop studying altogether (‘Dropout’)
-   Fail to graduate in time (‘Enrolled’)

According to the literature in the field, there is no agreed-upon definition of what constitutes a dropout. In this work, the authors defined dropouts from a micro-perspective, considering field and institution changes as dropouts regardless of when they occur. This approach results in much higher dropout rates than the macro-perspective, which considers only students who leave the higher education system without a degree.

Given that the number of libraries we can use is limited to those covered during the course, we decided to employ this dataset to answer a different research question. Our analysis will focus on building a model to assign a probability score to new students. This score will quantify the likelihood that a student will finish their course within a three-year timeframe based on data collected at enrollment and evaluating how much it may effect the scholarship award.

A detailed description of the original dataset can be found in Appendix A.

# Data Preprocessing

Let's start by loading our data and changing the graduated column in order to represent our research question correctly: we are going to rename the 'Output' variable into 'is_Graduated' and we make it binary, 'Yes' labels will correspond to the old 'Graduate' labels, while all the 'Enrolled' and 'Dropout' labels will be converted into 'No'.

```{r, results = 'hide'}

data <- read.csv('~/Downloads/Dropout and Success/student_data.csv', 
                 sep = ';')
str(data)

# Rename Columns
names(data)[names(data) == 'Nacionality'] <- 'Nationality'
names(data)[names(data) == 'Marital.status'] <- 'Married'
names(data)[names(data) == 'Output'] <- 'is_Graduated'

# Rename 'Enrolled' to have a binomial problem
data$is_Graduated[which(data$is_Graduated == 'Enrolled' | 
                       data$is_Graduated == 'Dropout')] <- 'No'
data$is_Graduated[which(data$is_Graduated == 'Graduate')] <- 'Yes'

```

***Missing values and data balence check:***

There were no missing values and the dataset is balanced.

```{r}
sum(is.na(data))
table(data$is_Graduated)
```

## Relabelling variables

We changed the labeling criteria in some of our variables in order to increase model explainability, in the following section we will discuss and showcase our changes.


```{r, echo = F}

## Marital status
index_single <- which(data$Married == 1) # Single
# index_married <- which(data$Married == 2) # Married
data$Married[index_single] <- 'No'
# data$Married[index_married] <- 'Married'
# data$Married[-c(index_single, index_married)] <- 'Others'
data$Married[-index_single] <- 'Yes'

```

```{r, echo = F}

## Mother's occupation
# Split in white / blue collar
index_m_no_worker <- which(data$Mother.s.occupation == 1| # Student
                             data$Mother.s.occupation == 12 | # Other
                             data$Mother.s.occupation == 13) # Blanck
index_m_white <- which(data$Mother.s.occupation == 2 |
                         data$Mother.s.occupation == 3 |
                         data$Mother.s.occupation == 4 |
                         data$Mother.s.occupation == 5 |
                         data$Mother.s.occupation == 6 |
                         data$Mother.s.occupation == 14 | # Armed forces Officers
                         data$Mother.s.occupation == 15 | # Armed forces sergeants
                         data$Mother.s.occupation == 17 |
                         data$Mother.s.occupation == 19 |
                         data$Mother.s.occupation == 20 |
                         data$Mother.s.occupation == 21 |
                         data$Mother.s.occupation == 22 |
                         data$Mother.s.occupation == 23 |
                         data$Mother.s.occupation == 24 |
                         data$Mother.s.occupation == 25 |
                         data$Mother.s.occupation == 26 |
                         data$Mother.s.occupation == 27 |
                         data$Mother.s.occupation == 28 |
                         data$Mother.s.occupation == 29)
data$Mother.s.occupation[index_m_no_worker] <- 'Others'
data$Mother.s.occupation[index_m_white] <- 'White Collar'
data$Mother.s.occupation[-c(index_m_no_worker, 
                            index_m_white)] <- 'Blue Collar'


## Father's occupation
# Slpit in white / blue collar
index_f_no_worker <- which(data$Father.s.occupation == 1| # Student
                             data$Father.s.occupation == 12 | #Other
                             data$Father.s.occupation == 13) # Blanck
index_f_white <- which(data$Father.s.occupation == 2 |
                         data$Father.s.occupation == 3 |
                         data$Father.s.occupation == 4 |
                         data$Father.s.occupation == 5 |
                         data$Father.s.occupation == 6 |
                         data$Father.s.occupation == 14 | # Armed forces Officers
                         data$Father.s.occupation == 15 | # Armed forces sergeants
                         data$Father.s.occupation == 17 |
                         data$Father.s.occupation == 19 |
                         data$Father.s.occupation == 20 |
                         data$Father.s.occupation == 21 |
                         data$Father.s.occupation == 22 |
                         data$Father.s.occupation == 23 |
                         data$Father.s.occupation == 24 |
                         data$Father.s.occupation == 25 |
                         data$Father.s.occupation == 26 |
                         data$Father.s.occupation == 27 |
                         data$Father.s.occupation == 28 |
                         data$Father.s.occupation == 29)
data$Father.s.occupation[index_f_no_worker] <- 'Others'
data$Father.s.occupation[index_f_white] <- 'White Collar'
data$Father.s.occupation[-c(index_f_no_worker, 
                            index_f_white)] <- 'Blue Collar'

```

```{r, echo = F}

## Mother's qualification
index_m_secondary <- which(data$Mother.s.qualification == 1 | 
                             data$Mother.s.qualification == 6 |
                             data$Mother.s.qualification == 13 |
                             data$Mother.s.qualification == 15 |
                             data$Mother.s.qualification == 22 |
                             data$Mother.s.qualification == 23 |
                             data$Mother.s.qualification == 29 |
                             data$Mother.s.qualification == 31 |
                             data$Mother.s.qualification == 32)
index_m_higher <- which(data$Mother.s.qualification == 2 |
                          data$Mother.s.qualification == 3 |
                          data$Mother.s.qualification == 4 |
                          data$Mother.s.qualification == 5 |
                          data$Mother.s.qualification == 30 |
                          data$Mother.s.qualification == 33 |
                          data$Mother.s.qualification == 34)
data$Mother.s.qualification[index_m_secondary] <- 'Secondary'
data$Mother.s.qualification[index_m_higher] <- 'Higher'
data$Mother.s.qualification[-c(index_m_secondary, 
                               index_m_higher)] <- 'No Secondary'

```

```{r, include = F}

## Previous qualification
# Group on (no secondary education, secondary education, bachelor's degree, 
# master's degree, phd or highter)
index_secondary <- which(data$Previous.qualification == 1 | 
                           data$Previous.qualification == 6 |
                           data$Previous.qualification == 14 |
                           data$Previous.qualification == 16)
index_higher <- which(data$Previous.qualification == 2 |
                        data$Previous.qualification == 3 |
                        data$Previous.qualification == 15 |
                        data$Previous.qualification == 4 | # Since just 8
                        data$Previous.qualification == 5| # Since just 1 phd
                        data$Previous.qualification == 17)
data$Previous.qualification[index_secondary] <- 'Secondary'
data$Previous.qualification[index_higher] <- 'Higher'
data$Previous.qualification[-c(index_secondary, 
                               index_higher)] <- 'No Secondary'


## Gender
index_male <- which(data$Gender == 1)
data$Gender[index_male] <- 'Male'
data$Gender[-index_male] <- 'Female'


## Father's qualification
index_f_secondary <- which(data$Father.s.qualification == 1 | 
                             data$Father.s.qualification == 6 |
                             data$Father.s.qualification == 13 |
                             data$Father.s.qualification == 15 |
                             data$Father.s.qualification == 22 |
                             data$Father.s.qualification == 23 |
                             data$Father.s.qualification == 29 |
                             data$Father.s.qualification == 31 |
                             data$Father.s.qualification == 32)
index_f_higher <- which(data$Father.s.qualification == 2 |
                          data$Father.s.qualification == 3 |
                          data$Father.s.qualification == 4 |
                          data$Father.s.qualification == 5 |
                          data$Father.s.qualification == 30 |
                          data$Father.s.qualification == 33 |
                          data$Father.s.qualification == 34)
data$Father.s.qualification[index_f_secondary] <- 'Secondary'
data$Father.s.qualification[index_f_higher] <- 'Higher'
data$Father.s.qualification[-c(index_f_secondary, 
                               index_f_higher)] <- 'No Secondary'


```



```{r, echo = F}

## Course
index_stem <- which(data$Course == 1 |     # Biofuel Production Technologies
                      data$Course == 4 |   # Agronomy
                      data$Course == 6 |   # Veterinary Nursing
                      data$Course == 7 |   # Informatics Engineerin
                      data$Course == 8 |   # Equiniculture
                      data$Course == 12 |  # Nursing
                      data$Course == 13)   # Oral Hygiene
data$Course[index_stem] <- 'Stem'
data$Course[-index_stem] <- 'No Stem'

```

```{r, echo = F}

## Nationality
data$Nationality[data$Nationality != 1] <- 'Others' # just 2%
data$Nationality[data$Nationality == 1] <- 'Portoguese'

```

```{r, include = F}

# Change variable's levels
# data$Married <- factor(data$Married, 
#                               levels = c('Single', 'Married', 'Others'))
data$Married <- factor(data$Married, 
                              levels = c('No', 'Yes'))
data$Course <- factor(data$Course, 
                      levels = c('Stem', 'No Stem'))
data$Previous.qualification <- factor(data$Previous.qualification, 
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Nationality <- factor(data$Nationality, 
                           levels = c('Portoguese', 'Others'))
data$Mother.s.qualification <- factor(data$Mother.s.qualification, 
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Father.s.qualification <- factor(data$Father.s.qualification,
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Mother.s.occupation <- factor(data$Mother.s.occupation,
                                   levels = c('Blue Collar', 'White Collar', 'Others'))
data$Father.s.occupation <- factor(data$Father.s.occupation,
                                   levels = c('Blue Collar', 'White Collar', 'Others'))
data$Gender <- factor(data$Gender,
                      levels = c('Female', 'Male'))
data$is_Graduated <- factor(data$is_Graduated,
                      levels = c('No', 'Yes'))

```


***Marital status:***

Categorical variable with 6 values indicating the marital status of the individual.
Given that the majority of the vast majority of students is single and only a few of them belong to one of the other categories we decided to create a binary variable indicating if a student is single or not.

```{r, echo = F, message = F}
library(knitr)
library(xtable)
library(kableExtra)

kable_styling(kable(table(data$Married),
                    caption = "Marital status",
                    col.names = c("Status", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)

```

***Mother/Father occupation:***

Categorical variables indicating the mother and father occupation respectively, while the original dataset had 32 labels for all kinds of different jobs, we decided to merge some of the labels and make this a 3-label categorical variable. Jobs were split based on a White/Blue collar distinction, as shown in the table below.

```{r, echo = F}

t_occ <- data.frame(Occupation = levels(data$Mother.s.occupation),
                    Mother = matrix(table(data$Mother.s.occupation)), 
                    Father = matrix(table(data$Father.s.occupation)))
kable_styling(kable(t_occ,
                    caption = "Parental occupation",
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)

```

***Mother/Father/Student qualification:***

Categorical variable indicating the level of each parents’ qualification as well as the student's. Again, we are dealing with many categorical variables, so we decided to merge them based on whether they have an completed an higher education cycle and if they have finished high school or not.

```{r, echo = F}

t_qual <- data.frame(Qualification = levels(data$Mother.s.qualification),
                     Mother = matrix(table(data$Mother.s.qualification)), 
                     Father = matrix(table(data$Father.s.qualification)), 
                     Student = matrix(table(data$Previous.qualification)))
kable_styling(kable(t_qual,
                    caption = "Qualification",
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)

```              

***Course:***

Categorical variable representing the course chosen at enrollment, we originally had 17 different courses, we decided to relabel the courses based on whether they belong to the STEM area or not.

```{r, echo = F}

kable_styling(kable(table(data$Course),
                    caption = "Course",
                    col.names = c("Course", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)

```

***Nationality:***

Categorical variable representing a students nationality: looking at the data we saw that while we had a lot of labels (one for each nationality), the vast majority of people were Portuguese, therefore we labeled data in the following way

```{r, echo = F}

kable_styling(kable(table(data$Nationality),
                    caption = "Nationality", 
                    col.names = c("Nationality", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position",
              full_width = F,
              font_size = 9)

```

## Feature removal

Given that to build our model we are going to use only information that was present at enrollment time there are some features which are not useful and will therefore be removed.

- Curricular data: we removed all information concerning student performance over the span of three years since it doesn't help us answer our research question

- Application mode/ application order: the original paper didn't offer a clear indication about the various labels of this feature, furthermore we don't believe them to be of any interest as far as our research question goes.

- Macroeconomics data (Inflation rate, GDP, Unemployment rate): these economic indicators were taken over the course of the three years data collection period, we also don't believe them to be of any use in answering our research question.

- Tuition fees up do date.

```{r, echo = F, message = F}

# Remove useless columns
data <- data[, -which(grepl('Application', colnames(data)) | 
                        grepl('Tuition', colnames(data)) |
                        grepl('Debtor', colnames(data)) |
                        grepl('Unemployment', colnames(data)) |
                        grepl('Inflation', colnames(data)) |
                        grepl('GDP', colnames(data)) |
                        grepl('Curricular', colnames(data)) |
                        grepl('evening', colnames(data)))]

```

We also removed the 'International' variable since it was rendundant.
```{r}
table(data$Nationality, data$International)
data <- data[, -which(colnames(data) == 'International')]

```


# Exploratory Data Analysis


```{r tab_dio, echo = F}

t1  <- kable(addmargins(table(data$Married, data$is_Graduated), 2), 
            format = "latex", booktabs = T) 
t2 <- kable(addmargins(table(data$Course, data$is_Graduated), 2), 
            format = "latex", booktabs = T)
t3 <- kable(addmargins(table(data$Previous.qualification, data$is_Graduated), 2), 
            format = "latex", booktabs = T)
t4 <- kable(addmargins(table(data$Nationality, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t5 <- kable(addmargins(table(data$Mother.s.qualification, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t6 <- kable(addmargins(table(data$Father.s.qualification, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t7 <- kable(addmargins(table(data$Mother.s.occupation, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t8 <- kable(addmargins(table(data$Father.s.occupation, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t9 <- kable(addmargins(table(data$Displaced, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t10 <- kable(addmargins(table(data$Educational.special.needs, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t11 <- kable(addmargins(table(data$Scholarship.holder, data$is_Graduated), 2),
             format = "latex", booktabs = T)
t12 <- kable(addmargins(table(data$Gender, data$is_Graduated), 2),
             format = "latex", booktabs = T)

```



Most of our dataset's features are categorical, therefore we will plot the data in the form of contingency tables. This procedure allows us to make some interesting observations:

- Most people who enroll are single and at the same time are more likely to graduate than others (51.4% vs 38.4%); (table 6)

- People who enroll after completing secondary school tend to graduate in time more often than those who do not (note: Portalegre's university allows some students to attend courses without having an high-school diploma); (table 7)

- Although there are more students enrolled in non-STEM courses (2,702 vs 1,722), the graduation rate is higher for those in STEM courses (52.3% vs 48.4%); (table 8)

- Mothers generally have a higher level of education compared to fathers. Specifically, 3076 fathers do not have an high school diploma, the same can be said only for 234 mothers, this does not seem to affect graduation rates of their children; (table 10)


- Students who left their parents home to study (i.e. the 'displaced' variable) have a higher graduation rate compared to those who did not (54.6% vs 44.3%); (table 14)

- Students who receive a scholarship have a higher graduation rate than those who do not (76.0% vs 41.3%); (table 16)

- Women seem graduate on time much more than men do (57.9% vs 35.2%). (table 17)

- From the figures below we can see that 'Age at enrollment' have not a normal distribution, we can see that the distribution of the age at enrollment is different for students who graduate and those who do not. Both have an high right tail with more outliers, but the distribution of graduate on time is more concentrated around the mean value (20 years). 


```{r sample, echo = F, results = 'asis'}

cat(c("\\begin{table}[!htb]
    \\fontsize{9}{11}\\selectfont
    \\begin{minipage}{.5\\linewidth}
      \\caption{Marital status VS is Graduated}
      \\centering",
        t1,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Previous qualification VS is Graduated}",
        t3,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Course VS is Graduated}
      \\centering",
        t2,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Nationality VS is Graduated}",
        t4,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Mother's qualification VS is Graduated}
      \\centering",
        t5,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Father's qualification VS is Graduated}",
        t6,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Mother's occupation VS is Graduated}
      \\centering",
        t7,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Father's occupation VS is Graduated}",
        t8,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Displaced VS is Graduated}
      \\centering",
        t9,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Education special VS is Graduated}",
        t10,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Scholarship VS is Graduated}
      \\centering",
        t11,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Gender VS is Graduated}",
        t12,
    "\\end{minipage}
\\end{table}"
))  
```
```{r, echo = F}

par(mfrow = c(1, 2))
boxplot(data$Age.at.enrollment ~ data$is_Graduated, 
        col = c('red', 'blue'),
        # main = 'Age at enrollment',
        xlab = 'is_Graduated', 
        ylab = 'Age at enrollment')
plot(density(data$Age.at.enrollment[data$is_Graduated == 'Yes']), 
     col = 'blue',
     main = '',
     xlab = 'Age at enrollment')
lines(density(data$Age.at.enrollment[data$is_Graduated == 'No']), 
      col = 'red')
legend('topright', 
       legend = c('Yes', 'No'), 
       col = c('blue', 'red'), 
       lty = 1)
title('Age at enrollment', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

\newpage
## Outliers detection and removal

As we have seen in the previous section, the distribution of the age at enrollment contains a lot of outliers. We will now remove these outliers from the training set only, in order to preserve test set to simulate a real-world scenario. This will allow us to build a more robust model that generalizes better to unseen data. 


We can see that there are more outliers left, but we decided to keep them to stay more faithful to the original data distribution.

```{r, echo = F, fig.align = 'center', out.height = '40%', fig.show = 'hide'}

# Train test split
set.seed(71)
index <- sample(1:nrow(data), 
                round(0.75 * nrow(data)))
train <- data[index, ]
test <- data[-index, ]

# outliers removal
par(mfrow = c(1, 2))
boxplot(train$Age.at.enrollment ~ train$is_Graduated, 
        col = c('red', 'blue'),
        # main = 'Age at enrollment',
        xlab = 'is_Graduated', 
        ylab = 'Age at enrollment')
plot(density(train$Age.at.enrollment[train$is_Graduated == 'Yes']), 
     col = 'blue',
     main = '',
     xlab = 'Age at enrollment')
lines(density(train$Age.at.enrollment[train$is_Graduated == 'No']), 
      col = 'red')
legend('topright', 
       legend = c('Yes', 'No'), 
       col = c('blue', 'red'), 
       lty = 1)
title('Age at enrollment', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

```{r, echo = F, fig.show = 'hide'}

# Removing outliers
max_age_drop <- min(boxplot(train$Age.at.enrollment[train$is_Graduated == 'No'])$out)
index_age_drop <- which(train$Age.at.enrollment >= max_age_drop &
                          train$is_Graduated == 'No')
max_age_grad <- min(boxplot(train$Age.at.enrollment[train$is_Graduated == 'Yes'])$out)
index_age_grad <- which(train$Age.at.enrollment >= max_age_grad &
                          train$is_Graduated == 'Yes')
train <- train[-c(index_age_drop, 
                  index_age_grad), ]


```

```{r, echo = F, out.height = '30%', fig.align = 'center', height = 5}

# After removal
par(mfrow = c(1, 2))
boxplot(train$Age.at.enrollment ~ train$is_Graduated, 
        col = c('red', 'blue'),
        # main = 'Age at enrollment',
        xlab = 'is_Graduated', 
        ylab = 'Age at enrollment')
plot(density(train$Age.at.enrollment[train$is_Graduated == 'Yes']), 
     col = 'blue',
     main = '',
     xlab = 'Age at enrollment')
lines(density(train$Age.at.enrollment[train$is_Graduated == 'No']), 
      col = 'red')
legend('topright', 
       legend = c('Yes', 'No'), 
       col = c('blue', 'red'), 
       lty = 1)

title('Age at enrollment', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

# Model building

We will now try to build different models using the training set, the models we will use include logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), ridge regression and lasso regression. We will then evaluate the performance of these models using the test set working with the confusion matrix and the ROC curve and finally we compare the relative coefficients.

## Logistic Regression

We start our analysis by considering the *Logistic Regression model*, this seems like a very robust choice given the fact that our research question revolves around finding a probability in a binary classification task.

The core idea behind logistic regression is to model the relationship between one or more independent variables (features) and a binary dependent variable (outcome) using the logistic function. This function maps a linear combination of the features to a probability score between 0 and 1.

In logistic regression, the coefficients associated with each feature are estimated using maximum likelihood estimation. These coefficients represent the impact of each feature on the log-odds of the outcome variable.


```{r, echo = F}
######################################
# Logistic Regression
######################################
```

```{r, echo = T, message = F, results = 'hide', warning = F}

fit_logit_1 <- glm(is_Graduated ~ .,
            family = binomial,
            data = train)
summary(fit_logit_1)

# Keep only significant variables
fit_logit_2 <- step(fit_logit_1)
summary(fit_logit_2)

# Removing outliers
index_outliers <- which(abs(scale(fit_logit_2$residuals)) > 2)
train <- train[-index_outliers, ]

```

```{r, echo = T}

# Final model
fit_logit_3 <- update(fit_logit_2, 
               .~.)
summary(fit_logit_3)
```

One very important feature of logistic regression is that it is possible to look is certain features have a significant influence over the outcome variable. Looking at the model we just built we can see that many of the predictions we made during exploratory data analysis hold true:

- being in a non-STEM course is associated with a decrease of 0.50 in the log-odds of graduating, compared to being in a STEM course (reference group);

- having a mother with secondary education is associated with an increase of 0.96 in the log-odds of graduating compared to having a mother with no education (reference group);

- having a mother with higher education is associated with an increase of 0.84 in the log-odds of graduating compared to having a mother with no education (reference group);

- being male is associated with a decrease of 0.94 in the log-odds of graduating compared to being female (reference group);

- being a scholarship holder is associated with a decrease of 2.18 in the log-odds of graduating compared to not being a scholarship holder;

- each additional year of age at enrollment is associated with a decrease of 0.37 in the log-odds of graduating.


```{r, echo = F, message = F, results = 'hide', warning = F}

# Predictions
pred_logit <- predict(fit_logit_3, 
                      newdata = test, 
                      type = 'response')

```


### Logistic Regression with interactions

In logistic regression, interactions occur when the effect of one predictor variable on the outcome depends on the value of another independent variable. We will now try to build a logistic regression model with interactions to see if we can improve the model's performance using only significant variables find previously.

```{r, echo = T, message = F, results = 'hide', warning = F}
fit_int_1 <- update(fit_logit_3, 
                          .~.*.)
summary(fit_int_1)
fit_int_2 <- step(fit_int_1)
```
```{r, echo = T}
summary(fit_int_2)
```


```{r, echo = F}
# Predictions
pred_int <- predict(fit_int_2, 
                      newdata = test, 
                      type = 'response')

```


We find that the single variables are like the simple model, but from the significant interactions we have that:

- being male in a non-STEM course is associated with an increase of 0.63 in the log-odds of graduating;

- having a scholarship and a mother with secondary or higher education is associated with an increase of, respectively, 1.36 and 1.67 in the log-odds of graduating;

- being male with a scholarship decrease the log-odds of graduating by 0.86;

- being a scholarship holder decreases log-odds of graduating by 0.12 for each year of age at enrollment time. Recalling that the coefficient for scholarship holder is 3.64 it means that if you enroll at 20 years old and you are a scholarship holder, so the log-odds of graduating are 3.64 - 0.12 * 20 = 1.24 compared to those who did not get the scholarship.


We then plot residuals for both models in order to gain even more insights about the data we are working with: by looking Q-Q residuals plot, it is evident that the residuals are not normally distributed, suggesting that the variables included in the model are not sufficient to explain the data variability.

Furthermore, the residuals vs leverage plot reveals the presence of points with very high leverage, indicating that there are still outliers in the dataset.

The plots for the logistic regression with interactions are more similar to the ones of the logistic regression, suggesting that even if the variables are significant they do not particularly improve the model's performance. As we can see from the ANCOVA test, it is significant, but not improve particularly the residual deviance (from 2694 to 2670).


```{r, echo = F, out.width = '90%', out.height = '40%', fig.align = 'center'}

# kable(round(summary(fit_logit_3)$coefficients, 3), 
#       caption = 'Logistic regression coefficients',
#       align = 'r',
#       format = 'latex',
#       booktabs = T, 
#       font_size = 6)

par(mfrow = c(2, 2))
plot(fit_logit_3)
title('Logistic regression', outer = T, cex = 1.5, line = -2)

plot(fit_int_2)
title('Logistic regression with interactions', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

```{r}
anova(fit_logit_3, fit_int_2)
```

As we have assumed previously, from the two confusion matrices below (table 18 and table 19), we can see that there are no much difference between the two models, the only little difference is that the model with interactions predict just a little more zero's than the simple model.


```{r, echo = F}
# Confusion matrix

cm_logit <- kable(table(test$is_Graduated, round(pred_logit)),
              format = 'latex',
              booktabs = T)

cm_int <- kable(table(test$is_Graduated, round(pred_int)),
                format = 'latex',
                booktabs = T) 

```




```{r, echo = F, results = 'asis'}

cat(c("\\begin{table}[!htb]
    \\fontsize{9}{11}\\selectfont
    \\begin{minipage}{.5\\linewidth}
      \\caption{Test vs Logistic predictions}
      \\centering",
      cm_logit,
      "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Test vs Logistic with interactions predictions}", 
      cm_int,
      "\\end{minipage}
\\end{table}"
))

```

\newpage
## Linear Discriminant Analysis (LDA)

LDA is a classic statistical technique utilized in machine learning and pattern recognition for classification tasks. We employ it to identify which linear combination of features best separates multiple classes or categories in our dataset.

At its core, LDA operates under the assumption that the data can be represented as multivariate Gaussian distributions and that the classes share the same covariance matrix. It seeks to find a projection of the data onto a lower-dimensional space while maximizing the separation between classes and minimizing the variance within each class. 
Even if the assumptions of LDA are not met, it is still possible to apply it when the variables are discrete. In such cases, LDA can still provide good separation between classes, especially if the differences between classes are sufficiently pronounced. Although the optimal performance of LDA is achieved when its assumptions are satisfied, it can still be a useful tool for exploring the data and gaining preliminary insights into the structure of the classes, even when the assumptions are violated.

From now on we will use only confusion matrix on test set to evaluate the models, we will comparate all the models at the end.

DA VEDERE SE CAMBIARE
ADESSO CI CONCENTRIAMO SOLO SUI RISULTATI PIÙ IMPORTANTI, QUALI LE CONFUSION MATRIX E LE ALTRE COSE...
I COEFFICIENTI DELLE VARIABILI VERRANNO CONFRONTATI ALLA FINE CON GLI ALTRI MODELLI
```{r, echo = F}
######################################
# Linear Discriminant Analysis (LDA)
######################################
```

```{r, echo = T, message = F}
library(MASS)

fit_lda <- lda(is_Graduated ~ ., 
               data = train)
```

We can observe the density of the LDA results respect to the "is_Graduated" variable on train set in the plot below. The coefficients represent the weights of each feature in the linear combination that best separates the classes, suggesting that the features included in the model are useful for predicting the outcome variable, although it is not linearly separable.

```{r, echo = F, message = F, out.height = '30%', fig.align = 'center'}
# Plot density of lda results with respect to the is_Graduated
plot(fit_lda, 
     type = 'both',
     main = 'LDA') # col not working


# kable_styling(kable((round((fit_lda$scaling), 3)),
#       caption = 'LDA coefficients',
#       align = 'c',
#       format = 'latex',
#       booktabs = T),
#       latex_options = c("hold_position", "scale_down"),
#       full_width = F,
#       position = "center",
#       font_size = 9)

# Predictions
pred_lda <- predict(fit_lda, 
                    newdata = test)$posterior[, 2]

```

## Quadratic Discriminant Analysis (QDA)

QDA is an extension of Linear Discriminant Analysis (LDA) used for classification tasks. While LDA assumes that different classes share the same covariance matrix, QDA relaxes this assumption, allowing each class to have its own covariance matrix.

```{r, echo = F}

######################################
# Quadratic Discriminant Analysis (QDA)
######################################

```
```{r, echo = T, message = F}
fit_qda <- qda(is_Graduated ~ ., 
               data = train)

```

```{r, echo = F}
# Plot density of qda coefficients with respect to the is_Graduated
pred_qda <- predict(fit_qda, 
                    newdata = test)$posterior[, 2]
detach('package:MASS')
```

Differently from LDA, QDA does not plot the density of the results, but we can still see the probability of the train set to be graduated on time in the tables below. We can see that QDA is really able to predict correctly who graduated on time, but it have not a good performance for predicting who did not graduate on time (89% vs 48%).

```{r, echo = F}
index_yes <- which(train$is_Graduated == 'Yes')
pred_train <- predict(fit_qda)$posterior[, 2]
kable_styling(kable(t(round(prop.table(table(round((1-pred_train[-index_yes]), 1))), 2)),
      caption = 'Distribution of the results respect to NOT graduating on time in the train set',
      align = 'c',
      format = 'latex',
      booktabs = T),
      latex_options = c("hold_position", "scale_down"),
      full_width = F,
      position = "center",
      font_size = 9)

kable_styling(kable(t(round(prop.table(table(round((pred_train[index_yes]), 1))), 2)),
      caption = 'Distribution of the results respect to graduating on time in the train set',
      align = 'c',
      format = 'latex',
      booktabs = T),
      latex_options = c("hold_position", "scale_down"),
      full_width = F,
      position = "center",
      font_size = 9)
```

We see that the LDA model is more balanced in predicting who graduated on time and who did not, unlike the QDA model which is very prone to graduate on time (table 22 and table 23).


```{r, echo = F}

cm_lda <- kable(table(test$is_Graduated, round(pred_lda)),
              format = 'latex',
              booktabs = T)

cm_qda <- kable(table(test$is_Graduated, round(pred_qda)),
                format = 'latex',
                booktabs = T) 

```

```{r, echo = F, results = 'asis'}

cat(c("\\begin{table}[!htb]
    \\fontsize{9}{11}\\selectfont
    \\begin{minipage}{.5\\linewidth}
      \\caption{Test vs LDA predictions}
      \\centering",
      cm_lda,
      "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Test vs QDA predictions}", 
      cm_qda,
      "\\end{minipage}
\\end{table}"
))

```

## Ridge Regression

Ridge regression is a regularization technique used in linear regression to mitigate the issues of multicollinearity and overfitting: it can be used to work with high dimensional data, which is why we find it valuable in our case.

```{r, echo = F, message = F}

###########################
# Ridge Regression
###########################
```
```{r, echo = T, message = F}
library(glmnet)

design_matrix_train <- model.matrix(is_Graduated ~ .,
                                    data = train)[, -1]

design_matrix_test <- model.matrix(is_Graduated ~ .,
                                   data = test)[, -1]

# Creating a grid for lamda
grid <- 10^seq(2, -3, length = 100)

# Ridge Regression with Binomial distribution
ridge_model <- glmnet(x = design_matrix_train, 
                      y = train$is_Graduated,
                      alpha = 0, 
                      lambda = grid,
                      intercept = T, 
                      family = "binomial")

# Find the best lambda
predictions <- predict(ridge_model, 
                       newx = design_matrix_test, 
                       type = 'response')

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$is_Graduated)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index_r <- which.max(accuracies)
pred_ridge <- matrix(predictions[, best_lambda_index_r], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index_r]

colnames(pred_ridge) <- 'pred_ridge'

```

In the figures below we can see the evolution of the coefficients of the ridge model with respect to the lambda value, left figure. We can see that the coefficients are shrinking to zero as the lambda value increases, this is a clear sign that the model is working correctly and that the regularization is working as expected. The right figure show the evolution of the deviance of the model with respect to coefficients value. We can also see that with the ridge model the coefficients can be change direction over the lambda value.

```{r, echo = F, fig.align = 'center', warning = F}

par(mfrow = c(1, 2))
# Some fancy graphs about ridge model
plot(ridge_model, xvar = 'lambda', label = T)
plot(ridge_model, xvar = 'dev', label = T)
title('Ridge model', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

## Lasso Regression

This regression techniques takes a slightly different approach by adding a penalty term that penalizes the absolute values of the regression coefficients, instead of their squares (which is what Rigde regression does).

This penalty term encourages sparsity in the coefficient vector, effectively driving some coefficients to exactly zero. As a result, Lasso regression not only helps in shrinking coefficient values but also performs variable selection by automatically excluding irrelevant features from the model.

```{r, echo = F}

###########################
# Lasso Regression
###########################
```
```{r, echo = T, message = F}
# Lasso Regression with Binomial distribution
lasso_model <- glmnet(x = design_matrix_train, 
                      y = train$is_Graduated,
                      alpha = 1, 
                      lambda = grid,
                      intercept = T, 
                      family = "binomial")

# Find the best lambda
predictions <- predict(lasso_model, 
                       newx = design_matrix_test, 
                       type = 'response')

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$is_Graduated)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index_l <- which.max(accuracies)
pred_lasso <- matrix(predictions[, best_lambda_index_l], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index_l]

colnames(pred_lasso) <- 'pred_lasso'

```

Like previously, we can see the evolution of the coefficients of the lasso model with respect to the lambda value, left figure. We can see that the coefficients are shrinking to zero as the lambda value increases, this is a clear sign that the model is working correctly and that the regularization is working as expected. The right figure shows the deviance of the model with respect to the lambda value, we can see that the deviance is decreasing as the lambda value increases, this is a clear sign that the model is working correctly and that the regularization is working as expected. However, differently from the ridge model, the coefficients difficultly change direction over the lambda value and they collapse to 0 more quickly than ridge model.

```{r, echo = F, fig.align = 'center', warning = F}

par(mfrow = c(1, 2))
# Some fancy graphs about lasso model
plot(lasso_model, xvar = 'lambda', label = T)
plot(lasso_model, xvar = 'dev', label = T)
title('Lasso model', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

From the confusion matrices below (table 24 and table 25), we can see that the ridge model predictions are more similar to the lasso model predictions, likely logistic models.

```{r, echo = F}

cm_ridge <- kable(table(test$is_Graduated, round(pred_ridge)),
              format = 'latex',
              booktabs = T)

cm_lasso <- kable(table(test$is_Graduated, round(pred_lasso)),
                format = 'latex',
                booktabs = T) 

```

```{r, echo = F, results = 'asis'}

cat(c("\\begin{table}[!htb]
    \\fontsize{9}{11}\\selectfont
    \\begin{minipage}{.5\\linewidth}
      \\caption{Test vs LDA predictions}
      \\centering",
      cm_ridge,
      "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Test vs QDA predictions}", 
      cm_lasso,
      "\\end{minipage}
\\end{table}"
))

```

# Model Evaluation

We intend to evaluate our models by plotting their respective ROC curves and computing some standard evaluation metrics, such as precision, accuracy, recall and f1 score.

ROC (Receiver Operating Characteristic) curves are graphical representations used to evaluate the performance of binary classification models. The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.


```{r, echo = F, message = F, warning = F}

# Create roc curve for all models
library(pROC)

# Logit
roc_out_logit <- roc(test$is_Graduated ~ pred_logit, 
                     levels = c('No', 'Yes'))

# Logit with interactions
roc_out_int <- roc(test$is_Graduated ~ pred_int, 
                     levels = c('No', 'Yes'))

# LDA
roc_out_lda <- roc(test$is_Graduated ~ pred_lda, 
                   levels = c('No', 'Yes'))

# QDA
roc_out_qda <- roc(test$is_Graduated ~ pred_qda, 
                   levels = c('No', 'Yes'))

# Ridge
roc_out_ridge <- roc(test$is_Graduated ~ pred_ridge, 
                     levels = c('No', 'Yes'))

# Lasso
roc_out_lasso <- roc(test$is_Graduated ~ pred_lasso, 
                     levels = c('No', 'Yes'))

# Plotting the ROC curves and resize the plot on 'F Positive Rate' = c(1, 0)
plot(roc_out_logit, 
     col = 'red', 
     lwd = 2, 
     main = 'ROC curves', 
     xlab = '1 - F Positive Rate', 
     ylab = 'T Positive Rate')
lines(roc_out_int, 
      col = 'black', 
      lwd = 2)
lines(roc_out_lda, 
      col = 'blue', 
      lwd = 2)
lines(roc_out_qda,
      col = 'green', 
      lwd = 2)
lines(roc_out_ridge,
      col = 'purple', 
      lwd = 2)
lines(roc_out_lasso,
      col = 'orange', 
      lwd = 2)
legend('bottomright', 
       legend = c('Logit', 'Interaction', 'LDA', 'QDA', 'Ridge', 'Lasso'), 
       col = c('red', 'black', 'blue', 'green', 'purple', 'orange'), 
       lty = 1)


```

```{r, echo = F}

# AUC
auc_logit <- round(auc(roc_out_logit), 3)
auc_int <- round(auc(roc_out_int), 3)
auc_lda <- round(auc(roc_out_lda), 3)
auc_qda <- round(auc(roc_out_qda), 3)
auc_ridge <- round(auc(roc_out_ridge), 3)
auc_lasso <- round(auc(roc_out_lasso), 3)

# Confusion matrix
confusion_matrix_logit <- addmargins(table(round(pred_logit), test$is_Graduated), 2)
confusion_matrix_int <- addmargins(table(round(pred_int), test$is_Graduated), 2)
confusion_matrix_lda <- addmargins(table(round(pred_lda), test$is_Graduated), 2)
confusion_matrix_qda <- addmargins(table(round(pred_qda), test$is_Graduated), 2)
confusion_matrix_ridge <- addmargins(table(round(pred_ridge), test$is_Graduated), 2)
confusion_matrix_lasso <- addmargins(table(round(pred_lasso), test$is_Graduated), 2)

# Accuracy
accuracy_logit <- sum(diag(confusion_matrix_logit)) / sum(confusion_matrix_logit[, -3])
accuracy_int <- sum(diag(confusion_matrix_int)) / sum(confusion_matrix_int[, -3])
accuracy_lda <- sum(diag(confusion_matrix_lda)) / sum(confusion_matrix_lda[, -3])
accuracy_qda <- sum(diag(confusion_matrix_qda)) / sum(confusion_matrix_qda[, -3])
accuracy_ridge <- sum(diag(confusion_matrix_ridge)) / sum(confusion_matrix_ridge[, -3])
accuracy_lasso <- sum(diag(confusion_matrix_lasso)) / sum(confusion_matrix_lasso[, -3])

# Precision
precision_logit <- confusion_matrix_logit[2, 2] / sum(confusion_matrix_logit[2, -3])
precision_int <- confusion_matrix_int[2, 2] / sum(confusion_matrix_int[2, -3])
precision_lda <- confusion_matrix_lda[2, 2] / sum(confusion_matrix_lda[2, -3])
precision_qda <- confusion_matrix_qda[2, 2] / sum(confusion_matrix_qda[2, -3])
precision_ridge <- confusion_matrix_ridge[2, 2] / sum(confusion_matrix_ridge[2, -3])
precision_lasso <- confusion_matrix_lasso[2, 2] / sum(confusion_matrix_lasso[2, -3])

# Recall
recall_logit <- confusion_matrix_logit[2, 2] / sum(confusion_matrix_logit[, 2])
recall_int <- confusion_matrix_int[2, 2] / sum(confusion_matrix_int[, 2])
recall_lda <- confusion_matrix_lda[2, 2] / sum(confusion_matrix_lda[, 2])
recall_qda <- confusion_matrix_qda[2, 2] / sum(confusion_matrix_qda[, 2])
recall_ridge <- confusion_matrix_ridge[2, 2] / sum(confusion_matrix_ridge[, 2])
recall_lasso <- confusion_matrix_lasso[2, 2] / sum(confusion_matrix_lasso[, 2])
  
# F1 score
f1_logit <- 2 * (precision_logit * recall_logit) / (precision_logit + recall_logit)
f1_int <- 2 * (precision_int * recall_int) / (precision_int + recall_int)
f1_lda <- 2 * (precision_lda * recall_lda) / (precision_lda + recall_lda)
f1_qda <- 2 * (precision_qda * recall_qda) / (precision_qda + recall_qda)
f1_ridge <- 2 * (precision_ridge * recall_ridge) / (precision_ridge + recall_ridge)
f1_lasso <- 2 * (precision_lasso * recall_lasso) / (precision_lasso + recall_lasso)

# Plot a matrix with all the results
results <- data.frame(Model = c('Logit', 'Interaction', 'LDA', 'QDA', 'Ridge', 'Lasso'),
                      AUC = round(c(auc_logit, auc_int, auc_lda, 
                                    auc_qda, auc_ridge, auc_lasso), 3),
                      Accuracy = round(c(accuracy_logit, accuracy_int, accuracy_lda, 
                                         accuracy_qda, accuracy_ridge, accuracy_lasso), 3),
                      Precision = round(c(precision_logit, precision_int, precision_lda, 
                                          precision_qda, precision_ridge, precision_lasso), 3),
                      Recall = round(c(recall_logit, recall_int, recall_lda, 
                                       recall_qda, recall_ridge, recall_lasso), 3),
                      F1 = round(c(f1_logit, f1_int, f1_lda, 
                                   f1_qda, f1_ridge, f1_lasso), 3))

kable_styling(kable(results,
      caption = 'Model evaluation',
      align = 'c',
      format = 'latex',
      booktabs = T), 
              latex_options = "hold_position", 
              full_width = F,
              # position = "center", 
              font_size = 9)

```

As we can see from the figure above and from table 26, all the models have a very similar performance, the only model that stands out is the QDA model, which has the best recall, but the worst precision since it predict more 'graduated on time' than other models. The other models have a very similar performance in terms of AUC, accuracy, precision, recall and F1 score. This tells us that there are no particular differences between the models we used and we have covered all the possible explanation from data.

To explore this topic further we can comparate the coefficients of all models keeping out the QDA model, which has no coefficients comparable, and the Logistic with interactions model due to the fact it has more variables than the other models.









Come possiamo vedere dalla tabella precedente, a parte per la QDA i modelli risultano molto simili tra le metriche utilizzate, in compenso la QDA ha ottenuto la migliore Recall. 
Questo risultato porta a pensare che non ci siano particolari differenze tra i modelli usati.
Date queste motivazioni, per semplicità di spiegazione si è deciso di usare il modello logistico semplice per spiegare il fenomeno.
Per come è strutturato il modello non si può valutare l'incremento percentuale di ogni singola variabile, ma si può osservare di quanto è il rapporto tra le variabili. Possiamo solo valurare solo gli odds ratio. 
Cioè per valurare quanto influenzi il sesso basta fare l'exp del coefficiente relativo al sesso.
(exp(sex_Male) = 0.382, quindi se il sesso è maschile, la probabilità di laurearsi è 0.382 volte quella di una femmina, cioè il (100 - 38.2)% in meno rispetto ad una femmina).
Mentre il rapporto tra Sposato e Single è di 1.06 quello tra Altro e Single è di 0.07, il che stà ad indicare che chi rientra nella categoria Altro ha una notevole differenza nello sposarsi in tempo. Chi sceglie materie non stem ha il 58.9% di probabilità relativa in meno rispetto a stem.
Si può notare che ha una madre istruita ha più del doppio di probabilitò di laurearsi in tempo rispetto a chi ha una madre che non ha terminato le superiori, a quanto pare conoscendo il titolo di studio della madre non è influente sapere anche quello del padre.
Ottimo chi decide di seguire una Scholarship, ha il 2.24 volte di probabilità di laurearsi in tempo rispetto a chi non la segue.
Come ci si poteva aspettare, andando avanti con l'età la probabilità di laurearsi in tempo diminuisce, infatti per ogni anno in più di età la probabilità di laurearsi in tempo diminuisce decresce a livello esponenziale del 70% per ogni anno in più, ovviamente è solo un'approssimazione del modello


```{r, echo = F}
coef_matrix <- matrix(0,
                      ncol = 5,
                      nrow = length(coef(fit_logit_1)))
coef_names <- names(coef(fit_logit_1))
coef1 <- coef(fit_logit_1)
coef2 <- coef(fit_logit_3)
coef3 <- coef(ridge_model)[, best_lambda_index_r]
coef4 <- coef(lasso_model)[, best_lambda_index_l]
coef5 <- c(0, coef(fit_lda))

coef_matrix[, 1] <- coef1
for(i in 1:nrow(coef_matrix)){
  if(coef_names[i] %in% names(coef2)){
    coef_matrix[i, 2] <- coef2[which(names(coef2) == coef_names[i])]
  }
}
coef_matrix[, 3] <- coef3
coef_matrix[, 4] <- coef4
coef_matrix[, 5] <- coef5

coef_df <- data.frame(Logit_full = coef_matrix[, 1],
                      Logit_final = coef_matrix[, 2],
                      Ridge = coef_matrix[, 3],
                      Lasso = coef_matrix[, 4],
                      LDA = coef_matrix[, 5], 
                      row.names = coef_names)

kable_styling(kable(round(coef_df, 2), 
      caption = 'Coefficients comparison',
      align = 'c',
      format = 'latex',
      booktabs = T), 
              latex_options = "hold_position", 
              full_width = F,
              # position = "center", 
              font_size = 9)


plot(coef_df$Logit_full, 
     type = 'l', 
     lwd = 2, 
     ylim = c(min(coef_df), max(coef_df)), 
     main = 'Coefficients comparison', 
     xlab = 'Coefficients', 
     ylab = 'Variables')
lines(coef_df$Logit_final, 
      lwd = 2, 
      col = 'red')
lines(coef_df$LDA, 
      lwd = 2, 
      col = 'blue')
lines(coef_df$Ridge,
      lwd = 2, 
      col = 'purple')
lines(coef_df$Lasso, 
      lwd = 2, 
      col = 'orange')
legend('topright', 
       legend = c('Logit full', 'Logit final', 'LDA', 'Ridge', 'Lasso'), 
       col = c('black', 'red', 'blue', 'purple', 'orange'), 
       lty = 1)

```

As previously assumed, from the graph we can see that the coefficients of the various models are very similar to each other. All models give a considerable importance to the intercept, i.e. the reference group, except for the LDA model which does not have a direct intercept in the Rstudio output. All models agree on assigning a negative weight to Male sex, a positive weight to Scholarship holder, which is consistent with the previous analysis and a negative weight for the age. Except for the Lasso model, they all agree with the positive importance of the mother's level of education, however Father's qualification is not significant since is high correlated with it. Finally, it can noted that only the Ridge model gives importance to the student's level of education, in contrast to what was assumed in the EDA (table 7).


## Explaining the model

As we can see previously, the logistic model is the best model to explain the phenomenon, so we will use it to explain the results.

Diversamente da quanto abbiamo assunto nell'EDA, essere sposati o non avere un titolo di studio superiore sembrano non essere le variabili influenti nel determinare se uno studente si laureerà in tempo o meno. L'unica variabile familiare che risulta rilevante è il titolo di studio della madre, che ha un effetto positivo sulla probabilità di laurearsi in tempo di almeno 2.3 volte rispetto a chi non ha una madre che non ha terminato le superiori.

Non c'è una differenza significativa tra i fuori sede e non e neanche tra chi ha bisogno di aiuti speciali.

Chi scegli un corso stem ha 1.64 volte di probabilità in più di laurearsi in tempo rispetto a chi sceglie un corso non stem.

Anche essere maschi impatta negativamente sulla probabilità di laurearsi in tempo, con un decremento di circa il 38% rispetto alla probabilità che hanno le femmine.

Mentre l'aiuto economico della borsa di studio ha un impatto notevolemente positivo sulla probabilità di laurearsi in tempo, con un incremento di 7.86 volte rispetto a chi non ha una borsa di studio, andando ad indicare che non dover pensare a come pagarsi gli studi e potersi permettere di non lavorare durante il periodo di studio ha un impatto notevolmente positivo.

Infine, l'età ha un effetto negativo sulla probabilità di laurearsi in tempo, con un decremento del 70% per ogni anno in più di età, andando ad indicare che più tardi s'inizia a fare l'università maggiori sono le probabilità probabilità di diventare fuoricorso o abbandonare gli studi.



**Data Characteristics**

*Feature Quality*: The predictors appear to have a similar level of influence on the response variable (is_Graduated). This suggests that no single model is able to extract substantially more information from the predictors than the others.

*Signal-to-Noise Ratio*: The comparable performance across models indicates a balanced signal-to-noise ratio in the data. If the data were very noisy or the predictors weak, greater variability in model performance would be expected.

*Multicollinearity*: The predictors might be collinear, meaning they contain overlapping information. This can cause different models to perform similarly because they are capturing the same patterns in the data.

**Model Complexity**:

simpler models (e.g., Logistic Regression) and more complex models (e.g., QDA) perform similarly, suggesting that increasing model complexity does not lead to significant improvements. This implies that the underlying relationships in the data are relatively straightforward and do not require complex modeling to capture.

**Data Linearity**:

the similar performance of linear models (Logit, LDA) and non-linear models (QDA) suggests that the relationships between predictors and the response variable may be approximately linear. Non-linear models do not have a significant advantage, indicating no strong non-linear patterns in the data that are not being captured by the linear models.

**Predictive Power**:

The results suggest that the predictors sare a limited predicting power, indicating that there are be other unobserved factors influencing whether a student graduates on time or not,this is consistent with our project framework. 

# Conclusions

We were able to build several models that are able with a moderate degree of certantly whether a student will graduate or not just by looking at enrollment data.
A tool like this could be useful in several real world applications, universities could use these to better understand who is struggling to graduate in time and implement policy changes to address eventual issues 

DA RIVEDERE

In conclusion, we have built several models that are able to predict with a moderate degree of certainty whether a student will graduate on time or not just by looking at enrollment data and if they are scholarship holder. These models can be useful in several real-world applications, such as helping universities identify students who are at risk of dropping out and implementing interventions to support them. By understanding the factors that influence student success, universities can take proactive measures to improve graduation rates and student outcomes. The models we have developed provide a valuable tool for universities to identify at-risk students and provide them with the support they need to succeed.

- SPIEGARE IL FATTO CHE PARTE DEL SUCCESSO NON È DOVUTO ALLE VARIABILI CONSIDERATE, MA A VARIABILI NON OSSERVATE. SI POTREBBE CONSIDERARE COME VARIABILE IL VOTO DI USCITA DALLE SUPERIORI.
- RISPIEGARE IN BREVE QUALI SONO LE VARIABILI CHE INFLUENZANO DI PIÙ IL RISULTATO
- RICORDARSI CHE IL MODELLO NON SPIEGA SE LO STUDENTE SI LAUREA O MENO, MA SE LO FA IN TEMPO
- SPIEGARE CHE IL MODELLO NON È PERFETTO, MA È UN BUON INIZIO PER CAPIRE QUALI SONO LE VARIABILI CHE INFLUENZANO DI PIÙ IL RISULTATO


## Limitations and Future Work: 
Our models are based on enrollment data and do not take into account other factors that may influence student success, such as personal circumstances, academic performance, or external factors. Future work could include incorporating additional data sources to improve the accuracy of the models and provide a more comprehensive understanding of student success. Additionally, further research could explore the impact of different interventions on student outcomes and identify the most effective strategies for supporting at-risk students.





# References

Realinho, V.; Machado, J.;Baptista, L.; Martins, M.V. Predicting
Student Dropout and Academic Success. Data 2022, 7, 146.
https://doi.org/10.3390/data7110146


Valentim Realinho, Jorge Machado, Luís Baptista, & Mónica V. Martins. (2021). Predict students' dropout and academic success (1.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.5777340



## Appendix A

```{r echo = F, message = F}

# Load necessary library
library(knitr)

# Create a data frame that represents your dataset
original_data <- data.frame(
  Variable = c("Married", "Application mode", "Application order", "Course", 
               "Evening attendance", "Displaced", "Educational special needs", 
               "Tuition fees up to date", "Gender", "Scholarship holder", 
               "Age at enrollment", "International", "Curricular units 1st sem (credited)", 
               "Curricular units 1st sem (enrolled)", "Curricular units 1st sem (evaluations)", 
               "Curricular units 1st sem (approved)", "Curricular units 1st sem (grade)", 
               "Curricular units 1st sem (without evaluations)", "Curricular units 2nd sem (credited)", 
               "Curricular units 2nd sem (enrolled)", "Curricular units 2nd sem (evaluations)", 
               "Curricular units 2nd sem (approved)", "Curricular units 2nd sem (grade)", 
               "Curricular units 2nd sem (without evaluations)", "Unemployment rate", 
               "Inflation rate", "GDP", "Output", "Previous qualification",
               "Nationality", "Mother's qualification", "Father's qualification",
               "Mother's occupation", "Father's occupation"),
  Description = c("Categorical variable indicating the marital status of the individual",
                  "Categorical variable indicating the mode of application",
                  "Numeric variable indicating the order of application",
                  "Categorical variable indicating the chosen course",
                  "Binary variable indicating whether the individual attends classes during the daytime or evening",
                  "Binary variable indicating whether the individual has been displaced",
                  "Binary variable indicating whether the individual has educational special needs",
                  "Binary variable indicating whether the tuition fees are up to date",
                  "Binary variable indicating the gender of the individual",
                  "Binary variable indicating whether the individual holds a scholarship",
                  "Numeric variable indicating the age of the individual at the time of enrollment",
                  "Binary variable indicating whether the individual is international",
                  "Numeric variable indicating the number of credited curricular units in the 1st semester",
                  "Numeric variable indicating the number of enrolled curricular units in the 1st semester",
                  "Numeric variable indicating the number of evaluations for curricular units in the 1st semester",
                  "Numeric variable indicating the number of approved curricular units in the 1st semester",
                  "Numeric variable indicating the average grade for curricular units in the 1st semester",
                  "Numeric variable indicating the number of curricular units in the 1st semester without evaluations",
                  "Numeric variable indicating the number of credited curricular units in the 2nd semester",
                  "Numeric variable indicating the number of enrolled curricular units in the 2nd semester",
                  "Numeric variable indicating the number of evaluations for curricular units in the 2nd semester",
                  "Numeric variable indicating the number of approved curricular units in the 2nd semester",
                  "Numeric variable indicating the average grade for curricular units in the 2nd semester",
                  "Numeric variable indicating the number of curricular units in the 2nd semester without evaluations",
                  "Variable indicating the unemployment rate (Unemployment rate (%))",
                  "Numeric variable indicating the inflation rate (Inflation rate (%))",
                  "Numeric variable indicating the Gross Domestic Product",
                  "Categorical variable indicating the target variable (e.g., Dropout, Graduate, Enrolled)",
                  "Numeric variable indicating the level of the previous qualification",
                  "Categorical variable indicating the nationality of the individual",
                  "Numeric variable indicating the level of the mother's qualification",
                  "Numeric variable indicating the level of the father's qualification",
                  "Categorical variable indicating the mother's occupation",
                  "Categorical variable indicating the father's occupation")
)

# Create the table using kable
kable(original_data, col.names = c("Variable", "Description"), align = c('l', 'l'), caption = "Dataset Description")
```
