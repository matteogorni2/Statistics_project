---
title: "Statistical learning final project"
date: "2024-07-11"
author: "Di Pasquale Giuseppe, Gorni Selvestrini Matteo"
output: 
    pdf_document:
      toc: true
      number_sections: true
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
---

------------------------------------------------------------------------

\newpage
# Introduction

The data was originally collected by the Polytechnic Institute of Portalegre in Portugal to build machine learning models that predict a student’s outcome based on various socioeconomic factors and academic performance. This was done to develop an analytics tool for the tutoring program to direct their efforts more effectively.

The dataset was created from several disjoint databases and includes students enrolled in different undergraduate degrees, such as agronomy, design, education, nursing, journalism, management, social service, and technologies, it consists of 4424 records with 35 features.

The prediction problem is formulated as a three-category classification task, which assesses, based on socio-economic data and performance metrics during the academic years, whether a student will:

-   Graduate within the three years of planned course activities (‘Graduate’)
-   Change course or stop studying altogether (‘Dropout’)
-   Fail to graduate in time (‘Enrolled’)

According to the literature in the field, there is no agreed-upon definition of what constitutes a dropout. In this work, the authors defined dropouts from a micro-perspective, considering field and institution changes as dropouts regardless of when they occur. This approach results in much higher dropout rates than the macro-perspective, which considers only students who leave the higher education system without a degree.

Given that the number of libraries we can use is limited to those covered during the course, we decided to employ this dataset to answer a different research question. Our analysis will focus on building a model to assign a probability score to new students. This score will quantify the likelihood that a student will finish their course within a three-year timeframe based on data collected at enrollment and evaluating how much it may effect the scholarship award.

A detailed description of the original dataset can be found in Appendix A.

# Data Preprocessing

Let's start by loading our data and changing the graduated column in order to represent our research question correctly: we are going to rename the 'Output' variable into 'is_Graduated' and we make it binary, 'Yes' labels will correspond to the old 'Graduate' labels, while all the 'Enrolled' and 'Dropout' labels will be converted into 'No'.

```{r, results = 'hide'}

data <- read.csv('~/Downloads/Dropout and Success/student_data.csv', 
                 sep = ';')
str(data)

# Rename Columns
names(data)[names(data) == 'Nacionality'] <- 'Nationality'
names(data)[names(data) == 'Marital.status'] <- 'Married'
names(data)[names(data) == 'Output'] <- 'is_Graduated'

# Rename 'Enrolled' to have a binomial problem
data$is_Graduated[which(data$is_Graduated == 'Enrolled' | 
                       data$is_Graduated == 'Dropout')] <- 'No'
data$is_Graduated[which(data$is_Graduated == 'Graduate')] <- 'Yes'

```

***Missing values and data balance check:***

There were no missing values and the dataset is balanced.

```{r}
sum(is.na(data))
table(data$is_Graduated)
```

## Relabelling variables

We changed the labeling criteria in some of our variables in order to increase model explainability, in the following section we will discuss and showcase our changes.


```{r, echo = F}

## Marital status
index_single <- which(data$Married == 1) # Single
# index_married <- which(data$Married == 2) # Married
data$Married[index_single] <- 'No'
# data$Married[index_married] <- 'Married'
# data$Married[-c(index_single, index_married)] <- 'Others'
data$Married[-index_single] <- 'Yes'

```

```{r, echo = F}

## Mother's occupation
# Split in white / blue collar
index_m_no_worker <- which(data$Mother.s.occupation == 1| # Student
                             data$Mother.s.occupation == 12 | # Other
                             data$Mother.s.occupation == 13) # Blanck
index_m_white <- which(data$Mother.s.occupation == 2 |
                         data$Mother.s.occupation == 3 |
                         data$Mother.s.occupation == 4 |
                         data$Mother.s.occupation == 5 |
                         data$Mother.s.occupation == 6 |
                         data$Mother.s.occupation == 14 | # Armed forces Officers
                         data$Mother.s.occupation == 15 | # Armed forces sergeants
                         data$Mother.s.occupation == 17 |
                         data$Mother.s.occupation == 19 |
                         data$Mother.s.occupation == 20 |
                         data$Mother.s.occupation == 21 |
                         data$Mother.s.occupation == 22 |
                         data$Mother.s.occupation == 23 |
                         data$Mother.s.occupation == 24 |
                         data$Mother.s.occupation == 25 |
                         data$Mother.s.occupation == 26 |
                         data$Mother.s.occupation == 27 |
                         data$Mother.s.occupation == 28 |
                         data$Mother.s.occupation == 29)
data$Mother.s.occupation[index_m_no_worker] <- 'Others'
data$Mother.s.occupation[index_m_white] <- 'White Collar'
data$Mother.s.occupation[-c(index_m_no_worker, 
                            index_m_white)] <- 'Blue Collar'


## Father's occupation
# Slpit in white / blue collar
index_f_no_worker <- which(data$Father.s.occupation == 1| # Student
                             data$Father.s.occupation == 12 | #Other
                             data$Father.s.occupation == 13) # Blanck
index_f_white <- which(data$Father.s.occupation == 2 |
                         data$Father.s.occupation == 3 |
                         data$Father.s.occupation == 4 |
                         data$Father.s.occupation == 5 |
                         data$Father.s.occupation == 6 |
                         data$Father.s.occupation == 14 | # Armed forces Officers
                         data$Father.s.occupation == 15 | # Armed forces sergeants
                         data$Father.s.occupation == 17 |
                         data$Father.s.occupation == 19 |
                         data$Father.s.occupation == 20 |
                         data$Father.s.occupation == 21 |
                         data$Father.s.occupation == 22 |
                         data$Father.s.occupation == 23 |
                         data$Father.s.occupation == 24 |
                         data$Father.s.occupation == 25 |
                         data$Father.s.occupation == 26 |
                         data$Father.s.occupation == 27 |
                         data$Father.s.occupation == 28 |
                         data$Father.s.occupation == 29)
data$Father.s.occupation[index_f_no_worker] <- 'Others'
data$Father.s.occupation[index_f_white] <- 'White Collar'
data$Father.s.occupation[-c(index_f_no_worker, 
                            index_f_white)] <- 'Blue Collar'

```

```{r, echo = F}

## Mother's qualification
index_m_secondary <- which(data$Mother.s.qualification == 1 | 
                             data$Mother.s.qualification == 6 |
                             data$Mother.s.qualification == 13 |
                             data$Mother.s.qualification == 15 |
                             data$Mother.s.qualification == 22 |
                             data$Mother.s.qualification == 23 |
                             data$Mother.s.qualification == 29 |
                             data$Mother.s.qualification == 31 |
                             data$Mother.s.qualification == 32)
index_m_higher <- which(data$Mother.s.qualification == 2 |
                          data$Mother.s.qualification == 3 |
                          data$Mother.s.qualification == 4 |
                          data$Mother.s.qualification == 5 |
                          data$Mother.s.qualification == 30 |
                          data$Mother.s.qualification == 33 |
                          data$Mother.s.qualification == 34)
data$Mother.s.qualification[index_m_secondary] <- 'Secondary'
data$Mother.s.qualification[index_m_higher] <- 'Higher'
data$Mother.s.qualification[-c(index_m_secondary, 
                               index_m_higher)] <- 'No Secondary'

```

```{r, include = F}

## Previous qualification
# Group on (no secondary education, secondary education, bachelor's degree, 
# master's degree, phd or highter)
index_secondary <- which(data$Previous.qualification == 1 | 
                           data$Previous.qualification == 6 |
                           data$Previous.qualification == 14 |
                           data$Previous.qualification == 16)
index_higher <- which(data$Previous.qualification == 2 |
                        data$Previous.qualification == 3 |
                        data$Previous.qualification == 15 |
                        data$Previous.qualification == 4 | # Since just 8
                        data$Previous.qualification == 5| # Since just 1 phd
                        data$Previous.qualification == 17)
data$Previous.qualification[index_secondary] <- 'Secondary'
data$Previous.qualification[index_higher] <- 'Higher'
data$Previous.qualification[-c(index_secondary, 
                               index_higher)] <- 'No Secondary'


## Gender
index_male <- which(data$Gender == 1)
data$Gender[index_male] <- 'Male'
data$Gender[-index_male] <- 'Female'


## Father's qualification
index_f_secondary <- which(data$Father.s.qualification == 1 | 
                             data$Father.s.qualification == 6 |
                             data$Father.s.qualification == 13 |
                             data$Father.s.qualification == 15 |
                             data$Father.s.qualification == 22 |
                             data$Father.s.qualification == 23 |
                             data$Father.s.qualification == 29 |
                             data$Father.s.qualification == 31 |
                             data$Father.s.qualification == 32)
index_f_higher <- which(data$Father.s.qualification == 2 |
                          data$Father.s.qualification == 3 |
                          data$Father.s.qualification == 4 |
                          data$Father.s.qualification == 5 |
                          data$Father.s.qualification == 30 |
                          data$Father.s.qualification == 33 |
                          data$Father.s.qualification == 34)
data$Father.s.qualification[index_f_secondary] <- 'Secondary'
data$Father.s.qualification[index_f_higher] <- 'Higher'
data$Father.s.qualification[-c(index_f_secondary, 
                               index_f_higher)] <- 'No Secondary'


```



```{r, echo = F}

## Course
index_stem <- which(data$Course == 1 |     # Biofuel Production Technologies
                      data$Course == 4 |   # Agronomy
                      data$Course == 6 |   # Veterinary Nursing
                      data$Course == 7 |   # Informatics Engineerin
                      data$Course == 8 |   # Equiniculture
                      data$Course == 12 |  # Nursing
                      data$Course == 13)   # Oral Hygiene
data$Course[index_stem] <- 'Stem'
data$Course[-index_stem] <- 'No Stem'

```

```{r, echo = F}

## Nationality
data$Nationality[data$Nationality != 1] <- 'Others' # just 2%
data$Nationality[data$Nationality == 1] <- 'Portoguese'

```

```{r, include = F}

# Change variable's levels
# data$Married <- factor(data$Married, 
#                               levels = c('Single', 'Married', 'Others'))
data$Married <- factor(data$Married, 
                              levels = c('No', 'Yes'))
data$Course <- factor(data$Course, 
                      levels = c('Stem', 'No Stem'))
data$Previous.qualification <- factor(data$Previous.qualification, 
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Nationality <- factor(data$Nationality, 
                           levels = c('Portoguese', 'Others'))
data$Mother.s.qualification <- factor(data$Mother.s.qualification, 
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Father.s.qualification <- factor(data$Father.s.qualification,
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Mother.s.occupation <- factor(data$Mother.s.occupation,
                                   levels = c('Blue Collar', 'White Collar', 'Others'))
data$Father.s.occupation <- factor(data$Father.s.occupation,
                                   levels = c('Blue Collar', 'White Collar', 'Others'))
data$Gender <- factor(data$Gender,
                      levels = c('Female', 'Male'))
data$is_Graduated <- factor(data$is_Graduated,
                      levels = c('No', 'Yes'))

```


***Marital status:***

Categorical variable with 6 values indicating the marital status of the individual.
Given that the majority of the vast majority of students is single and only a few of them belong to one of the other categories we decided to create a binary variable indicating if a student is single or not.

```{r, echo = F, message = F}
library(knitr)
library(xtable)
library(kableExtra)

kable_styling(kable(table(data$Married),
                    caption = "Marital status",
                    col.names = c("Status", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)

```

***Mother/Father occupation:***

Categorical variables indicating the mother and father occupation respectively, while the original dataset had 32 labels for all kinds of different jobs, we decided to merge some of the labels and make this a 3-label categorical variable. Jobs were split based on a White/Blue collar distinction, as shown in the table below.

```{r, echo = F}

t_occ <- data.frame(Occupation = levels(data$Mother.s.occupation),
                    Mother = matrix(table(data$Mother.s.occupation)), 
                    Father = matrix(table(data$Father.s.occupation)))
kable_styling(kable(t_occ,
                    caption = "Parental occupation",
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)

```

***Mother/Father/Student qualification:***

Categorical variable indicating the level of each parents’ qualification as well as the student's. Again, we are dealing with many categorical variables, so we decided to merge them based on whether they have an completed an higher education cycle and if they have finished high school or not.

```{r, echo = F}

t_qual <- data.frame(Qualification = levels(data$Mother.s.qualification),
                     Mother = matrix(table(data$Mother.s.qualification)), 
                     Father = matrix(table(data$Father.s.qualification)), 
                     Student = matrix(table(data$Previous.qualification)))
kable_styling(kable(t_qual,
                    caption = "Qualification",
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)

```              

***Course:***

Categorical variable representing the course chosen at enrollment, we originally had 17 different courses, we decided to relabel the courses based on whether they belong to the STEM area or not.

```{r, echo = F}

kable_styling(kable(table(data$Course),
                    caption = "Course",
                    col.names = c("Course", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)

```

***Nationality:***

Categorical variable representing a students nationality: looking at the data we saw that while we had a lot of labels (one for each nationality), the vast majority of people were Portuguese, therefore we labeled data in the following way.

```{r, echo = F}

kable_styling(kable(table(data$Nationality),
                    caption = "Nationality", 
                    col.names = c("Nationality", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = T),
              latex_options = "hold_position",
              full_width = F,
              font_size = 9)

```

## Feature removal

Given that to build our model we are going to use only information that was present at enrollment time there are some features which are not useful and will therefore be removed.

- Curricular data: we removed all information concerning student performance over the span of three years since it doesn't help us answer our research question

- Application mode/ application order: the original paper didn't offer a clear indication about the various labels of this feature, furthermore we don't believe them to be of any interest as far as our research question goes.

- Macroeconomics data (Inflation rate, GDP, Unemployment rate): these economic indicators were taken over the course of the three years data collection period, we also don't believe them to be of any use in answering our research question.

- Tuition fees up do date.

- We also removed the 'International' variable since it was rendundant.
```{r, echo = F, message = F}

# Remove useless columns
data <- data[, -which(grepl('Application', colnames(data)) | 
                        grepl('Tuition', colnames(data)) |
                        grepl('Debtor', colnames(data)) |
                        grepl('Unemployment', colnames(data)) |
                        grepl('Inflation', colnames(data)) |
                        grepl('GDP', colnames(data)) |
                        grepl('Curricular', colnames(data)) |
                        grepl('evening', colnames(data)))]

```


```{r}
table(data$Nationality, data$International)
data <- data[, -which(colnames(data) == 'International')]

```


# Exploratory Data Analysis


```{r tab_dio, echo = F}

t1  <- kable(addmargins(table(data$Married, data$is_Graduated), 2), 
            format = "latex", booktabs = T) 
t2 <- kable(addmargins(table(data$Course, data$is_Graduated), 2), 
            format = "latex", booktabs = T)
t3 <- kable(addmargins(table(data$Previous.qualification, data$is_Graduated), 2), 
            format = "latex", booktabs = T)
t4 <- kable(addmargins(table(data$Nationality, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t5 <- kable(addmargins(table(data$Mother.s.qualification, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t6 <- kable(addmargins(table(data$Father.s.qualification, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t7 <- kable(addmargins(table(data$Mother.s.occupation, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t8 <- kable(addmargins(table(data$Father.s.occupation, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t9 <- kable(addmargins(table(data$Displaced, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t10 <- kable(addmargins(table(data$Educational.special.needs, data$is_Graduated), 2),
            format = "latex", booktabs = T)
t11 <- kable(addmargins(table(data$Scholarship.holder, data$is_Graduated), 2),
             format = "latex", booktabs = T)
t12 <- kable(addmargins(table(data$Gender, data$is_Graduated), 2),
             format = "latex", booktabs = T)

```



Most of our dataset's features are categorical, therefore we will plot the data in the form of contingency tables. This procedure allows us to make some interesting observations:

- Most people who enroll are single and at the same time are more likely to graduate than others (51.4% vs 38.4%); (table 6)

- People who enroll after completing secondary school tend to graduate in time more often than those who do not (note: Portalegre's university allows some students to attend courses without having an high-school diploma); (table 7)

- Although there are more students enrolled in non-STEM courses (2,702 vs 1,722), the graduation rate is higher for those in STEM courses (52.3% vs 48.4%); (table 8)

- Mothers generally have a higher level of education compared to fathers. Specifically, 3076 fathers do not have an high school diploma, the same can be said only for 234 mothers, this does not seem to affect graduation rates of their children; (table 10)


- Students who left their parents home to study (i.e. the 'displaced' variable) have a higher graduation rate compared to those who did not (54.6% vs 44.3%); (table 14)

- Students who receive a scholarship have a higher graduation rate than those who do not (76.0% vs 41.3%); (table 16)

- Women seem to graduate on time much more than men do (57.9% vs 35.2%). (table 17)

- From the figures below we can see that the distribution of 'Age at enrollment' does not showcase a normal behaviour and it's different between students who graduate and those who do not: both of them have a high right tail with more outliers, but the distribution of graduate on time is more concentrated around the mean value (20 years). 


```{r sample, echo = F, results = 'asis'}

cat(c("\\begin{table}[!htb]
    \\fontsize{9}{11}\\selectfont
    \\begin{minipage}{.5\\linewidth}
      \\caption{Marital status VS is Graduated}
      \\centering",
        t1,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Previous qualification VS is Graduated}",
        t3,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Course VS is Graduated}
      \\centering",
        t2,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Nationality VS is Graduated}",
        t4,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Mother's qualification VS is Graduated}
      \\centering",
        t5,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Father's qualification VS is Graduated}",
        t6,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Mother's occupation VS is Graduated}
      \\centering",
        t7,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Father's occupation VS is Graduated}",
        t8,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Displaced VS is Graduated}
      \\centering",
        t9,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Education special VS is Graduated}",
        t10,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Scholarship VS is Graduated}
      \\centering",
        t11,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Gender VS is Graduated}",
        t12,
    "\\end{minipage}
\\end{table}"
))  
```
```{r, echo = F}

par(mfrow = c(1, 2))
boxplot(data$Age.at.enrollment ~ data$is_Graduated, 
        col = c('red', 'blue'),
        # main = 'Age at enrollment',
        xlab = 'is_Graduated', 
        ylab = 'Age at enrollment')
plot(density(data$Age.at.enrollment[data$is_Graduated == 'Yes']), 
     col = 'blue',
     main = '',
     xlab = 'Age at enrollment')
lines(density(data$Age.at.enrollment[data$is_Graduated == 'No']), 
      col = 'red')
legend('topright', 
       legend = c('Yes', 'No'), 
       col = c('blue', 'red'), 
       lty = 1)
title('Age at enrollment', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

\newpage
## Outliers detection and removal

As we have seen in the previous section, the distribution of the age at enrollment contains a lot of outliers. We will now remove these outliers from the training set only, in order to preserve test set to simulate a real-world scenario. This will allow us to build a more robust model that generalizes better to unseen data. 


We can see that there are more outliers left, but we decided to keep them to stay more faithful to the original data distribution.

```{r, echo = F, fig.align = 'center', out.height = '40%', fig.show = 'hide'}

# Train test split
set.seed(71)
index <- sample(1:nrow(data), 
                round(0.75 * nrow(data)))
train <- data[index, ]
test <- data[-index, ]

# outliers removal
par(mfrow = c(1, 2))
boxplot(train$Age.at.enrollment ~ train$is_Graduated, 
        col = c('red', 'blue'),
        # main = 'Age at enrollment',
        xlab = 'is_Graduated', 
        ylab = 'Age at enrollment')
plot(density(train$Age.at.enrollment[train$is_Graduated == 'Yes']), 
     col = 'blue',
     main = '',
     xlab = 'Age at enrollment')
lines(density(train$Age.at.enrollment[train$is_Graduated == 'No']), 
      col = 'red')
legend('topright', 
       legend = c('Yes', 'No'), 
       col = c('blue', 'red'), 
       lty = 1)
title('Age at enrollment', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

```{r, echo = F, fig.show = 'hide'}

# Removing outliers
max_age_drop <- min(boxplot(train$Age.at.enrollment[train$is_Graduated == 'No'])$out)
index_age_drop <- which(train$Age.at.enrollment >= max_age_drop &
                          train$is_Graduated == 'No')
max_age_grad <- min(boxplot(train$Age.at.enrollment[train$is_Graduated == 'Yes'])$out)
index_age_grad <- which(train$Age.at.enrollment >= max_age_grad &
                          train$is_Graduated == 'Yes')
train <- train[-c(index_age_drop, 
                  index_age_grad), ]


```

```{r, echo = F, out.height = '30%', fig.align = 'center', height = 5}

# After removal
par(mfrow = c(1, 2))
boxplot(train$Age.at.enrollment ~ train$is_Graduated, 
        col = c('red', 'blue'),
        # main = 'Age at enrollment',
        xlab = 'is_Graduated', 
        ylab = 'Age at enrollment')
plot(density(train$Age.at.enrollment[train$is_Graduated == 'Yes']), 
     col = 'blue',
     main = '',
     xlab = 'Age at enrollment')
lines(density(train$Age.at.enrollment[train$is_Graduated == 'No']), 
      col = 'red')
legend('topright', 
       legend = c('Yes', 'No'), 
       col = c('blue', 'red'), 
       lty = 1)

title('Age at enrollment', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

# Model building

We will now try to build different models using the training set, the models we will use include logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), ridge regression and lasso regression. We will then evaluate the performance of these models using the test set by plotting the confusion matrix and the ROC curve. Finally, we'll compare the relative coefficients.

Before proceeding, we will translate the 'Age at enrollment' variable to avoid a high intercept value.

```{r}
train$Age.at.enrollment <- train$Age.at.enrollment - 17
test$Age.at.enrollment <- test$Age.at.enrollment - 17
```




## Logistic Regression

We start our analysis by considering the *Logistic Regression model*, this seems like a very robust choice given the fact that our research question revolves around finding a probability in a binary classification task.

The core idea behind logistic regression is to model the relationship between one or more independent variables (features) and a binary dependent variable (outcome) using the logistic function. This function maps a linear combination of the features to a probability score between 0 and 1.

In logistic regression, the coefficients associated with each feature are estimated using maximum likelihood estimation. These coefficients represent the impact of each feature on the log-odds of the outcome variable.


```{r, echo = F}
######################################
# Logistic Regression
######################################
```

```{r, echo = T, message = F, results = 'hide', warning = F}

fit_logit_1 <- glm(is_Graduated ~ .,
            family = binomial,
            data = train)
summary(fit_logit_1)

# Stepwise selection
fit_logit_2 <- step(fit_logit_1)
summary(fit_logit_2)

# Removing outliers
index_outliers <- which(abs(scale(fit_logit_2$residuals)) > 2)
train <- train[-index_outliers, ]

```

```{r, echo = T}

# Final model
fit_logit_3 <- update(fit_logit_2, 
               .~.)
summary(fit_logit_3)
```

One very important feature of logistic regression is that it is possible to look is certain features have a significant influence over the outcome variable. Looking at the model we just built we can see that many of the predictions we made during exploratory data analysis hold true:

- being in a non-STEM course is associated with a decrease of 0.50 in the log-odds of graduating, compared to being in a STEM course (reference group);

- having a mother with secondary education is associated with an increase of 0.96 in the log-odds of graduating compared to having a mother with no education (reference group);

- having a mother with higher education is associated with an increase of 0.84 in the log-odds of graduating compared to having a mother with no education (reference group);

- being male is associated with a decrease of 0.94 in the log-odds of graduating compared to being female (reference group);

- being a scholarship holder is associated with a decrease of 2.18 in the log-odds of graduating compared to not being a scholarship holder;

- each additional year of age at enrollment is associated with a decrease of 0.366 in the log-odds of graduating. 


```{r, echo = F, message = F, results = 'hide', warning = F}

# Predictions
pred_logit <- predict(fit_logit_3, 
                      newdata = test, 
                      type = 'response')

```


### Logistic Regression with interactions

In logistic regression, interactions occur when the effect of one predictor variable on the outcome depends on the value of another independent variable. We will now try to build a logistic regression model with interactions to see if we can improve the model's performance using only significant variables find previously.

```{r, echo = T, message = F, results = 'hide', warning = F}
fit_int_1 <- update(fit_logit_3, 
                          .~.*.)
summary(fit_int_1)

# Stepwise selection
fit_int_2 <- step(fit_int_1)
```
```{r, echo = T}
summary(fit_int_2)
```


```{r, echo = F}
# Predictions
pred_int <- predict(fit_int_2, 
                      newdata = test, 
                      type = 'response')

```

\newpage
We find that the single variables are like the simple model, but from the significant interactions we have that:

- being male in a non-STEM course is associated with an increase of 0.63 in the log-odds of graduating;

- having a scholarship and a mother with secondary or higher education is associated with an increase of, respectively, 1.36 and 1.67 in the log-odds of graduating;

- being male with a scholarship decrease the log-odds of graduating by 0.86;

- being a scholarship holder decreases log-odds of graduating by 0.12 for each year of age at enrollment time. Recalling that in this case the coefficient for scholarship holder is 1.60, it means that if you enroll at 20 years old and you have a scholarship, the log-odds of graduating are 1.60 - 0.12 * 20 = -0.80 compared to those who did not get the scholarship.


We then plot residuals for both models in order to gain even more insights about the data we are working with: by looking Q-Q residuals plot, it is evident that the residuals are not normally distributed, suggesting that the variables included in the model are not sufficient to explain the data variability.

Furthermore, the residuals vs leverage plot reveals the presence of points with very high leverage, indicating that there are still outliers in the dataset.

The plots for the logistic regression with interactions are more similar to the ones of the logistic regression, suggesting that even if the variables are significant they do not particularly improve the model's performance. As we can see from the ANCOVA test, it is significant, but not improve particularly the residual deviance (from 2694 to 2670).


```{r, echo = F, out.width = '90%', out.height = '40%', fig.align = 'center'}

# kable(round(summary(fit_logit_3)$coefficients, 3), 
#       caption = 'Logistic regression coefficients',
#       align = 'r',
#       format = 'latex',
#       booktabs = T, 
#       font_size = 6)

par(mfrow = c(2, 2))
plot(fit_logit_3)
title('Logistic regression', outer = T, cex = 1.5, line = -2)

plot(fit_int_2)
title('Logistic regression with interactions', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

```{r}
anova(fit_logit_3, fit_int_2)
```

As we have assumed previously, from the two confusion matrices below (table 18 and table 19), we can see that there are no much difference between the two models, the only little difference is that the model with interactions predict just a little more zero's than the simple model.


```{r, echo = F}
# Confusion matrix

cm_logit <- kable(table(test$is_Graduated, round(pred_logit)),
              format = 'latex',
              booktabs = T)

cm_int <- kable(table(test$is_Graduated, round(pred_int)),
                format = 'latex',
                booktabs = T) 

```




```{r, echo = F, results = 'asis'}

cat(c("\\begin{table}[!htb]
    \\fontsize{9}{11}\\selectfont
    \\begin{minipage}{.5\\linewidth}
      \\caption{Test vs Logistic predictions}
      \\centering",
      cm_logit,
      "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Test vs Logistic with interactions predictions}", 
      cm_int,
      "\\end{minipage}
\\end{table}"
))

```

\newpage
For the upcoming models we decided to use confusion matrices as the main metric to evaluate our models' performance, We choose to do this by applying Occam's razor principle and choose to extract only information that are relevant to our research question, and finally we compare all the models using classification evaluation metrics. 

## Linear Discriminant Analysis (LDA)

LDA is a classic statistical technique utilized in machine learning and pattern recognition for classification tasks. We employ it to identify which linear combination of features best separates multiple classes or categories in our dataset.

At its core, LDA operates under the assumption that the data can be represented as multivariate Gaussian distributions and that the classes share the same covariance matrix. It seeks to find a projection of the data onto a lower-dimensional space while maximizing the separation between classes and minimizing the variance within each class. 
Even if the assumptions of LDA are not met, it is still possible to apply it when the variables are discrete. In such cases, LDA can still provide good separation between classes, especially if the differences between classes are sufficiently pronounced. 


```{r, echo = F}
######################################
# Linear Discriminant Analysis (LDA)
######################################
```

```{r, echo = T, message = F}
library(MASS)

fit_lda <- lda(is_Graduated ~ ., 
               data = train)
```

We can observe the density of the LDA results respect to the "is_Graduated" variable on train set in the plot below. The coefficients represent the weights of each feature in the linear combination that best separates the classes, suggesting that the features included in the model are useful for predicting the outcome variable, although it is not linearly separable.

```{r, echo = F, message = F, out.height = '30%', fig.align = 'center'}
# Plot density of lda results with respect to the is_Graduated
plot(fit_lda, 
     type = 'both',
     main = 'LDA') # col not working


# kable_styling(kable((round((fit_lda$scaling), 3)),
#       caption = 'LDA coefficients',
#       align = 'c',
#       format = 'latex',
#       booktabs = T),
#       latex_options = c("hold_position", "scale_down"),
#       full_width = F,
#       position = "center",
#       font_size = 9)

# Predictions
pred_lda <- predict(fit_lda, 
                    newdata = test)$posterior[, 2]

```

## Quadratic Discriminant Analysis (QDA)

QDA is an extension of Linear Discriminant Analysis (LDA) used for classification tasks. While LDA assumes that different classes share the same covariance matrix, QDA relaxes this assumption, allowing each class to have its own covariance matrix.

```{r, echo = F}

######################################
# Quadratic Discriminant Analysis (QDA)
######################################

```
```{r, echo = T, message = F}
fit_qda <- qda(is_Graduated ~ ., 
               data = train)

```

```{r, echo = F}
# Plot density of qda coefficients with respect to the is_Graduated
pred_qda <- predict(fit_qda, 
                    newdata = test)$posterior[, 2]
detach('package:MASS')
```

Differently from LDA, QDA does not plot the density of the results, but we can still see the probability of the train set to be graduated on time in the tables below. We can see that QDA is really able to predict correctly who graduated on time, but it have not a good performance for predicting who did not graduate on time (89% vs 48%).

```{r, echo = F}
index_yes <- which(train$is_Graduated == 'Yes')
pred_train <- predict(fit_qda)$posterior[, 2]
kable_styling(kable(t(round(prop.table(table(round((1-pred_train[-index_yes]), 1))), 2)),
      caption = 'Distribution of the results respect to NOT graduating on time in the train set',
      align = 'c',
      format = 'latex',
      booktabs = T),
      latex_options = c("hold_position", "scale_down"),
      full_width = F,
      position = "center",
      font_size = 9)

kable_styling(kable(t(round(prop.table(table(round((pred_train[index_yes]), 1))), 2)),
      caption = 'Distribution of the results respect to graduating on time in the train set',
      align = 'c',
      format = 'latex',
      booktabs = T),
      latex_options = c("hold_position", "scale_down"),
      full_width = F,
      position = "center",
      font_size = 9)
```

We see that the LDA model is more balanced in predicting who graduated on time and who did not, unlike the QDA model which is very prone to graduate on time (table 22 and table 23).


```{r, echo = F}

cm_lda <- kable(table(test$is_Graduated, round(pred_lda)),
              format = 'latex',
              booktabs = T)

cm_qda <- kable(table(test$is_Graduated, round(pred_qda)),
                format = 'latex',
                booktabs = T) 

```

```{r, echo = F, results = 'asis'}

cat(c("\\begin{table}[!htb]
    \\fontsize{9}{11}\\selectfont
    \\begin{minipage}{.5\\linewidth}
      \\caption{Test vs LDA predictions}
      \\centering",
      cm_lda,
      "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Test vs QDA predictions}", 
      cm_qda,
      "\\end{minipage}
\\end{table}"
))

```

## Ridge Regression

Ridge regression is a regularization technique used in linear regression to mitigate the issues of multicollinearity and overfitting: it can be used to work with high dimensional data, which is why we find it valuable in our case.

```{r, echo = F, message = F}

###########################
# Ridge Regression
###########################
```
```{r, echo = T, message = F}
library(glmnet)

design_matrix_train <- model.matrix(is_Graduated ~ .,
                                    data = train)[, -1]

design_matrix_test <- model.matrix(is_Graduated ~ .,
                                   data = test)[, -1]

# Creating a grid for lamda
grid <- 10^seq(2, -3, length = 100)

# Ridge Regression with Binomial distribution
ridge_model <- glmnet(x = design_matrix_train, 
                      y = train$is_Graduated,
                      alpha = 0, 
                      lambda = grid,
                      intercept = T, 
                      family = "binomial")

# Find the best lambda
predictions <- predict(ridge_model, 
                       newx = design_matrix_test, 
                       type = 'response')

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$is_Graduated)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index_r <- which.max(accuracies)
pred_ridge <- matrix(predictions[, best_lambda_index_r], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index_r]

colnames(pred_ridge) <- 'pred_ridge'

```

In the figures below we can see the evolution of the coefficients of the ridge model with respect to the lambda value, left figure. We can see that the coefficients are shrinking to zero as the lambda value increases, this is a clear sign that the model is working correctly and that the regularization is working as expected. The right figure show the evolution of the deviance of the model with respect to coefficients value. We can also see that with the ridge model the coefficients can be change direction over the lambda value.

```{r, echo = F, fig.align = 'center', warning = F}

par(mfrow = c(1, 2))
# Some fancy graphs about ridge model
plot(ridge_model, xvar = 'lambda', label = T)
plot(ridge_model, xvar = 'dev', label = T)
title('Ridge model', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

## Lasso Regression

This regression techniques takes a slightly different approach by adding a penalty term that penalizes the absolute values of the regression coefficients, instead of their squares (which is what Rigde regression does).

This penalty term encourages sparsity in the coefficient vector, effectively driving some coefficients to exactly zero. As a result, Lasso regression not only helps in shrinking coefficient values but also performs variable selection by automatically excluding irrelevant features from the model.

```{r, echo = F}

###########################
# Lasso Regression
###########################
```
```{r, echo = T, message = F}
# Lasso Regression with Binomial distribution
lasso_model <- glmnet(x = design_matrix_train, 
                      y = train$is_Graduated,
                      alpha = 1, 
                      lambda = grid,
                      intercept = T, 
                      family = "binomial")

# Find the best lambda
predictions <- predict(lasso_model, 
                       newx = design_matrix_test, 
                       type = 'response')

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$is_Graduated)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index_l <- which.max(accuracies)
pred_lasso <- matrix(predictions[, best_lambda_index_l], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index_l]

colnames(pred_lasso) <- 'pred_lasso'

```

Like previously, we can see the evolution of the coefficients of the lasso model with respect to the lambda value, left figure. We can see that the coefficients are shrinking to zero as the lambda value increases, this is a clear sign that the model is working correctly and that the regularization is working as expected. The right figure shows the deviance of the model with respect to the lambda value, we can see that the deviance is decreasing as the lambda value increases, this is a clear sign that the model is working correctly and that the regularization is working as expected. However, differently from the ridge model, the coefficients difficultly change direction over the lambda value and they collapse to 0 more quickly than ridge model.

```{r, echo = F, fig.align = 'center', warning = F}

par(mfrow = c(1, 2))
# Some fancy graphs about lasso model
plot(lasso_model, xvar = 'lambda', label = T)
plot(lasso_model, xvar = 'dev', label = T)
title('Lasso model', outer = T, cex = 1.5, line = -2)
par(mfrow = c(1, 1))

```

From the confusion matrices below (table 24 and table 25), we can see that the ridge model predictions are more similar to the lasso model predictions, likely logistic models.

```{r, echo = F}

cm_ridge <- kable(table(test$is_Graduated, round(pred_ridge)),
              format = 'latex',
              booktabs = T)

cm_lasso <- kable(table(test$is_Graduated, round(pred_lasso)),
                format = 'latex',
                booktabs = T) 

```

```{r, echo = F, results = 'asis'}

cat(c("\\begin{table}[!htb]
    \\fontsize{9}{11}\\selectfont
    \\begin{minipage}{.5\\linewidth}
      \\caption{Test vs Ridge predictions}
      \\centering",
      cm_ridge,
      "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Test vs Lasso predictions}", 
      cm_lasso,
      "\\end{minipage}
\\end{table}"
))

```

# Model Evaluation

We intend to evaluate our models by plotting their respective ROC curves and computing some standard evaluation metrics, such as precision, accuracy, recall and f1 score.

ROC (Receiver Operating Characteristic) curves are graphical representations used to evaluate the performance of binary classification models. The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.


```{r, echo = F, message = F, warning = F}

# Create roc curve for all models
library(pROC)

# Logit
roc_out_logit <- roc(test$is_Graduated ~ pred_logit, 
                     levels = c('No', 'Yes'))

# Logit with interactions
roc_out_int <- roc(test$is_Graduated ~ pred_int, 
                     levels = c('No', 'Yes'))

# LDA
roc_out_lda <- roc(test$is_Graduated ~ pred_lda, 
                   levels = c('No', 'Yes'))

# QDA
roc_out_qda <- roc(test$is_Graduated ~ pred_qda, 
                   levels = c('No', 'Yes'))

# Ridge
roc_out_ridge <- roc(test$is_Graduated ~ pred_ridge, 
                     levels = c('No', 'Yes'))

# Lasso
roc_out_lasso <- roc(test$is_Graduated ~ pred_lasso, 
                     levels = c('No', 'Yes'))

# Plotting the ROC curves and resize the plot on 'F Positive Rate' = c(1, 0)
plot(roc_out_logit, 
     col = 'red', 
     lwd = 2, 
     main = 'ROC curves', 
     xlab = '1 - F Positive Rate', 
     ylab = 'T Positive Rate')
lines(roc_out_int, 
      col = 'black', 
      lwd = 2)
lines(roc_out_lda, 
      col = 'blue', 
      lwd = 2)
lines(roc_out_qda,
      col = 'green', 
      lwd = 2)
lines(roc_out_ridge,
      col = 'purple', 
      lwd = 2)
lines(roc_out_lasso,
      col = 'orange', 
      lwd = 2)
legend('bottomright', 
       legend = c('Logit', 'Interaction', 'LDA', 'QDA', 'Ridge', 'Lasso'), 
       col = c('red', 'black', 'blue', 'green', 'purple', 'orange'), 
       lty = 1)


```

```{r, echo = F}

# AUC
auc_logit <- round(auc(roc_out_logit), 3)
auc_int <- round(auc(roc_out_int), 3)
auc_lda <- round(auc(roc_out_lda), 3)
auc_qda <- round(auc(roc_out_qda), 3)
auc_ridge <- round(auc(roc_out_ridge), 3)
auc_lasso <- round(auc(roc_out_lasso), 3)

# Confusion matrix
confusion_matrix_logit <- addmargins(table(round(pred_logit), test$is_Graduated), 2)
confusion_matrix_int <- addmargins(table(round(pred_int), test$is_Graduated), 2)
confusion_matrix_lda <- addmargins(table(round(pred_lda), test$is_Graduated), 2)
confusion_matrix_qda <- addmargins(table(round(pred_qda), test$is_Graduated), 2)
confusion_matrix_ridge <- addmargins(table(round(pred_ridge), test$is_Graduated), 2)
confusion_matrix_lasso <- addmargins(table(round(pred_lasso), test$is_Graduated), 2)

# Accuracy
accuracy_logit <- sum(diag(confusion_matrix_logit)) / sum(confusion_matrix_logit[, -3])
accuracy_int <- sum(diag(confusion_matrix_int)) / sum(confusion_matrix_int[, -3])
accuracy_lda <- sum(diag(confusion_matrix_lda)) / sum(confusion_matrix_lda[, -3])
accuracy_qda <- sum(diag(confusion_matrix_qda)) / sum(confusion_matrix_qda[, -3])
accuracy_ridge <- sum(diag(confusion_matrix_ridge)) / sum(confusion_matrix_ridge[, -3])
accuracy_lasso <- sum(diag(confusion_matrix_lasso)) / sum(confusion_matrix_lasso[, -3])

# Precision
precision_logit <- confusion_matrix_logit[2, 2] / sum(confusion_matrix_logit[2, -3])
precision_int <- confusion_matrix_int[2, 2] / sum(confusion_matrix_int[2, -3])
precision_lda <- confusion_matrix_lda[2, 2] / sum(confusion_matrix_lda[2, -3])
precision_qda <- confusion_matrix_qda[2, 2] / sum(confusion_matrix_qda[2, -3])
precision_ridge <- confusion_matrix_ridge[2, 2] / sum(confusion_matrix_ridge[2, -3])
precision_lasso <- confusion_matrix_lasso[2, 2] / sum(confusion_matrix_lasso[2, -3])

# Recall
recall_logit <- confusion_matrix_logit[2, 2] / sum(confusion_matrix_logit[, 2])
recall_int <- confusion_matrix_int[2, 2] / sum(confusion_matrix_int[, 2])
recall_lda <- confusion_matrix_lda[2, 2] / sum(confusion_matrix_lda[, 2])
recall_qda <- confusion_matrix_qda[2, 2] / sum(confusion_matrix_qda[, 2])
recall_ridge <- confusion_matrix_ridge[2, 2] / sum(confusion_matrix_ridge[, 2])
recall_lasso <- confusion_matrix_lasso[2, 2] / sum(confusion_matrix_lasso[, 2])
  
# F1 score
f1_logit <- 2 * (precision_logit * recall_logit) / (precision_logit + recall_logit)
f1_int <- 2 * (precision_int * recall_int) / (precision_int + recall_int)
f1_lda <- 2 * (precision_lda * recall_lda) / (precision_lda + recall_lda)
f1_qda <- 2 * (precision_qda * recall_qda) / (precision_qda + recall_qda)
f1_ridge <- 2 * (precision_ridge * recall_ridge) / (precision_ridge + recall_ridge)
f1_lasso <- 2 * (precision_lasso * recall_lasso) / (precision_lasso + recall_lasso)

# Plot a matrix with all the results
results <- data.frame(Model = c('Logit', 'Interaction', 'LDA', 'QDA', 'Ridge', 'Lasso'),
                      AUC = round(c(auc_logit, auc_int, auc_lda, 
                                    auc_qda, auc_ridge, auc_lasso), 3),
                      Accuracy = round(c(accuracy_logit, accuracy_int, accuracy_lda, 
                                         accuracy_qda, accuracy_ridge, accuracy_lasso), 3),
                      Precision = round(c(precision_logit, precision_int, precision_lda, 
                                          precision_qda, precision_ridge, precision_lasso), 3),
                      Recall = round(c(recall_logit, recall_int, recall_lda, 
                                       recall_qda, recall_ridge, recall_lasso), 3),
                      F1 = round(c(f1_logit, f1_int, f1_lda, 
                                   f1_qda, f1_ridge, f1_lasso), 3))

kable_styling(kable(results,
      caption = 'Model evaluation',
      align = 'c',
      format = 'latex',
      booktabs = T), 
              latex_options = "hold_position", 
              full_width = F,
              # position = "center", 
              font_size = 9)

```

As we can see from the figure above and from table 26, all the models have a very similar performance, the only model that stands out is the QDA model, which has the best recall, but the worst precision since it predicts 'graduated on time' more often than other models. The other models have a very similar performance in terms of AUC, accuracy, precision, recall and F1 score. This tells us that there are no particular differences between the models we used and we have covered all the possible explanation from data.
This result suggests that there are no significant differences between the models used.

To explore this topic further we comparate the coefficients of all models keeping out the QDA model, which has no coefficients comparable, and the Logistic with interactions model due to the fact it has more variables than the other models.







Given these reasons, it was decided to use the simple logistic model to explain the phenomenon for simplicity.
Due to the structure of the model, we cannot assess the percentage increase of each individual variable, but we can observe the ratio between the variables. We can only evaluate the odds ratios.
To evaluate the influence of gender, we need to take the exp of the coefficient related to gender.
(exp(sex_Male) = 0.382, so if the gender is male, the probability of graduating is 0.382 times that of a female, which is (100 - 38.2)% less than that of a female).
While the ratio between Married and Single is 1.06, the ratio between Other and Single is 0.07, which indicates that those in the Other category have a significant difference in marrying on time. Those who choose non-STEM subjects have a 58.9% lower relative probability compared to those who choose STEM subjects.
It can be noted that those with an educated mother have more than twice the probability of graduating on time compared to those whose mother did not complete high school; apparently, knowing the mother's education level makes knowing the father's education level irrelevant.
Notably, those who decide to follow a scholarship have 2.24 times the probability of graduating on time compared to those who do not follow one.
As expected, as age increases, the probability of graduating on time decreases; in fact, for each additional year of age, the probability of graduating on time decreases exponentially by 70%, although this is only an approximation of the model.




```{r, echo = F}
coef_matrix <- matrix(0,
                      ncol = 5,
                      nrow = length(coef(fit_logit_1)))
coef_names <- names(coef(fit_logit_1))
coef1 <- coef(fit_logit_1)
coef2 <- coef(fit_logit_3)
coef3 <- coef(ridge_model)[, best_lambda_index_r]
coef4 <- coef(lasso_model)[, best_lambda_index_l]
coef5 <- c(0, coef(fit_lda))

coef_matrix[, 1] <- coef1
for(i in 1:nrow(coef_matrix)){
  if(coef_names[i] %in% names(coef2)){
    coef_matrix[i, 2] <- coef2[which(names(coef2) == coef_names[i])]
  }
}
coef_matrix[, 3] <- coef3
coef_matrix[, 4] <- coef4
coef_matrix[, 5] <- coef5

coef_df <- data.frame(Logit_full = coef_matrix[, 1],
                      Logit_final = coef_matrix[, 2],
                      Ridge = coef_matrix[, 3],
                      Lasso = coef_matrix[, 4],
                      LDA = coef_matrix[, 5], 
                      row.names = coef_names)

kable_styling(kable(round(coef_df, 2), 
      caption = 'Coefficients comparison',
      align = 'c',
      format = 'latex',
      booktabs = T), 
              latex_options = "hold_position", 
              full_width = F,
              # position = "center", 
              font_size = 9)


plot(coef_df$Logit_full, 
     type = 'l', 
     lwd = 2, 
     ylim = c(min(coef_df), max(coef_df)), 
     main = 'Coefficients comparison', 
     xlab = 'Coefficients', 
     ylab = 'Variables')
lines(coef_df$Logit_final, 
      lwd = 2, 
      col = 'red')
lines(coef_df$LDA, 
      lwd = 2, 
      col = 'blue')
lines(coef_df$Ridge,
      lwd = 2, 
      col = 'purple')
lines(coef_df$Lasso, 
      lwd = 2, 
      col = 'orange')
legend('topleft', 
       legend = c('Logit full', 'Logit final', 'LDA', 'Ridge', 'Lasso'), 
       col = c('black', 'red', 'blue', 'purple', 'orange'), 
       lty = 1)

```

As previously assumed, from the graph we can see that the coefficients of the various models are very similar to each other. All models give some importance to the intercept, i.e. the reference group, except for the LDA model which does not have a direct intercept in the Rstudio output. All models agree on assigning a negative weight to Male sex and for Age and a positive weight to Scholarship holder, which is consistent with the previous analysis. Except for the Lasso model, they all agree with the positive importance of the mother's level of education, however Father's qualification is not significant since is high correlated with it. Finally, it can noted that only the Ridge model gives importance to the student's level of education, in contrast to what was assumed in the EDA (table 7).


## Explaining the model

As we said before, since all models are similar, the logistic model is the best model to explain the phenomenon, so we will use it to explain the results:

- Contrary to what we assumed in the EDA, being married or not having a higher education degree do not seem to be influential variables in determining whether a student will graduate on time. The only relevant family variable is the mother's education level, which has a positive effect on the probability of graduating on time by at least 2.3 times compared to those whose mothers did not finish high school.

- There is no significant difference between those who live away from home and those who do not, nor between those who need special assistance and those who do not.

- Those who choose a STEM course have 1.64 times the probability of graduating on time compared to those who choose a non-STEM course.

- Being male also negatively impacts the probability of graduating on time, with about a 38% decrease compared to the probability for females.

- The financial aid from a scholarship has a significantly positive impact on the probability of graduating on time, with an increase of 7.86 times compared to those who do not have a scholarship. This indicates that not having to worry about paying for education and being able to afford not working during the study period has a significantly positive impact.

- Finally, age has a negative effect on the probability of graduating on time, with a 70% decrease for each additional year of age. This indicates that the later one starts university, the higher the chances of falling behind or dropping out.


# Conclusions

We built several models capable of moderately predicting whether a student will graduate based on enrollment data. Such a tool could be valuable for various real-world applications. For example, universities could use it to identify students who are struggling to graduate on time and implement policies to address these issues.

We also investigated whether the University of Portalegre awarded scholarships based on merit or need (i.e., coming from a low-income family). If scholarships were awarded based on merit, it would likely indicate that recipients graduate sooner simply because they are better students. However, we found that scholarships are awarded based on need. This suggests that the program effectively supports students during their academic journey, and increasing funding for it would be beneficial.

Our analysis also revealed that non-STEM students take longer to graduate. This finding warrants further investigation to determine if there are issues with the course structure or other contributing factors.


## Limitations and Future Work: 
Our models are based on enrollment data and do not account for other factors that may influence student success, such as personal circumstances, academic performance, or external factors. Additionally, our models only predict whether a student will graduate on time, which may not be the best metric to evaluate overall academic success.

Future work could involve incorporating additional data sources to improve the accuracy of the models and provide a more comprehensive understanding of student success. Further research could also explore the impact of different interventions on student outcomes and identify the most effective strategies for supporting at-risk students.







# References

Realinho, V.; Machado, J.;Baptista, L.; Martins, M.V. Predicting
Student Dropout and Academic Success. Data 2022, 7, 146.
https://doi.org/10.3390/data7110146


Valentim Realinho, Jorge Machado, Luís Baptista, & Mónica V. Martins. (2021). Predict students' dropout and academic success (1.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.5777340



## Appendix A

```{r echo = F, message = F}

# Load necessary library
library(knitr)

# Create a data frame that represents your dataset
original_data <- data.frame(
  Variable = c("Married", "Application mode", "Application order", "Course", 
               "Evening attendance", "Displaced", "Educational special needs", 
               "Tuition fees up to date", "Gender", "Scholarship holder", 
               "Age at enrollment", "International", "Curricular units 1st sem (credited)", 
               "Curricular units 1st sem (enrolled)", "Curricular units 1st sem (evaluations)", 
               "Curricular units 1st sem (approved)", "Curricular units 1st sem (grade)", 
               "Curricular units 1st sem (without evaluations)", "Curricular units 2nd sem (credited)", 
               "Curricular units 2nd sem (enrolled)", "Curricular units 2nd sem (evaluations)", 
               "Curricular units 2nd sem (approved)", "Curricular units 2nd sem (grade)", 
               "Curricular units 2nd sem (without evaluations)", "Unemployment rate", 
               "Inflation rate", "GDP", "Output", "Previous qualification",
               "Nationality", "Mother's qualification", "Father's qualification",
               "Mother's occupation", "Father's occupation"),
  Description = c("Categorical variable indicating the marital status of the individual",
                  "Categorical variable indicating the mode of application",
                  "Numeric variable indicating the order of application",
                  "Categorical variable indicating the chosen course",
                  "Binary variable indicating whether the individual attends classes during the daytime or evening",
                  "Binary variable indicating whether the individual has been displaced",
                  "Binary variable indicating whether the individual has educational special needs",
                  "Binary variable indicating whether the tuition fees are up to date",
                  "Binary variable indicating the gender of the individual",
                  "Binary variable indicating whether the individual holds a scholarship",
                  "Numeric variable indicating the age of the individual at the time of enrollment",
                  "Binary variable indicating whether the individual is international",
                  "Numeric variable indicating the number of credited curricular units in the 1st semester",
                  "Numeric variable indicating the number of enrolled curricular units in the 1st semester",
                  "Numeric variable indicating the number of evaluations for curricular units in the 1st semester",
                  "Numeric variable indicating the number of approved curricular units in the 1st semester",
                  "Numeric variable indicating the average grade for curricular units in the 1st semester",
                  "Numeric variable indicating the number of curricular units in the 1st semester without evaluations",
                  "Numeric variable indicating the number of credited curricular units in the 2nd semester",
                  "Numeric variable indicating the number of enrolled curricular units in the 2nd semester",
                  "Numeric variable indicating the number of evaluations for curricular units in the 2nd semester",
                  "Numeric variable indicating the number of approved curricular units in the 2nd semester",
                  "Numeric variable indicating the average grade for curricular units in the 2nd semester",
                  "Numeric variable indicating the number of curricular units in the 2nd semester without evaluations",
                  "Variable indicating the unemployment rate (Unemployment rate (%))",
                  "Numeric variable indicating the inflation rate (Inflation rate (%))",
                  "Numeric variable indicating the Gross Domestic Product",
                  "Categorical variable indicating the target variable (e.g., Dropout, Graduate, Enrolled)",
                  "Numeric variable indicating the level of the previous qualification",
                  "Categorical variable indicating the nationality of the individual",
                  "Numeric variable indicating the level of the mother's qualification",
                  "Numeric variable indicating the level of the father's qualification",
                  "Categorical variable indicating the mother's occupation",
                  "Categorical variable indicating the father's occupation")
)

# Create the table using kable
kable(original_data, col.names = c("Variable", "Description"), align = c('l', 'l'), caption = "Dataset Description")
```
