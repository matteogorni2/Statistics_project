---
title: "Stat_text"
date: "2024-05-23"
output: pdf_document
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
---

------------------------------------------------------------------------

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|r|}
  \hline
  Header1 & Header2 & Header3 \\
  \hline
  Row1Col1 & Row1Col2 & Row1Col3 \\
  Row2Col1 & Row2Col2 & Row2Col3 \\
  \hline
\end{tabular}
\caption{Tabella di esempio}
\end{table}

### 1. Obtaining the Data

We originally found the dataset on Kaggle. The data was collected by the Polytechnic Institute of Portalegre in Portugal to build machine learning models that predict a student’s outcome based on various socioeconomic factors and academic performance. This was done to develop an analytics tool for the tutoring program to direct their efforts more effectively.

The dataset was created from several disjoint databases and includes students enrolled in different undergraduate degrees, such as agronomy, design, education, nursing, journalism, management, social service, and technologies, it consists of 4424 records with 35 features.

The prediction problem is formulated as a three-category classification task, which assesses, based on socio-economic data and performance metrics during the academic years, whether a student will:

-   Graduate within the three years of planned course activities (‘Graduate’)
-   Change course or stop studying altogether (‘Dropout’)
-   Fail to graduate in time (‘Enrolled’)

According to the literature in the field, there is no agreed-upon definition of what constitutes a dropout. In this work, the authors defined dropouts from a micro-perspective, considering field and institution changes as dropouts regardless of when they occur. This approach results in much higher dropout rates than the macro-perspective, which considers only students who leave the higher education system without a degree.

Given that the number of libraries we can use is limited to those covered during the course, we decided to employ this dataset to answer a different research question. Our analysis will focus on building a model to assign a probability score to new students. This score will quantify the likelihood that a student will finish their course within a three-year timeframe based on data collected at enrollment.

A detailed description of the original dataset can be found in Appendix A.

### Step 2. Data Preprocessing

Let's start by loading our data and changing the output column in order to represent our research question correctly: we are going to convert all the 'Enrolled' labels into 'Dropouts' since they did not manage to complete their course in the scheduled timeframe

```{r,results='hide'}
data <- read.csv('~/Downloads/Dropout and Success/student_data.csv', 
                 sep = ';')
str(data)
names(data)[names(data) == 'Nacionality'] <- 'Nationality'
table(data$Output)
# Remove 'Enrolled' to have a binomial problem

index_enrolled <- which(data$Output == 'Enrolled')
# data <- data[-index, ]
data$Output[index_enrolled] <- 'Dropout'
```

```{r, include = FALSE}
library(knitr)
library(kableExtra)
# Rimuoviamo le variabili Curricular

# Remove Application and Curricular and evening data
data <- data[, -which(grepl('Application', colnames(data)) | 
                        grepl('Curricular', colnames(data)) |
                        grepl('evening', colnames(data)))]
### Collapse some categorical factor


```

Missing values:

there were no missing values in the dataset.

```{r}
sum(is.na(data))
```

**Data labeling**

We changed the label criteria for some of our variables in order to increase model explainability, in the following section we will discuss and showcase our changes.


```{r, echo = FALSE}
## Marital status
#1 – single 2 – married 3 – widower 4-divorced 5 de-facto union 6–legally separated)



index_single <- which(data$Marital.status == 1) # Single
index_married <- which(data$Marital.status == 2) # Married
data$Marital.status[index_single] <- 'Single'
data$Marital.status[index_married] <- 'Married'
data$Marital.status[-c(index_single, index_married)] <- 'Others'
```

```{r, echo = FALSE}
## Mother's occupation
# Split in white / blue collar
index_m_no_worker <- which(data$Mother.s.occupation == 1| # Student
                             data$Mother.s.occupation == 12 | # Other
                             data$Mother.s.occupation == 13) # Blanck
index_m_white <- which(data$Mother.s.occupation == 2 |
                         data$Mother.s.occupation == 3 |
                         data$Mother.s.occupation == 4 |
                         data$Mother.s.occupation == 5 |
                         data$Mother.s.occupation == 6 |
                         data$Mother.s.occupation == 14 | # Armed forces Officers
                         data$Mother.s.occupation == 15 | # Armed forces sergeants
                         data$Mother.s.occupation ==  17 |
                         data$Mother.s.occupation == 19 |
                         data$Mother.s.occupation == 20 |
                         data$Mother.s.occupation == 21 |
                         data$Mother.s.occupation == 22 |
                         data$Mother.s.occupation == 23 |
                         data$Mother.s.occupation == 24 |
                         data$Mother.s.occupation == 25 |
                         data$Mother.s.occupation == 26 |
                         data$Mother.s.occupation == 27 |
                         data$Mother.s.occupation == 28 |
                         data$Mother.s.occupation == 29)
data$Mother.s.occupation[index_m_no_worker] <- 'Others'
data$Mother.s.occupation[index_m_white] <- 'White Collar'
data$Mother.s.occupation[-c(index_m_no_worker, 
                            index_m_white)] <- 'Blue Collar'


## Father's occupation
# Slpit in white / blue collar
index_f_no_worker <- which(data$Father.s.occupation == 1| # Student
                             data$Father.s.occupation == 12 | #Other
                             data$Father.s.occupation == 13) # Blanck
index_f_white <- which(data$Father.s.occupation == 2 |
                         data$Father.s.occupation == 3 |
                         data$Father.s.occupation == 4 |
                         data$Father.s.occupation == 5 |
                         data$Father.s.occupation == 6 |
                         data$Father.s.occupation == 14 | # Armed forces Officers
                         data$Father.s.occupation == 15 | # Armed forces sergeants
                         data$Father.s.occupation ==  17 |
                         data$Father.s.occupation == 19 |
                         data$Father.s.occupation == 20 |
                         data$Father.s.occupation == 21 |
                         data$Father.s.occupation == 22 |
                         data$Father.s.occupation == 23 |
                         data$Father.s.occupation == 24 |
                         data$Father.s.occupation == 25 |
                         data$Father.s.occupation == 26 |
                         data$Father.s.occupation == 27 |
                         data$Father.s.occupation == 28 |
                         data$Father.s.occupation == 29)
data$Father.s.occupation[index_f_no_worker] <- 'Others'
data$Father.s.occupation[index_f_white] <- 'White Collar'
data$Father.s.occupation[-c(index_f_no_worker, 
                            index_f_white)] <- 'Blue Collar'

```

```{r, echo = FALSE}
## Mother's qualification

index_m_secondary <- which(data$Mother.s.qualification == 1 | 
                             data$Mother.s.qualification == 6 |
                             data$Mother.s.qualification == 13 |
                             data$Mother.s.qualification == 15 |
                             data$Mother.s.qualification == 22 |
                             data$Mother.s.qualification == 23 |
                             data$Mother.s.qualification == 29 |
                             data$Mother.s.qualification == 31 |
                             data$Mother.s.qualification == 32)
index_m_higher <- which(data$Mother.s.qualification == 2 |
                          data$Mother.s.qualification == 3 |
                          data$Mother.s.qualification == 4 |
                          data$Mother.s.qualification == 5 |
                          data$Mother.s.qualification == 30 |
                          data$Mother.s.qualification == 33 |
                          data$Mother.s.qualification == 34)
data$Mother.s.qualification[index_m_secondary] <- 'Secondary'
data$Mother.s.qualification[index_m_higher] <- 'Higher'
data$Mother.s.qualification[-c(index_m_secondary, 
                               index_m_higher)] <- 'No Secondary'

```

```{r, include = FALSE}

## Previous qualification
# Group on (no secondary education, secondary education, bachelor's degree, 
# master's degree, phd or highter)
index_secondary <- which(data$Previous.qualification == 1 | 
                           data$Previous.qualification == 6 |
                           data$Previous.qualification == 14 |
                           data$Previous.qualification == 16)
# Aggiungiamo '6' perchè se frequenti l'uni ti sei già diplomato e 
# se fai dei corsi da specializzando tecnico dovresti aver 
# fatto le superiori, però non valgono come la laurea

index_higher <- which(data$Previous.qualification == 2 |
                        data$Previous.qualification == 3 |
                        data$Previous.qualification == 15 |
                        data$Previous.qualification == 4 | # Since just 8
                        data$Previous.qualification == 5| # Since just 1 phd
                        data$Previous.qualification == 17)
data$Previous.qualification[index_secondary] <- 'Secondary'
data$Previous.qualification[index_higher] <- 'Higher'
data$Previous.qualification[-c(index_secondary, 
                               index_higher)] <- 'No Secondary'

# Gender
index_male <- which(data$Gender == 1)
data$Gender[index_male] <- 'Male'
data$Gender[-index_male] <- 'Female'



## Father's qualification
index_f_secondary <- which(data$Father.s.qualification == 1 | 
                             data$Father.s.qualification == 6 |
                             data$Father.s.qualification == 13 |
                             data$Father.s.qualification == 15 |
                             data$Father.s.qualification == 22 |
                             data$Father.s.qualification == 23 |
                             data$Father.s.qualification == 29 |
                             data$Father.s.qualification == 31 |
                             data$Father.s.qualification == 32)
index_f_higher <- which(data$Father.s.qualification == 2 |
                          data$Father.s.qualification == 3 |
                          data$Father.s.qualification == 4 |
                          data$Father.s.qualification == 5 |
                          data$Father.s.qualification == 30 |
                          data$Father.s.qualification == 33 |
                          data$Father.s.qualification == 34)
data$Father.s.qualification[index_f_secondary] <- 'Secondary'
data$Father.s.qualification[index_f_higher] <- 'Higher'
data$Father.s.qualification[-c(index_f_secondary, 
                               index_f_higher)] <- 'No Secondary'


```



```{r, echo = FALSE}
## Course
index_stem <- which(data$Course == 1 |
                      data$Course == 4 |
                      data$Course == 6 |
                      data$Course == 7 |
                      data$Course == 8 |
                      data$Course == 12 |
                      data$Course == 13)
data$Course[index_stem] <- 'Stem'
data$Course[-index_stem] <- 'No Stem'

```

```{r, echo = FALSE}

## Nationality

data$Nationality[data$Nationality != 1] <- 'Others' # just 2%
data$Nationality[data$Nationality == 1] <- 'Portoguese'

```

```{r, include = FALSE}
# Change features as categorical
colnames(data)
categorical_names <- colnames(data)[-c(15, 17:19)]

for(i in categorical_names){
  data[, i] <- as.factor(data[, i])
}

# Change variable's levels
data$Marital.status <- factor(data$Marital.status, 
                              levels = c('Single', 'Married', 'Others'))
data$Course <- factor(data$Course, 
                      levels = c('Stem', 'No Stem'))
data$Previous.qualification <- factor(data$Previous.qualification, 
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Nationality <- factor(data$Nationality, 
                           levels = c('Portoguese', 'Others'))
data$Mother.s.qualification <- factor(data$Mother.s.qualification, 
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Father.s.qualification <- factor(data$Father.s.qualification,
                                      levels = c('No Secondary', 'Secondary', 'Higher'))
data$Mother.s.occupation <- factor(data$Mother.s.occupation,
                                   levels = c('Blue Collar', 'White Collar', 'Others'))
data$Father.s.occupation <- factor(data$Father.s.occupation,
                                   levels = c('Blue Collar', 'White Collar', 'Others'))
data$Gender <- factor(data$Gender,
                      levels = c('Female', 'Male'))
data$Output <- factor(data$Output,
                      levels = c('Dropout', 'Graduate'))


table(data$Nationality, data$International)
data <- data[, -which(colnames(data) == 'International')]
```

Marital status

Categorical variable indicating the marital status of the individual. We only have 4 widowers and 6 legally separated instances, therefore we collapsed them under the variable 'Others', we also decided to merge 'divorced' and 'legally separated' since the difference between the two instances is not relevant for our analysis.

```{r, echo = F}
kable_styling(kable(table(data$Marital.status),
                    caption = "Marital status",
                    col.names = c("Status", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = TRUE),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)
```

Mother/Father occupation

Categorical variables indicating the mother and father occupation respectively, while the original dataset had 32 labels for all kinds of different jobs, we decided to merge some of the labels and make this a 3-label categorical variable. Jobs were split based on a White/Blue collar distinction, as shown in the table below.

```{r, echo = F}
t_occ <- data.frame(Occupation = levels(data$Mother.s.occupation),
                    Mother = matrix(table(data$Mother.s.occupation)), 
                    Father = matrix(table(data$Father.s.occupation)))
kable_styling(kable(t_occ,
                    caption = "Parent's occupation",
                    align = 'c',
                    format = "latex",
                    booktabs = TRUE),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)
```

Mother/Father/Student education

Categorical variable indicating the level of each parents’ qualification as well as the student's. Again, we are dealing with many categorical variables, so we decided to merge them based on whether they have an completed an higher education cycle and if they have finished high school or not

```{r, echo = F}
t_qual <- data.frame(Qualification = levels(data$Mother.s.qualification),
                     Mother = matrix(table(data$Mother.s.qualification)), 
                     Father = matrix(table(data$Father.s.qualification)), 
                     Student = matrix(table(data$Previous.qualification)))
kable_styling(kable(t_qual,
                    caption = "Qualification",
                    align = 'c',
                    format = "latex",
                    booktabs = TRUE),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)
```              


Courses

Categorical variable representing the course chosen at enrollment, we originally had 17 different courses, we decided to relabel the courses using whether they are STEM subjects or not as a splitting criterion

```{r, echo = F}
kable_styling(kable(table(data$Course),
                    caption = "Course",
                    col.names = c("Course", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = TRUE),
              latex_options = "hold_position", 
              full_width = F,
              font_size = 9)
```

Nationality:

categorical variable representing a students nationality: looking at the data we saw that while we had a lot of labels (one for each nationality), the vast majority of people were Portuguese, therefore we labeled data in the following way

```{r, echo = F}
kable_styling(kable(table(data$Nationality),
                    caption = "Nationality", 
                    col.names = c("Nationality", "Freq"),
                    align = 'c',
                    format = "latex",
                    booktabs = TRUE),
              latex_options = "hold_position",
              full_width = F,
              font_size = 9)
```

**FEATURE REMOVAL**

We decided to remove some features from our data (non mi viene cosa scrivere ora).

1 Curricular data: we removed all information concerning student performance over the span of three years since it doesn't help us answer our research question

2 Application mode/ application order: the original paper didn't offer a clear indication about the various labels of this feature, furthermore we don't believe them to be of any interest as far as our research question goes.



### 3. Exploratory Data Analysis

In this section we will take a closer look at our data. First, we'll plot some contingency tables for our categorical variables. TESTO PRESO DA CPT, BEPPE VEDI SE ALCUNE COSE HANNO SENSO Key Findings Among single students, the dropout rate is notably high with 1904 dropouts compared to 2015 graduates. This trend suggests that single students face challenges that significantly impact their ability to complete their studies. In contrast, married students exhibit a lower dropout rate (231) and a relatively modest number of graduates (148), indicating different dynamics at play in this group. Students with other marital statuses show much lower numbers overall, making it difficult to draw significant conclusions.

When examining course types, students enrolled in non-STEM courses exhibit substantial dropout (1333) and graduation (1309) rates. Those in STEM courses have fewer dropouts (822) and slightly more graduates (900), suggesting a marginally higher retention rate in STEM fields.

The influence of prior qualifications is pronounced. Students with higher qualifications before enrollment show lower dropout numbers (124) and a smaller cohort of graduates (80). Conversely, those with only secondary education qualifications have the highest dropout (1922) and graduation (2066) rates, indicating a larger population and perhaps greater variability in outcomes.

Nationality plays a crucial role, with Portuguese students showing high numbers of both dropouts (2159) and graduates (2155), reflecting their dominant presence in the dataset. Other nationalities have significantly lower figures, with 56 dropouts and 54 graduates, suggesting different educational dynamics or smaller population sizes.

Parental education appears influential. Students whose parents have secondary education show the highest numbers of both dropouts and graduates, pointing to the significant impact of secondary-level parental education. Higher parental education correlates with fewer dropouts but does not significantly boost graduate numbers.

Parental occupation also correlates with student outcomes. Students with blue-collar parents have higher dropout rates compared to those with white-collar parents. In particular, blue-collar fathers correlate with 1220 dropouts compared to 849 for white-collar fathers. The data suggests a connection between parental occupation and student success, with white-collar backgrounds associated with higher graduation rates.

Displacement status significantly affects outcomes. Displaced students have a much lower dropout rate (1113) and a higher graduation rate (1324) compared to their non-displaced peers, highlighting the effectiveness of support systems for displaced individuals.

Financial stability, indicated by up-to-date tuition fees and scholarship holding, plays a crucial role. Students with up-to-date tuition fees show higher graduation numbers (1810) compared to those with overdue fees (264 graduates). Scholarship holders, in particular, fare better with 1374 graduates compared to 1340 non-holders, and fewer dropouts (801 compared to 1507), underscoring the positive impact of financial aid on student retention and success.

Gender differences are evident, with female students showing higher numbers of both dropouts (1207) and graduates (1681) compared to males (1008 dropouts and 548 graduates). This indicates a larger female student population and a higher success rate among females, pointing to gender-specific factors that influence educational outcomes.


```{r, echo = F}
library(knitr)
library(xtable)
t1 <- kable(addmargins(table(data$Marital.status, data$Output), 2), 
            format = "latex", booktabs = TRUE)
t2 <- kable(addmargins(table(data$Course, data$Output), 2), 
            format = "latex", booktabs = TRUE)
t3 <- kable(addmargins(table(data$Previous.qualification, data$Output), 2), 
            format = "latex", booktabs = TRUE)
t4 <- kable(addmargins(table(data$Nationality, data$Output), 2),
            format = "latex", booktabs = TRUE)
t5 <- kable(addmargins(table(data$Mother.s.qualification, data$Output), 2),
            format = "latex", booktabs = TRUE)
t6 <- kable(addmargins(table(data$Father.s.qualification, data$Output), 2),
            format = "latex", booktabs = TRUE)
t7 <- kable(addmargins(table(data$Mother.s.occupation, data$Output), 2),
            format = "latex", booktabs = TRUE)
t8 <- kable(addmargins(table(data$Father.s.occupation, data$Output), 2),
            format = "latex", booktabs = TRUE)
t9 <- kable(addmargins(table(data$Displaced, data$Output), 2),
            format = "latex", booktabs = TRUE)
t10 <- kable(addmargins(table(data$Tuition.fees, data$Output), 2),
             format = "latex", booktabs = TRUE)
t11 <- kable(addmargins(table(data$Scholarship, data$Output), 2),
             format = "latex", booktabs = TRUE)
t12 <- kable(addmargins(table(data$Gender, data$Output), 2),
             format = "latex", booktabs = TRUE)

```
*AGGIUNGERE LINK TABELLE DI RIFERIMENTO*
Dalle tabelle sottostanti si può notare che chi è single riesce a laurearsi prima rispetto a chi non lo è. Generalmente chi è uscito dalla scuola superiore ha un tasso di laurearsi in tempo superiore a chi non ha terminato il liceo o ha conseguito un altro titolo di studio post-liceo. 
Nonostante ci siano più persone che frequentano corsi non STEM (2702 vs 1722), il tasso di laurea è maggiore per chi frequenta corsi STEM (48.4% vs 52.3%).
Far notare che le madri hanno un titolo di studio superiore ai padri, 234 vs 3076 sono le madri e i padri che non hanno terminato le superiori. Da notare che a prescindere dal titolo di studio dei genitori, il tasso di laurea degli studenti con i genitori laureati è inferiore rispetto a chi ha genitori con titolo di studio inferiore.
Mentre non vi è una particolare differenza tra chi ha i genitori con professioni diverse, tranne per chi ha segnato 'Other' che hanno un tasso di laurearsi in tempo inferiore.
Notare che ci usa 'Displaced' ha un tasso di laurea maggiore rispetto a chi non lo è.
E come si poteva aspettare, chi non riesce a pagare le tasse in tempo ha un elevato tasso di abbandono rispetto a chi riesce a pagare in tempo (94.5% vs 44.0%).
Chi riesce a fare la Scholarship ha un tasso di laurea maggiore rispetto a chi non la fa (76.0% vs 41.3%).
Ed infine si può notare che le donne hanno una probabilità notevolmente maggiore rispetto agli uomini di laurearsi in tempo (57.9% vs 35.2%).

*Da valutare di togliere le tabelle prevedenti e tenere solo queste che contengono le stesse informazioni più la differenza tra i due gruppi, e sopra fare solo una descrizione di come sono stati raggruppati i gruppi.*

```{r sample, echo=FALSE, results='asis'}

# \\fontsize{18}{5}\\selectfont # Il primo {} è la dimensione del font, 
#  il secondo è lo spazio tra le righe

cat(c("\\begin{table}[!htb]
    \\fontsize{9}{11}\\selectfont
    \\begin{minipage}{.5\\linewidth}
      \\caption{Marital status VS Output}
      \\centering",
        t1,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Previous qualification VS Output}",
        t3,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Course VS Output}
      \\centering",
        t2,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Nationality VS Output}",
        t4,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Mother's qualification VS Output}
      \\centering",
        t5,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Father's qualification VS Output}",
        t6,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Mother's occupation VS Output}
      \\centering",
        t7,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Father's occupation VS Output}",
        t8,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Displaced VS Output}
      \\centering",
        t9,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\centering
        \\caption{Tuition fees VS Output}",
        t10,
    "\\end{minipage}
    \\begin{minipage}{.5\\linewidth}
      \\caption{Scholarship VS Output}
      \\centering",
        t11,
    "\\end{minipage}%
    \\begin{minipage}{.5\\linewidth}
      \\caption{Gender VS Output}
      \\centering",
        t12,
    "\\end{minipage}
        
        
\\end{table}"
))  
```

```{r, echo = F}
# Supponiamo di avere i seguenti dataframe
data1 <- data.frame(
  Var1 = sample(c("A", "B", "C"), 6, replace = TRUE),
  Var2 = sample(c("D", "E", "F"), 6, replace = TRUE),
  Var3 = sample(c("G", "H", "I"), 6, replace = TRUE)
)

data2 <- data.frame(
  Var1 = sample(c("J", "K", "L"), 6, replace = TRUE),
  Var2 = sample(c("M", "N", "O"), 6, replace = TRUE),
  Var3 = sample(c("P", "Q", "R"), 6, replace = TRUE)
)

data3 <- data.frame(
  Var1 = sample(c("S", "T", "U"), 6, replace = TRUE),
  Var2 = sample(c("V", "W", "X"), 6, replace = TRUE),
  Var3 = sample(c("Y", "Z", "A"), 6, replace = TRUE)
)

# Creiamo e formattiamo le tabelle
table1 <- kable(data1, format = "latex", booktabs = TRUE, col.names = c("Var1", "Var2", "Var3")) %>%
  kable_styling(latex_options = c("striped", "hold_position"))

table2 <- kable(data2, format = "latex", booktabs = TRUE, col.names = c("Var1", "Var2", "Var3")) %>%
  kable_styling(latex_options = c("striped", "hold_position"))

table3 <- kable(data3, format = "latex", booktabs = TRUE, col.names = c("Var1", "Var2", "Var3")) %>%
  kable_styling(latex_options = c("striped", "hold_position"))

# Genera il codice LaTeX per disporre le tabelle su una riga
# cat(paste0("\\begin{table}[H]
# \\centering
# \\begin{tabular}{c c c}
# ", table1, " & ", table2, " & ", table3, " \\\\
# \\end{tabular}
# \\end{table}"))


```



**OUTLIERS DETECTION AND REMOVAL**

We looked for outliers among numerical features in our data, 'age at enrollment' was the only one that presented some. Since it have not a normal distribution, we decided to remove all values which are outliers in the boxplot of values condictioned by the outputs. We then perform a train-test split before and remove the outliers from the training set only: keeping outliers in the test set improves the ecological validity of the model and reduces overfitting.

```{r, echo = F, fig.align = 'center', out.height = '40%'}
# Train test split
set.seed(71)
index <- sample(1:nrow(data), 
                round(0.75 * nrow(data)))
train <- data[index, ]
test <- data[-index, ]

# outliers removal
par(mfrow = c(1, 2))
boxplot(train$Age.at.enrollment ~ train$Output, 
        col = c('red', 'blue'),
        main = 'Age at enrollment',
        xlab = 'Output')
plot(density(train$Age.at.enrollment[train$Output == 'Graduate']), 
     col = 'blue',
     main = 'Age at enrollment', 
     xlab = 'Age at enrollment')
lines(density(train$Age.at.enrollment[train$Output == 'Dropout']), 
      col = 'red')
legend('topright', 
       legend = c('Graduate', 'Dropout'), 
       col = c('blue', 'red'), 
       lty = 1)
par(mfrow = c(1, 1))

```

```{r, echo = F, fig.show = 'hide'}
# Removing outliers
max_age_drop <- min(boxplot(train$Age.at.enrollment[train$Output == 'Dropout'])$out)
index_age_drop <- which(train$Age.at.enrollment >= max_age_drop &
                          train$Output == 'Dropout')
max_age_grad <- min(boxplot(train$Age.at.enrollment[train$Output == 'Graduate'])$out)
index_age_grad <- which(train$Age.at.enrollment >= max_age_grad &
                          train$Output == 'Graduate')
train <- train[-c(index_age_drop, 
                  index_age_grad), ]


```

```{r, echo = F, out.height = '30%', fig.align = 'center', height = 5}
# After removal
par(mfrow = c(1, 2))
boxplot(train$Age.at.enrollment ~ train$Output, 
        col = c('red', 'blue'),
        main = 'Age at enrollment',
        xlab = 'Output', 
        ylab = 'Age at enrollment')
plot(density(train$Age.at.enrollment[train$Output == 'Graduate']), 
     col = 'blue',
     main = 'Age at enrollment', 
     xlab = 'Age at enrollment')
lines(density(train$Age.at.enrollment[train$Output == 'Dropout']), 
      col = 'red')
legend('topright', 
       legend = c('Graduate', 'Dropout'), 
       col = c('blue', 'red'), 
       lty = 1)
par(mfrow = c(1, 1))
```

### 4. Model Building
```{r, echo = F, results= 'hide', message= FALSE}
library(MASS)

######################################
# Logistic Regression
######################################
fit_logit_1 <- glm(Output ~ .,
            family = binomial,
            data = train)
summary(fit_logit_1)
fit_logit_2 <- step(fit_logit_1)
summary(fit_logit_2)
# par(mfrow = c(2, 2))
# plot(fit_logit_2)
# par(mfrow = c(1, 1))

# Removing outliers
index_outliers <- which(abs(scale(fit_logit_2$residuals)) > 2)
train <- train[-index_outliers, ]
fit_logit_3 <- update(fit_logit_2, 
               .~.)
summary(fit_logit_3)


```
```{r, echo = F, , out.width = '50%'}


```
Nel grafico dei Q-Q residuals possiamo vedere che i residui non sono distribuiti normalmente, questo sta ad indicare che le variabili inserite nel modello non sono sufficienti per spiegare la variabilità dei dati. Inoltre, il grafico dei residui vs leverage mostra che ci sono dei valori che hanno un leverage molto alto, stando ad indicare che sono ancora presenti valori anomali nel dataset.



```{r, echo = F, out.width = '90%', out.height= '40%', fig.align = 'center'}
kable(round(summary(fit_logit_3)$coefficients, 3), 
      caption = 'Logistic regression coefficients',
      align = 'r',
      format = 'latex',
      booktabs = T, 
      font_size = 6)

par(mfrow = c(2, 2))
plot(fit_logit_3)
par(mfrow = c(1, 1))

# Predictions
pred_logit <- predict(fit_logit_3, 
                      newdata = test, 
                      type = 'response')
```
```{r, echo = F}
# Removing outliers
index_outliers <- which(abs(scale(fit_logit_3$residuals)) > 2)
train2 <- train[-index_outliers, ]
fit_logit_4 <- update(fit_logit_3, 
                      .~., 
                      data = train2)
summary(fit_logit_4)

# Removing outliers
index_outliers <- which(abs(scale(fit_logit_4$residuals)) > 2)
train2 <- train2[-index_outliers, ]
fit_logit_5 <- update(fit_logit_4,
                      .~.)
summary(fit_logit_5)

# Removing outliers
index_outliers <- which(abs(scale(fit_logit_5$residuals)) > 2)
train2 <- train2[-index_outliers, ]
fit_logit_6 <- update(fit_logit_5, 
                      .~.)
summary(fit_logit_6)

# Removing outliers
index_outliers <- which(abs(scale(fit_logit_6$residuals)) > 2)
train2 <- train2[-index_outliers, ]
fit_logit_7 <- update(fit_logit_6, 
                      .~.)
summary(fit_logit_7)


# Removing outliers
index_outliers <- which(abs(scale(fit_logit_7$residuals)) > 2)
train2 <- train2[-index_outliers, ]
fit_logit_8 <- update(fit_logit_7, 
                      .~.)
summary(fit_logit_8)

# Removing outliers
index_outliers <- which(abs(scale(fit_logit_8$residuals)) > 2)
train2 <- train2[-index_outliers, ]
fit_logit_9 <- update(fit_logit_8, 
                      .~.)
summary(fit_logit_9)


```


```{r, echo = F, out.height= '30%'}
######################################
# Linear Discriminant Analysis (LDA)
######################################

fit_lda <- lda(Output ~ ., 
               data = train)
# Plot density of lda coefficients with respect to the output
plot(fit_lda, 
     type = 'both',
     main = 'LDA') # col not working


kable_styling(kable((round(fit_lda$scaling, 3)), 
      caption = 'LDA coefficients',
      align = 'c',
      format = 'latex',
      booktabs = T), 
      latex_options = c("hold_position", "scale_down"),
      full_width = F,
      position = "center",
      font_size = 9)


# Predictions
pred_lda <- predict(fit_lda, 
                    newdata = test)$posterior[, 2]


```

```{r, echo = F}
######################################
# Quadratic Discriminant Analysis (QDA)
######################################

fit_qda <- qda(Output ~ ., 
               data = train)
kable_styling(kable((round(fit_qda$scaling, 3)),
      caption = 'QDA coefficients',
      align = 'c',
      format = 'latex',
      booktabs = T), 
      latex_options = c("hold_position", "scale_down"),
      full_width = F,
      position = "right",
      font_size = 9)

# Plot density of qda coefficients with respect to the output
pred_qda <- predict(fit_qda, 
                    newdata = test)$posterior[, 2]
detach('package:MASS')
```

```{r, echo = FALSE, message= FALSE}
###########################
# Ridge Regression
###########################

library(glmnet)

design_matrix_train <- model.matrix(Output ~ .,
                                    data = train)

design_matrix_test <- model.matrix(Output ~ .,
                                   data = test)

# Adattamento del modello Ridge con regressione Binomiale
ridge_model <- glmnet(x = design_matrix_train, 
                      y = train$Output,
                      alpha = 0, 
                      intercept = F, 
                      family = "binomial")
# Find the best lambda

predictions <- predict(ridge_model, 
                       newx = design_matrix_test, 
                       type = 'response')
##########################

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$Output)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index_r <- which.max(accuracies)
pred_ridge <- matrix(predictions[, best_lambda_index_r], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index_r]

colnames(pred_ridge) <- 'pred_ridge'
```

```{r, echo = F}
###########################
# Lasso Regression
###########################

# Adattamento del modello Lasso con regressione Binomiale
lasso_model <- glmnet(x = design_matrix_train, 
                      y = train$Output,
                      alpha = 1, 
                      intercept = F, 
                      family = "binomial")
# Find the best lambda

predictions <- predict(lasso_model, 
                       newx = design_matrix_test, 
                       type = 'response')
##########################

accuracies <- c()

for (i in 1:ncol(predictions)) {
  predicted_classes <- round(predictions[, i])
  confusion_matrix <- table(predicted_classes, test$Output)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  accuracies <- c(accuracies, accuracy)
}

best_lambda_index_l <- which.max(accuracies)
pred_lasso <- matrix(predictions[, best_lambda_index_l], 
                     ncol = 1)
best_accuracy <- accuracies[best_lambda_index_l]

colnames(pred_lasso) <- 'pred_lasso'
```
```{r, echo = F}
# Some fancy graphs about ridge model
plot(ridge_model, xvar = 'lambda', label = TRUE)
plot(ridge_model, xvar = 'dev', label = TRUE)

# Some fancy graphs about lasso model
plot(lasso_model, xvar = 'lambda', label = TRUE)
plot(lasso_model, xvar = 'dev', label = TRUE)

```

```{r, echo = F, message = F, warning = FALSE}
# Create roc curve for all models
library(pROC)

# Logit
roc_out_logit <- roc(test$Output ~ pred_logit, 
                     levels = c('Dropout', 'Graduate'))


# LDA
roc_out_lda <- roc(test$Output ~ pred_lda, 
                   levels = c('Dropout', 'Graduate'))


# QDA
roc_out_qda <- roc(test$Output ~ pred_qda, 
                   levels = c('Dropout', 'Graduate'))


# Ridge
roc_out_ridge <- roc(test$Output ~ pred_ridge, 
                     levels = c('Dropout', 'Graduate'))

# Lasso
roc_out_lasso <- roc(test$Output ~ pred_lasso, 
                     levels = c('Dropout', 'Graduate'))

# Plotting the ROC curves and resize the plot on 'False Positive Rate' = c(1, 0)
plot(roc_out_logit, 
     col = 'red', 
     lwd = 2, 
     main = 'ROC curves', 
     xlab = 'False Positive Rate', 
     ylab = 'True Positive Rate')
lines(roc_out_lda, 
      col = 'blue', 
      lwd = 2)
lines(roc_out_qda,
      col = 'green', 
      lwd = 2)
lines(roc_out_ridge,
      col = 'purple', 
      lwd = 2)
lines(roc_out_lasso,
      col = 'orange', 
      lwd = 2)
legend('bottomright', 
       legend = c('Logit', 'LDA', 'QDA', 'Ridge', 'Lasso'), 
       col = c('red', 'blue', 'green', 'purple', 'orange'), 
       lty = 1)




```

### 5. Model Evaluation

```{r, echo = F}
# AUC
auc_logit <- round(auc(roc_out_logit), 3)
auc_lda <- round(auc(roc_out_lda), 3)
auc_qda <- round(auc(roc_out_qda), 3)
auc_ridge <- round(auc(roc_out_ridge), 3)
auc_lasso <- round(auc(roc_out_lasso), 3)

# Confusion matrix
confusion_matrix_logit <- addmargins(table(round(pred_logit), test$Output), 2)
confusion_matrix_lda <- addmargins(table(round(pred_lda), test$Output), 2)
confusion_matrix_qda <- addmargins(table(round(pred_qda), test$Output), 2)
confusion_matrix_ridge <- addmargins(table(round(pred_ridge), test$Output), 2)
confusion_matrix_lasso <- addmargins(table(round(pred_lasso), test$Output), 2)


# Accuracy
accuracy_logit <- sum(diag(confusion_matrix_logit)) / sum(confusion_matrix_logit[, -3])
accuracy_lda <- sum(diag(confusion_matrix_lda)) / sum(confusion_matrix_lda[, -3])
accuracy_qda <- sum(diag(confusion_matrix_qda)) / sum(confusion_matrix_qda[, -3])
accuracy_ridge <- sum(diag(confusion_matrix_ridge)) / sum(confusion_matrix_ridge[, -3])
accuracy_lasso <- sum(diag(confusion_matrix_lasso)) / sum(confusion_matrix_lasso[, -3])

# Precision
precision_logit <- confusion_matrix_logit[2, 2] / sum(confusion_matrix_logit[2, -3])
precision_lda <- confusion_matrix_lda[2, 2] / sum(confusion_matrix_lda[2, -3])
precision_qda <- confusion_matrix_qda[2, 2] / sum(confusion_matrix_qda[2, -3])
precision_ridge <- confusion_matrix_ridge[2, 2] / sum(confusion_matrix_ridge[2, -3])
precision_lasso <- confusion_matrix_lasso[2, 2] / sum(confusion_matrix_lasso[2, -3])

# Recall
recall_logit <- confusion_matrix_logit[2, 2] / sum(confusion_matrix_logit[, 2])
recall_lda <- confusion_matrix_lda[2, 2] / sum(confusion_matrix_lda[, 2])
recall_qda <- confusion_matrix_qda[2, 2] / sum(confusion_matrix_qda[, 2])
recall_ridge <- confusion_matrix_ridge[2, 2] / sum(confusion_matrix_ridge[, 2])
recall_lasso <- confusion_matrix_lasso[2, 2] / sum(confusion_matrix_lasso[, 2])
  
# F1 score
f1_logit <- 2 * (precision_logit * recall_logit) / (precision_logit + recall_logit)
f1_lda <- 2 * (precision_lda * recall_lda) / (precision_lda + recall_lda)
f1_qda <- 2 * (precision_qda * recall_qda) / (precision_qda + recall_qda)
f1_ridge <- 2 * (precision_ridge * recall_ridge) / (precision_ridge + recall_ridge)
f1_lasso <- 2 * (precision_lasso * recall_lasso) / (precision_lasso + recall_lasso)

# Plot a matrix with all the results
results <- data.frame(Model = c('Logit', 'LDA', 'QDA', 'Ridge', 'Lasso'),
                      AUC = round(c(auc_logit, auc_lda, auc_qda, auc_ridge, auc_lasso), 3),
                      Accuracy = round(c(accuracy_logit, accuracy_lda, accuracy_qda, accuracy_ridge, accuracy_lasso), 3),
                      Precision = round(c(precision_logit, precision_lda, precision_qda, precision_ridge, precision_lasso), 3),
                      Recall = round(c(recall_logit, recall_lda, recall_qda, recall_ridge, recall_lasso), 3),
                      F1 = round(c(f1_logit, f1_lda, f1_qda, f1_ridge, f1_lasso), 3))



kable_styling(kable(results,
      caption = 'Model evaluation',
      align = 'c',
      format = 'latex',
      booktabs = T), 
              latex_options = "hold_position", 
              full_width = F,
              # position = "center", 
              font_size = 9)
```
